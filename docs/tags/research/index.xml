<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Research on ShixiangWang
王诗翔</title>
    <link>/tags/research/</link>
    <description>Recent content in Research on ShixiangWang
王诗翔</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Sep 2021 00:00:00 +0000</lastBuildDate><atom:link href="/tags/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>「转载」可重复性危机</title>
      <link>/blog/reproducibility-issue/</link>
      <pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/reproducibility-issue/</guid>
      <description>原文来自《现代科研指北》第3.3节。
 可重复性危机是当前科研领域里最大的问题，如果结论不可被重复验证，那么科学性就无从谈起。这里我们先讨论科研里通用假设检验的问题，然后讨论下规律性，最后介绍应对这个危机的可重复性研究与开放科学趋势。
3.3.1 零假设显著性检验（NHST） 零假设显著性检验（NHST）则是可重复性危机的核心。NHST 更常见的形式是 p 值，也就是在零假设成立的条件下某事件发生的概率。打个比方，我们从一个混合了黑白两种颜色小球的口袋里有放回的取一个小球三次，结果都是白球。这里我们设定零假设为黑球白球各一半，那么发生三次白球的概率为12.5%，这个不算极端。但是，如果有放回取了十次，结果还是都是白球，这情况发生概率大概为千分之一，这就比较极端了。在此基础上，我们有理由认为零假设不成立，而此时就需要一个阈值来帮助我们判断是否成立，目前学术界会认为5%或0.05的概率可以作为显著性与否的阈值。科研中我们会去计算零假设下出现当前实验结果的概率，也就是p值，如果低于阈值就可以认为是极端事件就拒绝零假设而高于阈值则认为零假设下可能发生。
当然，我们现在科研用的p值还会考虑零假设之外的备择假设，如果拒绝了零假设就转而接受备择假设。不过一旦引入备择假设就需要讨论错误，这里我们把决策出的结果分为阴性与阳性，而事实分为真假。零假设为真但接受了备择假设的情况，这就是假阳性或者第一类错误；零假设为假但没拒绝零假设就是假阴性或者第二类错误。这里我们可以看到第一类错误与前面设定的决策阈值密切相关，如果设定在5%或者0.05，那么我们就有5%的可能性做出了错误判断。第二类错误则与统计功效也就是真阴性的概率有关，通常会设定在80%，如果功效过低，例如10%，那么犯第二类错误的概率就很高。举例来说，我脚43码的但我不知道，这时去买鞋别人问我脚尺码我说44码的其实是错了，但不影响脚能穿进去，此时尺码的区别功效就不足。但如果我穿久了就会发现确实是大了，此时相当于我通过多次实验或采样提高了统计功效，但可能这个差别虽然明显但也不影响穿。通常NHST关心第一类错误，但设计实验会考虑第二类错误，通过提高样本量来提高统计功效。
p 值有多流行呢？根据 Jeff Leek 的估计，如果把 p 值当成一篇文献，那么其被引次数已经超过 300 万次了，当之无愧的史上被引次数之王，甩第二名一个数量级。原因其实很简单，p 值已经渗透到几乎所有学科的研究中了，特别是实验学科。可想而知，如果产生 p 值的 NHST 出了问题其影响力有多大。下面谈下 NHST 具体的问题：
如果一个假设对另一个假设来说很稀少，NHST 会在很低的条件概率下拒绝掉，然后那些稀少的事情在 NHST 里就成了无法被检验的事情。这个例子最早是 Cohen 提出用来说明人们在使用 NHST 时的问题。零假设是某人是中国人，备择假设是非中国人。我们知道张三是人大代表的概率大概是百万分之二，这是个事实。不过这个事实在零假设里很难发生，备择假设里也无法发生。零假设我们拒绝了某人是中国人，那么根据 NHST，他不是中国人。但问题是人大代表一定要是中国人，此时就会出现事实跟NHST矛盾的情况。在此类问题里，NHST 永远无法认定稀有事件，也就是功效永远不足，并会给出错误答案。
这个问题本质上是多数人在使用 p 值时搞混了条件概率，拿上面人大代表的例子来说，我们的假设 H0 在面对张三这个数据 D 时给出了拒绝 p(H0|D)=0p(H0|D)=0，这个决定是构建在假设 H0 成立时出现 D 的概率太低（即 p(D|H0)p(D|H0)）之上，也就是说 NHST 下，我们默认下面的概率是成立的：
p(D|H0)=p(H0|D)p(D|H0)=p(H0|D)如果你修过任何基础的统计学课程都会知道这两个概率之间差了一个贝叶斯公式。通过使用贝叶斯定理，在新数据出现后原有概率是要被更新而不是直接拒绝掉的。p 值给的是前者，要想知道随机生成的概率，需要知道零假设为真的概率。通俗点说就是 NHST 属于革命派，不认可就打倒你；贝叶斯属于改良派，用新的证据更新原有理论。这个问题的本质就是把假设下的事实与事实下的假设搞混导致的，这是 NHST 的一个致命问题，然而致命问题可不止这一个。
过去的一百年，测量方法的精度是在不断提高的，而精度其实又会影响研究结果，很不幸，也是通过 NHST 来进行的。其实 NHST 在实验物理学里用的还是好好的，例如我去检测一个物理量，只有数据出现在其理论预测下数值四五个标准差以外才会对理论产生实质作用。此时，测量精度越高，由于测量误差导致的对原有理论的冲击就会越少，因为物理学的预测性要比化学生物等学科要好不少且此时 NHST 检测的原有理论是比较真实的。但在其他学科，特别是心理学跟医学的控制实验里，在实验开始前你几乎就可以确定零假设是不成立的，要不然你也没必要分组，此时你去搞 NHST ，几乎一定可以找到差异，此时测量精度如果不断上升，那么你会识别到一系列差异，但这些差异的效果是无法体现在p值里的，p值可能非常小，但效应却属于明显但很微弱，这样的结果也许可以发表，但对实际问题的解决几乎没有贡献。更极端的情况是如果你加大了样本量来提高统计功效，你总是能发现差异的，也就是你的零假设里原有学科理论为真也是会被方法学进步给推翻的。总结下就是 Meehl 在60年代就提出的悖论：方法学的进步与增大样本数对于相对硬（理论根基深厚）的学科证伪是正面的，但对相对软（理论比较模糊）的学科则是弱化。方法学悖论的根基其实是应用学科与基础学科的矛盾，基础学科用 NHST 检验观察事实中的理论，但应用学科用 NHST 来检验的是实验设计预测下的事实，此时实验设计的那个假设与 NHST 的零假设并不对应，而 NHST 先天弱化零假设的问题就凸显了。</description>
    </item>
    
    <item>
      <title>Sigflow: an automated and comprehensive pipeline for cancer genome mutational signature analysis</title>
      <link>/talk/bioconductor-asia-2020/</link>
      <pubDate>Fri, 09 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/talk/bioconductor-asia-2020/</guid>
      <description>Video link: https://www.youtube.com/watch?v=nzAxPDTznm4</description>
    </item>
    
  </channel>
</rss>
