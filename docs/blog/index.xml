<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Garrick Aden-Buie's Blog</title>
    <link>/blog/</link>
    <description>Recent posts from Garrick Aden-Buie</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 Dec 2021 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="/blog/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Complexheatmap合并figure legends以利用空间</title>
      <link>/blog/merge-complexheatmap-figure-legends/</link>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/merge-complexheatmap-figure-legends/</guid>
      <description>最近在绘制热图的时候遇到这样一个问题：
library(ComplexHeatmap) set.seed(1) m = matrix(sample(c(&amp;amp;quot;&amp;amp;quot;, &amp;amp;quot;AMP&amp;amp;quot;, &amp;amp;quot;DEL&amp;amp;quot;), 100, prob = c(0.8, 0.1, 0.1), replace = TRUE), nrow = 10) rownames(m) = paste0(&amp;amp;quot;A&amp;amp;quot;, 1:10) colnames(m) = paste0(&amp;amp;quot;B&amp;amp;quot;, 1:10) oncoPrint(m, top_annotation = HeatmapAnnotation(cbar = anno_oncoprint_barplot(), g = c(&amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;D&amp;amp;quot;))) 我们可以看到Alterations和g两组图例是按行排列的，这样需要整个图更多的宽度，如果按列排列个人感觉效果更好。 翻遍了complexHeatmap的文档和相关QA，没有搜索到相关的内容。比较接近的是设置legends_param列表，它可以操作单个图例 的排列，比如分类特别多，可以指定为几行几列这种。但无法排布多个legends的布局。
最终还是把问题抛给了开发者顾神（https://github.com/jokergoo/ComplexHeatmap/issues/850）。
下面是作者提供的简单解决办法：
ht = oncoPrint(m, top_annotation = HeatmapAnnotation(cbar = anno_oncoprint_barplot(), g = c(&amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;A&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;B&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;C&amp;amp;quot;, &amp;amp;quot;D&amp;amp;quot;))) draw(ht, merge_legends = TRUE) 感谢感谢！</description>
    </item>
    
    <item>
      <title>深度学习笔记摘录</title>
      <link>/blog/deep-learning-notes/</link>
      <pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/deep-learning-notes/</guid>
      <description>https://discoverml.github.io/simplified-deeplearning/
 神经网络中的非线性导致它的大部分代价函数变得非凸，对于非凸的损失函数，梯度下降算法不能保证收敛到全局最优，因此神经网络模型中的参数初始化是非常重要的，通常会将所有的权重初始化为一个较小的随机数，并且将偏置初始化为0或者较小的正值。
大多数现代神经网络使用极大似然原理，也就是说模型的损失函数和训练数据和模型分布间的交叉熵等价。
由于神经网络的特殊结构，导致神经网络必须注意的是损失函数的梯度必须有足够大的预测性，这样才能很好的指导算法的学习。很多输出单元都会包含一个指数函数，当变量取绝对值非常大的负值时函数会变得饱和（函数变得很“平”），函数梯度变得很小，而负的对数似然能够抵消输出单元中的指数效果。
对于实现最大似然估计的交叉熵损失函数通常需要使用正则化技术来避免过拟合的情况。
对于隐藏单元，logistic sigmoid函数只有在输入接近0的时候它们的梯度才比较大，因此不鼓励将它们作为前馈网络中的隐藏层，对于上文提到的输出层，对数似然损失函数抵消了sigmoid的饱和性，因此可以用在基于梯度学习的输出单元中。
双曲正切激活函数通常比sigmoid函数表现要好，它和sigmoid激活函数关系密切。
神经网络的架构（architecture）指网络的整体架构：神经网络需要多少单元以及单元之间的连接方式。大多数神经网络被组织成层的单元组，然后将这些层布置成链式结构，其中每一层是前一层的函数。在这个链式结构中，主要考虑的是网络的深度和每一层的宽度。通常来说更深的网络对每一层能够使用更少的单元数以及参数，并且泛化效果更好，但是它也更能难以训练。
万能近似定理（universal approximation theorem）表明一个前馈神经网络如果具有线性输出层和至少一层具有任何一种 “挤压”性质的激活函数（如logistic sigmoid激活函数）的隐藏层，只要给与网络足够数量的隐藏单元，它可以以任意精度来近似任何从一个有限维空间到另一有限维空间的Borel可测函数，前馈网络的导数也可以任意精度来近似函数的导数。万能近似定理说明了存在达到任意精度的这么一个神经网络，但是没有指出这个网络有多大。
在很多情况下，使用更深的模型能够减少表示期望函数所需的单元数量，并且可以减少泛化误差。增加网络的深度往往能够得到比增加宽度更加好的泛化能力。（宽度是指隐藏层的维度）
过拟合是无法彻底避免的，我们所能做的只是“缓解”以减少其风险。
L2参数正则化（也称为岭回归、Tikhonov正则）通常被称为权重衰减（weight decay)，是通过向目标函数添加一个正则项使权重更加接近原点。
将L2正则化的参数惩罚项Ω(θ)由权重衰减项修改为各个参数的绝对值之和，即得到L1正则化
将目标函数作二次泰勒展开近似
相比L2正则化，L1正则化会产生更稀疏的解。正则化策略可以被解释为最大后验（MAP）贝叶斯推断。L2 正则化相当于权重是高斯先验的MAP贝叶斯推断L1 正则化相当于权重是Laplace先验的MAP贝叶斯推。
作为约束的范数惩罚。较大的α将得到一个较小的约束区域，而较小的α将得到一个较大的约束区域。（重投影的显示约束对优化过程增加了一定的稳定性。例如当学习率较高时，很可能进入正反馈，即大的权重诱导大的梯度，使权重获得较大的更新。如果持续更新增加权重大小，则会使θ迅速增大而远离原点发生溢出。）
让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练，因此需要在有限的数据中创建假数据并添加到训练集中。数据集增强在对象识别领域是特别有效的方法。
 数据集的各种变换，如对图像的平移、旋转和缩放。 在输入层注入噪声，也可以看作数据集增强的一种方法（如去噪自编码器）。通过将随机噪声添加到输入再进行训练能够大大改善神经网络的健壮性。  噪声鲁棒性。
 将噪声加入到输入。在一般情况下,注入噪声远比简单地收缩参数强大,特别是噪声被添加到隐藏单元时会更加强大（如Dropout）。对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚。 将噪声加入到权重。这项技术主要用于循环神经网络。这可以被解释为关于权重的贝叶斯推断的随机实现。贝叶斯学习过程将权重视为不确定的,并且可以通过概率分布表示这种不确定性，向权重添加噪声是反映这种不确定性的一种实用的随机方法。（这种正则化鼓励参数进入权重小扰动对输出相对影响较小的参数空间区域。换句话说，它推动模型进入对权重小的变化相对不敏感的区域，找到的点不只是极小点，而且是由平坦区域所包围的极小点） 将噪声加入到输出。即显式地对标签上的噪声进行建模。正则化具有k个输出的softmax函数的模型。softmax函数值永远在0-1区间内而达不到0或1，标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。  多任务学习（参数共享）：从深度学习的观点看，底层的先验知识为：能解释数据变化的因素中，某些因素是跨多个任务共享的。
如果我们只要返回使验证集误差最低的参数，就可以获得验证集误差更低的模型。这种策略被称为提前终止（early stopping）。由于它的有效性和简单性，这可能是深度学习中最常用的正则化形式。（提前终止相当于L2正则化，提前终止为何具有正则化效果？其真正机制可理解为将优化过程的参数空间限制在初始参数值θ0的小邻域内。提前终止比L2正则化更具有优势，提前终止能自动确定正则化的正确量，而权重衰减需要进行多个不同超参数的训练实验。）
稀疏表示也是卷积神经网络经常用到的正则化方法。L1正则化会诱导稀疏的参数，使得许多参数为0；而稀疏表示是惩罚神经网络的激活单元，稀疏化激活单元。换言之，稀疏表示是使得每个神经元的输入单元变得稀疏，很多输入是0。
Bagging(bootstrap aggregating)是通过结合几个模型降低泛化误差的技术。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子,被称为模型平均(model averaging)。采用这种策略的技术被称为集成方法。
Bagging是一种允许重复多次使用同一种模型、训练算法和目标函数的方法。具体来说,Bagging涉及构造k个不同的数据集。每个数据集从原始数据集中重复采样构成，和原始数据集具有相同数量的样例。模型平均是一个减少泛化误差的非常强大可靠的方法。集成平方误差的期望随集成规模的增大而线性减少。
其他集成方法，如Boosting，通过向集成逐步添加神经网络，可以构建比单个模型容量更高的集成模型。
Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。但是Bagging方法涉及训练多个模型，并且在每个测试样本上评估多个模型。当每个模型都是一个大型神经网络时，Bagging方法会耗费很多的时间和内存。而Dropout则提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。
区别：Bagging所有模型都是独立的。Dropout所有模型共享参数，其中每个模型继承父神经网络参数的不同子集。参数共享使得在有限内存下表示指数级数量的模型变得可能。
Dropout优缺点：
 计算方便 适用广 相比其他正则化方法（如权重衰减、过滤器约束和稀疏激活）更有效 不适合宽度太窄的网络 不适合训练数据太小（如小于5000）的网络。训练数据太小时，Dropout没有其他方法表现好。 不适合非常大的数据集。数据集大的时候正则化效果有限（大数据集本身的泛化误差就很小），使用Dropout的代价可能超过正则化的好处。  有时候我们的真正损失函数，比如 0-1 分类误差并无法被有效的优化，此时我们会使用代理损失函数（surrogate loss function）来作为原来目标的替代，而且会带来好处。比如，正确分类类别的负对数似然通常用作 0-1 损失的替代，。负对数似然允许模型估计给定样本的类别的条件概率，能够输出期望最小分类误差所对应的类型。有些情况下，代理损失函数可以比原损失函数学到更多的东西，比如对数似然代替 0-1 分类误差函数时，当训练集上的误差达到0之后，测试集上的误差还可以持续下降，也就是说此时模型可以继续学习以拉开不同类别直接的距离以提高分类的鲁棒性。也就是说，代理损失函数从训练数据中学到了更多的东西。
使用整个训练集的优化方法被称为批量(batch) 或确定性（deterministic）梯度算法，他们会在每次更新参数时计算所有样本。通常，“批量梯度下降”指使用全部训练集，而“批量”单独出现时，指一组样本。每次只使用部分样本的方法被称为随机（stochastic）或者在线（online）算法。在线通常是指从连续产生的数据流（stream）中提取样本，而不是从一个固定大小的样本中遍历多次采样的情形。大多数深度学习算法介于两者之间，使用一个以上但不是全部的训练样本，传统上称这种方法为小批量（minibatch）或者小批量随机（minibatch stochastic）方法，现在统称为随机（stochastic）方法。
在凸优化问题中，优化问题可以简化为寻找一个局部极小值点，因为任何的局部极小值就是全局最小值。虽然有些凸函数底部是一个很大的平坦区域，并非单一的极值点，但是应用过程中实际上该区域中每一个极小值点都是一个可以接受的点。所以说，对于凸优化问题来说，找到任何形式的临界点，就是找到了一个不错的可行解。而对于非凸函数问题，比如神经网络问题，可能会存在很多的局部极小值点。</description>
    </item>
    
    <item>
      <title>torch入门：使用预训练模型预测图像分类</title>
      <link>/blog/learn-torch-predict-image-with-pretrained-model/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/learn-torch-predict-image-with-pretrained-model/</guid>
      <description>代码来源图书 deep-learning-with-pytorch。
from torchvision import models import torch dir(models) resnet = models.resnet101(pretrained=True) from torchvision import transforms preprocess = transforms.Compose([ transforms.Resize(256), transforms.CenterCrop(224), transforms.ToTensor(), transforms.Normalize( mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225] )]) from PIL import Image img = Image.open(&amp;amp;#34;../data/p1ch2/bobby.jpg&amp;amp;#34;) img_t = preprocess(img) batch_t = torch.unsqueeze(img_t, 0) resnet.eval() out = resnet(batch_t) out with open(&amp;amp;#39;../data/p1ch2/imagenet_classes.txt&amp;amp;#39;) as f: labels = [line.strip() for line in f.readlines()] _, index = torch.max(out, 1) percentage = torch.nn.functional.softmax(out, dim=1)[0] * 100 # 转换为概率 labels[index[0]], percentage[index[0]].</description>
    </item>
    
    <item>
      <title>yum升级git版本</title>
      <link>/blog/yum-upgrade-git/</link>
      <pubDate>Mon, 29 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/yum-upgrade-git/</guid>
      <description>安装新的RPM仓库：
sudo yum -y install https://packages.endpoint.com/rhel/7/os/x86_64/endpoint-repo-1.7-1.x86_64.rpm 更新Git：
sudo yum upgrade git  参考：https://www.seozen.top/centos-update-upgrade-git.html
 </description>
    </item>
    
    <item>
      <title>autoxgboost例子</title>
      <link>/blog/autoxgboost-example/</link>
      <pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/autoxgboost-example/</guid>
      <description>library(OpenML) library(autoxgboost) data = getOMLDataSet(31) GermanCredit = convertOMLDataSetToMlr(data) # reg_task &amp;amp;lt;- makeRegrTask(data = data_train, target = &amp;amp;#34;Share_Temporary&amp;amp;#34;) # reg_task &amp;amp;lt;- makeRegrTask(data = data_train, target = &amp;amp;#34;Share_Temporary&amp;amp;#34;) autoxgbparset.mixed = makeParamSet( makeDiscreteParam(&amp;amp;#34;booster&amp;amp;#34;, values = c(&amp;amp;#34;gbtree&amp;amp;#34;, &amp;amp;#34;gblinear&amp;amp;#34;, &amp;amp;#34;dart&amp;amp;#34;)), makeDiscreteParam(&amp;amp;#34;sample_type&amp;amp;#34;, values = c(&amp;amp;#34;uniform&amp;amp;#34;, &amp;amp;#34;weighted&amp;amp;#34;), requires = quote(booster == &amp;amp;#34;dart&amp;amp;#34;)), makeDiscreteParam(&amp;amp;#34;normalize_type&amp;amp;#34;, values = c(&amp;amp;#34;tree&amp;amp;#34;, &amp;amp;#34;forest&amp;amp;#34;), requires = quote(booster == &amp;amp;#34;dart&amp;amp;#34;)), makeNumericParam(&amp;amp;#34;rate_drop&amp;amp;#34;, lower = 0, upper = 1, requires = quote(booster == &amp;amp;#34;dart&amp;amp;#34;)), makeNumericParam(&amp;amp;#34;skip_drop&amp;amp;#34;, lower = 0, upper = 1, requires = quote(booster == &amp;amp;#34;dart&amp;amp;#34;)), makeLogicalParam(&amp;amp;#34;one_drop&amp;amp;#34;, requires = quote(booster == &amp;amp;#34;dart&amp;amp;#34;)), makeDiscreteParam(&amp;amp;#34;grow_policy&amp;amp;#34;, values = c(&amp;amp;#34;depthwise&amp;amp;#34;, &amp;amp;#34;lossguide&amp;amp;#34;)), makeIntegerParam(&amp;amp;#34;max_leaves&amp;amp;#34;, lower = 0, upper = 8, trafo = function(x) 2^x, requires = quote(grow_policy == &amp;amp;#34;lossguide&amp;amp;#34;)), makeIntegerParam(&amp;amp;#34;max_bin&amp;amp;#34;, lower = 2L, upper = 9, trafo = function(x) 2^x), makeNumericParam(&amp;amp;#34;eta&amp;amp;#34;, lower = 0.</description>
    </item>
    
    <item>
      <title>「转载」线性模型</title>
      <link>/blog/zhibei-linear-model/</link>
      <pubDate>Mon, 08 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/zhibei-linear-model/</guid>
      <description>本文主要是转载《指北》线性模型一节的内容，有删减。
在线性模型中，如果我们将两个共相关的变量放到同一个线性回归模型之中，那么这两个变量的系数估计的标准误都会扩大。下面我们展示下这个干扰过程：
set.seed(42) a &amp;amp;lt;- runif(100,min=0,max=10)&#43;rnorm(100) b &amp;amp;lt;- a*1.2&#43;rnorm(100) c &amp;amp;lt;- b*1.2&#43;rnorm(100) y &amp;amp;lt;- a&#43;b&#43;c&#43;rnorm(100) 上面我们由两个变量生成了一个新变量，然而这两个变量是相关的，此时我们进行回归分析：
summary(lm(y~a)) ## ## Call: ## lm(formula = y ~ a) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1929 -1.9935 -0.2595 1.9301 5.3011 ## ## Coefficients: ## Estimate Std. Error t value Pr(&amp;amp;gt;|t|) ## (Intercept) -0.14295 0.44101 -0.324 0.747 ## a 3.65619 0.07162 51.048 &amp;amp;lt;2e-16 *** ## --- ## Signif. codes: 0 &amp;amp;#39;***&amp;amp;#39; 0.</description>
    </item>
    
    <item>
      <title>读《指北》：多重假设检验记录与思考</title>
      <link>/blog/multiple-stats-testing-and-thinking/</link>
      <pubDate>Thu, 04 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/multiple-stats-testing-and-thinking/</guid>
      <description>本文有记录和思考2个方面，记录是根据《现代科研指北》的统计推断的一部分内容进行记录和学习，思考是在记录和学习的过程中添加一些自己的理解和思索。
首先谈谈为什么是这样的形式，而不是直接转载。对于个人而言，学习的本质是为了掌握知识，而不是记录知识。本文的主题是统计分析特别是组学统计分析中常用却甚少思考的一个基本点：多重比较与假设检验。我不知道有多少人像我一样，在有几年的数据处理经验之后，对这种比较基础的理论还一知半解。 现在，我们可以轻而易举的使用R的p.adj()对p值进行校正，甚至使用 Bioconductor的一些专门的包（如qvalue）进行处理。但我们真的了解它吗？你能简单地说出p.adj()中提供的方法原理和区别吗？如果你的目标是数据分析师，完成工作任务，仅仅作为赚钱养家的技能。ok，没必要深入学习，会调包调函数完全足够了。但如果我们有更高的追求，比如数据科学家， 无论是工业界还是学术界，那么我们必须对概念和问题产生自己的见解。
下面是《指北》中的一些内容。
多重比较的场景 科研里最常用的比较是两独立样本均值比较的t检验与评价单因素多水平影响的方差分析。t检验可以看作方差分析的特例，使用统计量t来比较而方差分析通常是用分类变量所解释的变异比上分类变量以外的变异去进行F检验。换句话讲，如果分类变量可以解释大部分响应变量的变异，我们就说这种分类变量对响应变量的解释有意义。
但是仅仅知道是否受影响是不够的，我们知道的仅仅是存在一种分类方法可以解释响应的全部变化，其内部也是均匀的，但不同分类水平间的差异我们并不知道，这就需要多重比较了。例如，当我们对两组数据做置信度0.05的t检验，我们遇到假阳性的概率为5%。但如果面对多组数据例如3组，进行两两比较的话就有\(3\choose2\)也就是3组对比，那么我们遇到假阳性的概率就为\(1-(1-0.05)^3\)，也就是14.3%，远高于0.05的置信度。组越多，两两对比就越多，整体上假阳性的概率就越来越大，到最后就是两组数据去对比，无论如何你都会检验出差异。
 值得思考的一个点是：这里一般提出的比较是多组，如A、B、C这3个组比较同一个指标的差异。而在组学分析中的比较是固定的A、B这2个组不同的指标的比较。它们能看作一样的事情吗？
  本质上是一样的，关键在对比的数量。我们可以把比较拆开为独立的1对1的比较。那么比较一次假设出现错误的概率是0.05，那么比对的数量越多，整体上的分析结果中出现一次错误的概率会越大于0.05。
 此外就方向而样，虽然我们都不承认零假设（要不然还做什么实验），但当我们默认设定为双尾检验时，假阳性就被默认发生在两个方向上了，这样的多重比较必然导致在其中一个方向上的错误率被夸大了。就影响大小而言，如果我们每次重复都选择效应最强的那一组，重复越多，预设的偏态就越重，换言之，我们的零假设因为重复实验的选择偏好而发生了改变。
 多重比较 那么多重比较如何应对这个问题呢？有两种思路，一种思路是我依旧采取两两对比，进行t检验，但p值的选取方法要修改，例如Bonferroni方法中就把p的阈值调整为进行多重比较的次数乘以计算得到的p值。如果我们关心的因素为2，那么计算得到的p值都要乘2来跟0.05或0.01的边界置信度进行比较；另一种思路则是修改两两比较所用的统计量，给出一个更保守的分布，那么得到p值就会更大。不论怎样，我们这样做都是为了降低假阳性，但同时功效不可避免的降低了。（有得必有失）
多重比较的方法类型包括单步法与逐步法。 单步法只考虑对零假设的影响而不考虑其他影响而逐步法则会考虑其他假设检验对单一检验的影响，例如可以先按不同分组均值差异从大到小排序，先对比第一个，有差异对比下一个，当出现无差异时停止对比；或者从下到大排序，有差异时停止对比，之后均认为有差异。此时还要注意一种特殊情况，因为F检验是从方差角度来考虑影响显著性与否，所以可能存在F检验显著但组间均值差异均不显著的情况，此时要考虑均值间线性组合的新均值的差异性（？？？）。不过，大多数情况我们只用考虑不同组间两两差异比较即可。
具体而言，单步法等方差多重比较最常见的是Tukey’s HSD方法，这是一个两两比较的方法，基于 studentized range 分布计算出q统计量，然后基于这个统计量进行两两间差异的假设检验。该方法适用于分组间等方差等数目的场景，如果分组内数目不同，需要用 Tukey-Kranmer 方法。该方法适用于两两比较，在分组数目相同时统计功效等同于从大到小排序的逐步法。
此外，还有些多重比较的方法在特定学科里也很常见。从总体控制错误率的角度，如果是两两比较应该选 Tukey’s HSD方法；如果侧重组间差异线性组合的均值用 Scheffe test；如果对比数指定了，功效按 Gabriel、GT2、DST、 Bonferroni顺序来选；如果是各分组都跟控制组比，应该选Dunnett法；如果各分组方差不相等，用GH，C，T3等方法。此外，如果打算保证每个比较中的置信水平，应该选 Tukey、 Scheffe、Dunnett法。
 远比想象中要复杂。
  多重检验 与多重比较类似的一个统计推断问题是多重检验问题。多重检验指的是同时进行多次假设检验的场景，其实多重比较可以看作多重检验在方差分析里的一个特例。
举例而言，我对两组样品（暴露组跟对照组）中每一个样品测定了10000个指标，每组有10个样品，那么如果我想知道差异有多大就需要对比10000次，具体说就是10000次双样本t检验。那么如果我对t检验的置信水平设置在95%，也就是5%假阳性，做完这10000次检验，我会期望看到500个假阳性，而这500个有显著差异的指标其实对分组不敏感也可以随机生成。假如真实测到了600个有显著差异的指标，那么如何区分其中哪些是对分组敏感？哪些又仅仅只是随机的呢？随机的会不会只有500个整呢？这个场景在组学技术与传感器技术采集高通量高维数据的今天变得越来越普遍。
这个问题在做经典科研实验时往往会忽略，深层次的原因是经典的科研实验往往是理论或经验主导需要进行检验的假说（注：经典实验比较的数目量也上不去）。例如，我测定血液中白血球的数目就可以知道你是不是处于炎症中，其背后是医学知识的支撑。然而，在组学或其他高通量实验中，研究实际是数据导向的，也就是不管有用没用反正我测了一堆指标，然后就去对比差异，然后就是上面的问题了，我们可能分不清楚哪些是真的相关，哪些又是随机出现的。
对于单次比较，当我们看到显著差异的p值脑子里想的是零假设为真时发生的概率，当我们置信水平设定在0.95而p值低于对应的阈值，那么我们应该拒绝零假设。但对比次数多了从概率上就会出现已经被拒绝的假设实际是错误的而你不知道是哪一个。整体错误率控制的思路就是我不管单次比较了，我只对你这所有的对比次数的总错误率进行控制。还是上面的例子，对于10000次假设检验我只能接受1个错误，整体犯错概率为0.0001，那么对于单次比较，其假阳性也得设定在这个水平上去进行假设检验，结果整体上错误率是控制住了，但对于单次比较就显得十分严格了。下面用一个仿真实验来说明：
# 随机数的10000次比较 set.seed(42) pvalue &amp;amp;lt;- NULL for (i in 1:10000){ a &amp;amp;lt;- rnorm(10) b &amp;amp;lt;- rnorm(10) c &amp;amp;lt;- t.test(a,b) pvalue[i] &amp;amp;lt;- c$p.value } # 看下p值分布 hist(pvalue) # 小于0.</description>
    </item>
    
    <item>
      <title>使用GenomicRanges操作区间数据</title>
      <link>/blog/operate-range-data-with-genomicranges/</link>
      <pubDate>Mon, 01 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/operate-range-data-with-genomicranges/</guid>
      <description>资料来源：Bioinformatics Data Skills
 准备 涉及的包：
 GenomicRanges - 表示和处理基因组区间 GenomicFeatures - 表示和处理基因组元件（基因、外显子等） Biostrings/BSgenome - 操作基因组序列 rtracklayer - 读入常见生物学数据文件（BED、GTF/GFF和WIG等）   从IRanges开始 基本用法 IRange是区间的基本数据构造：
library(IRanges) rng = IRanges(start = 1, end = 15) rng ## IRanges object with 1 range and 0 metadata columns: ## start end width ## &amp;amp;lt;integer&amp;amp;gt; &amp;amp;lt;integer&amp;amp;gt; &amp;amp;lt;integer&amp;amp;gt; ## [1] 1 15 15 构造出来的对象区间起始和终止都是闭合的，另外与R索引一致，都是从1开始。
 构造向量 向量是最常见的了：
x = IRanges(start=c(4, 7, 2, 20), end=c(13, 7, 5, 23)) x ## IRanges object with 4 ranges and 0 metadata columns: ## start end width ## &amp;amp;lt;integer&amp;amp;gt; &amp;amp;lt;integer&amp;amp;gt; &amp;amp;lt;integer&amp;amp;gt; ## [1] 4 13 10 ## [2] 7 7 1 ## [3] 2 5 4 ## [4] 20 23 4 给区间命名：</description>
    </item>
    
    <item>
      <title>机器学习分类性能常用一些指标</title>
      <link>/blog/measures-for-classification-in-ml/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/measures-for-classification-in-ml/</guid>
      <description>这篇文章的目的主要是记录一些分类器相关的度量指标。 从混淆矩阵中衍生出来的指标特别多，而我们中文与英文可能又存在多种对应 关系，这造成了记忆和理解上的困难。
 来源：https://zhuanlan.zhihu.com/p/111274912
 灵敏度与特异性 灵敏度 灵敏度（sensitivity），又称真阳性率，即实际有病，并且按照该诊断试验的标准被正确地判为有病的百分比。它反映了诊断试验发现病人的能力。
该研究中，根据手术病理结果有100例乳腺癌患者，但胸部扪诊只检测出其中80例患者。这说明该诊断试验只能发现80%的病人。
特异性 特异度（specificity），又称真阴性率，即实际没病，同时被诊断试验正确地判为无病的百分比。它反映了诊断试验确定非病人的能力。
例如有900例不是乳腺癌患者，但胸部扪诊只识别了其中的800例。特异性为89%。
比较 如果一项诊断试验的灵敏度比较低，那么会出现很多假阴性的患者。这会延误患者的就诊，影响病程发展和愈后，甚至导致患者过早死亡。
如果一项诊断试验的特异度比较低，那么会出现很多假阳性的患者。这样会浪费医疗资源、造成患者无端的恐慌和焦虑。
这两个指标主要可以通过ROC曲线同时查看。
 本节参考：https://www.mediecogroup.com/zhuanlan/lessons/229/  精度与召回率 首先需要说明的是这两者类似于ROC曲线，可以通过PR曲线同时进行观测。
精度 精度，precision。预测所关注的事件的结果中，预测正确的概率（共预测了 20 次，8 次正确，12 次错误）。
与Accuracy的区别：Accuracy不管正负类，算全部预测正确占总数的比率。而精度关注 预测正确的正类数目占全部正类数目的比率。
召回率/查全率 recall。对所有所关注的类型（一般就是正类），将其预测出的概率（共 10 个癌症患者，预测出 8 个）。
 本节参考：https://www.jianshu.com/p/dcf4deddff9f  </description>
    </item>
    
    <item>
      <title>此mutate非彼mutate</title>
      <link>/blog/this-mutate-is-not-that-mutate/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/this-mutate-is-not-that-mutate/</guid>
      <description>程序出乎意料，怎么办？
今天在星球圈里收到提问：
我对ddply()这个函数是不熟悉的，只知道hadley一个过时的包plyr里有一系列这样的函数。 所以我首先想到的是这位朋友用错了。不过我马上就排除了，这种问题是非常容易发现和处理的。
因此还是得动手实际检验一下这个问题在我的电脑上是否可以重复。
我们首先把数据导入进来：
library(scales) library(tidyverse) library(plyr) ts &amp;amp;lt;- openxlsx::read.xlsx(&amp;amp;quot;~/Downloads/示例数据.xlsx&amp;amp;quot;) head(ts) ## Name variable value ## 1 SLCO1B1 TCGA-44-2666 3.52916020 ## 2 GCGR TCGA-44-2666 0.08499940 ## 3 HTR3A TCGA-44-2666 0.05029628 ## 4 CA9 TCGA-44-2666 0.19814361 ## 5 TNFSF11 TCGA-44-2666 0.28202803 ## 6 FGB TCGA-44-2666 4.56223499 按照两种不同的方法生成结果：
out1 = ts %&amp;amp;gt;% ddply(., .(variable), transform, rescale = rescale(value)) %&amp;amp;gt;% arrange(variable, Name) head(out1) ## Name variable value rescale ## 1 ADRB2 TCGA-05-4390 2.</description>
    </item>
    
    <item>
      <title>ezcox v1.0.2 更新</title>
      <link>/blog/ezcox-v1-2-update/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/ezcox-v1-2-update/</guid>
      <description>针对@lijing-lin在GitHub的ezcox仓库提出的Fast way to add interaction terms?问题， 这两天闲暇时废了些脑细胞进行解决。同时也fix之前记录的一个遗留问题。
remotes::install_github(&amp;amp;quot;ShixiangWang/ezcox&amp;amp;quot;) 交互项支持 之前为了解决用户数据列名不符合的R命名规则，在源代码例自动对不合法名字进行了反撇号标记。 这会导致R的公式没法进行解析，例如sex:age会被判断为一个列名，R的公式没法解析它，因为 找不到数据中对应的sex:age列，所以会报错。
library(survival) library(ezcox) lung$ph.ecog &amp;amp;lt;- factor(lung$ph.ecog) ezcox(lung, covariates = c(&amp;amp;quot;age&amp;amp;quot;), controls = &amp;amp;quot;sex:ph.ecog&amp;amp;quot;) ## # A tibble: 5 × 12 ## Variable is_control contrast_level ref_level n_contrast n_ref beta HR ## &amp;amp;lt;chr&amp;amp;gt; &amp;amp;lt;lgl&amp;amp;gt; &amp;amp;lt;chr&amp;amp;gt; &amp;amp;lt;lgl&amp;amp;gt; &amp;amp;lt;lgl&amp;amp;gt; &amp;amp;lt;lgl&amp;amp;gt; &amp;amp;lt;dbl&amp;amp;gt; &amp;amp;lt;dbl&amp;amp;gt; ## 1 age FALSE age NA NA NA 0.00844 1.01 ## 2 age TRUE sex:ph.ecog0 NA NA NA -0.890 0.</description>
    </item>
    
    <item>
      <title>R6编程</title>
      <link>/blog/r6-programming/</link>
      <pubDate>Fri, 22 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/r6-programming/</guid>
      <description>原文来源：https://r6.r-lib.org/articles/Introduction.html
 R6包为R提供了封装的面向对象编程的实现（有时也被称为经典的面向对象编程）。它类似于R的引用类，但它更高效，不依赖于S4类和方法包。
与R中的许多对象不同，R6类的实例(对象)具有引用语义。R6类还支持：
 公共和私有方法 active bindings 跨包工作的继承(超类)  基础 下面是如何创建一个简单的R6类。public参数是一个项目列表，可以是函数和字段(非函数)。函数将被用作方法。
library(R6) Person &amp;amp;lt;- R6Class( &amp;amp;quot;Person&amp;amp;quot;, public = list( name = NULL, hair = NULL, initialize = function(name = NA, hair = nA) { self$name &amp;amp;lt;- name self$hair &amp;amp;lt;- hair self$greet() }, set_hair = function(val) { self$hair &amp;amp;lt;- val }, greet = function() { cat(paste0(&amp;amp;quot;Hello, my name is &amp;amp;quot;, self$name, &amp;amp;quot;.\n&amp;amp;quot;)) } ) ) 使用$new()进行初始化：
ann &amp;amp;lt;- Person$new(&amp;amp;quot;Ann&amp;amp;quot;, &amp;amp;quot;black&amp;amp;quot;) ## Hello, my name is Ann.</description>
    </item>
    
    <item>
      <title>跳过R包check系统使用无法显式载入DESCRIPTION的外部包</title>
      <link>/blog/skip-r-check-system/</link>
      <pubDate>Wed, 20 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/skip-r-check-system/</guid>
      <description>.attach_this &amp;amp;lt;- function() { if (!&amp;amp;#34;ggpubr&amp;amp;#34; %in% (.packages())) { tryCatch(eval(parse(text = &amp;amp;#34;library(ggpubr)&amp;amp;#34;)), error = function(e) { eval(parse(text = &amp;amp;#39;remotes::install_github(&amp;amp;#34;ggpubr&amp;amp;#34;)&amp;amp;#39;)) eval(parse(text = &amp;amp;#34;library(ggpubr)&amp;amp;#34;)) }) } } `%:::%` &amp;amp;lt;- function(pkg, fun, inherits = TRUE) { get(fun, envir = asNamespace(pkg), inherits = inherits ) } .attach_this() ggboxp &amp;amp;lt;- &amp;amp;#34;ggpubr&amp;amp;#34;%:::%&amp;amp;#34;ggboxplot&amp;amp;#34; args(ggboxp) </description>
    </item>
    
    <item>
      <title>rstatix使用fisher检验处理比例关系</title>
      <link>/blog/rstatix-fisher-test/</link>
      <pubDate>Tue, 19 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/rstatix-fisher-test/</guid>
      <description>Fisher检验R默认就可以做，但是只支持一次检验，为了更好地处理数据，这篇文章通过rstatix包的相关功能来 学习一些新知识。
library(rstatix)  本文的相关代码文档可以运行?rstatix::fisher_test()查看。
 比较2个比例值 生成数据：
xtab &amp;amp;lt;- as.table(rbind(c(490, 10), c(400, 100))) dimnames(xtab) &amp;amp;lt;- list( group = c(&amp;amp;quot;grp1&amp;amp;quot;, &amp;amp;quot;grp2&amp;amp;quot;), smoker = c(&amp;amp;quot;yes&amp;amp;quot;, &amp;amp;quot;no&amp;amp;quot;) ) xtab ## smoker ## group yes no ## grp1 490 10 ## grp2 400 100 进行比较：
fisher_test(xtab) ## # A tibble: 1 × 3 ## n p p.signif ## * &amp;amp;lt;dbl&amp;amp;gt; &amp;amp;lt;dbl&amp;amp;gt; &amp;amp;lt;chr&amp;amp;gt; ## 1 1000 8.77e-22 **** # 给出更多的比较信息 fisher_test(xtab, detailed = TRUE) ## # A tibble: 1 × 8 ## n estimate p conf.</description>
    </item>
    
    <item>
      <title>解决igraph使用optimap_函数报错：GLPK is not available, Unimplemented function call</title>
      <link>/blog/fix-igprah-glpk-error/</link>
      <pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/fix-igprah-glpk-error/</guid>
      <description>在使用igraph的测试用例时，发生GLPK相关的报错：
&amp;amp;gt; g &amp;amp;lt;- make_graph(&amp;amp;#34;Zachary&amp;amp;#34;) &amp;amp;gt; oc &amp;amp;lt;- cluster_optimal(g) Error in cluster_optimal(g) : At optimal_modularity.c:85 : GLPK is not available, Unimplemented function call GitHub的帖子#273对该问题进行了一些 积极的讨论，不过主要集中在MacOS系统上。而我要解决的是CentOS上的问题。
不过原理相通，加上cluster_optimal函数文档的描述，大体知道了CRAN不允许igraph团队 内置该库，所以从1.2.1版本后就移除了，因此需要安装包之前在相关系统上安装好该库， 这样该包安装的时候就能够编译相应的函数。否则，相应的函数使用就会报错。
一种解决的思路就是安装之前的版本，我尝试了下，发现一些编译报错。可能是旧代码存在一些 bug吧，所以只能用最新版本。
这样需要先用root权限安装库：
yum install glpk glpk-devel 然后再安装：
install.packages(&amp;amp;#34;igraph&amp;amp;#34;) 安装时间会比较长。
如果仔细观察的话，会发现g&#43;&#43;的命令中会指定加入-lglpk选项用于加入相关的库进行编译。
g&#43;&#43; -m64 -std=gnu&#43;&#43;11 -shared -L/usr/lib64/R/lib -Wl,-z,relro -o igraph.so AMD/Source/amd.o AMD/Source/amd_1.o AMD/Source/amd_2.o AMD/Source/amd_aat.o AMD/Source/amd_control.o AMD/Source/amd_defaults.o AMD/Source/amd_dump.o AMD/Source/amd_global.o AMD/Source/amd_info.o AMD/Source/amd_order.o AMD/Source/amd_post_tree.o AMD/Source/amd_postorder.o AMD/Source/amd_preprocess.o AMD/Source/amd_valid.o AMD/Source/amdbar.o CHOLMOD/Check/cholmod_check.o CHOLMOD/Check/cholmod_read.o CHOLMOD/Check/cholmod_write.o CHOLMOD/Cholesky/cholmod_amd.o CHOLMOD/Cholesky/cholmod_analyze.o CHOLMOD/Cholesky/cholmod_colamd.o CHOLMOD/Cholesky/cholmod_etree.o CHOLMOD/Cholesky/cholmod_factorize.o CHOLMOD/Cholesky/cholmod_postorder.</description>
    </item>
    
    <item>
      <title>「转载」可重复性危机</title>
      <link>/blog/reproducibility-issue/</link>
      <pubDate>Wed, 29 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/reproducibility-issue/</guid>
      <description>原文来自《现代科研指北》第3.3节。
 可重复性危机是当前科研领域里最大的问题，如果结论不可被重复验证，那么科学性就无从谈起。这里我们先讨论科研里通用假设检验的问题，然后讨论下规律性，最后介绍应对这个危机的可重复性研究与开放科学趋势。
3.3.1 零假设显著性检验（NHST） 零假设显著性检验（NHST）则是可重复性危机的核心。NHST 更常见的形式是 p 值，也就是在零假设成立的条件下某事件发生的概率。打个比方，我们从一个混合了黑白两种颜色小球的口袋里有放回的取一个小球三次，结果都是白球。这里我们设定零假设为黑球白球各一半，那么发生三次白球的概率为12.5%，这个不算极端。但是，如果有放回取了十次，结果还是都是白球，这情况发生概率大概为千分之一，这就比较极端了。在此基础上，我们有理由认为零假设不成立，而此时就需要一个阈值来帮助我们判断是否成立，目前学术界会认为5%或0.05的概率可以作为显著性与否的阈值。科研中我们会去计算零假设下出现当前实验结果的概率，也就是p值，如果低于阈值就可以认为是极端事件就拒绝零假设而高于阈值则认为零假设下可能发生。
当然，我们现在科研用的p值还会考虑零假设之外的备择假设，如果拒绝了零假设就转而接受备择假设。不过一旦引入备择假设就需要讨论错误，这里我们把决策出的结果分为阴性与阳性，而事实分为真假。零假设为真但接受了备择假设的情况，这就是假阳性或者第一类错误；零假设为假但没拒绝零假设就是假阴性或者第二类错误。这里我们可以看到第一类错误与前面设定的决策阈值密切相关，如果设定在5%或者0.05，那么我们就有5%的可能性做出了错误判断。第二类错误则与统计功效也就是真阴性的概率有关，通常会设定在80%，如果功效过低，例如10%，那么犯第二类错误的概率就很高。举例来说，我脚43码的但我不知道，这时去买鞋别人问我脚尺码我说44码的其实是错了，但不影响脚能穿进去，此时尺码的区别功效就不足。但如果我穿久了就会发现确实是大了，此时相当于我通过多次实验或采样提高了统计功效，但可能这个差别虽然明显但也不影响穿。通常NHST关心第一类错误，但设计实验会考虑第二类错误，通过提高样本量来提高统计功效。
p 值有多流行呢？根据 Jeff Leek 的估计，如果把 p 值当成一篇文献，那么其被引次数已经超过 300 万次了，当之无愧的史上被引次数之王，甩第二名一个数量级。原因其实很简单，p 值已经渗透到几乎所有学科的研究中了，特别是实验学科。可想而知，如果产生 p 值的 NHST 出了问题其影响力有多大。下面谈下 NHST 具体的问题：
如果一个假设对另一个假设来说很稀少，NHST 会在很低的条件概率下拒绝掉，然后那些稀少的事情在 NHST 里就成了无法被检验的事情。这个例子最早是 Cohen 提出用来说明人们在使用 NHST 时的问题。零假设是某人是中国人，备择假设是非中国人。我们知道张三是人大代表的概率大概是百万分之二，这是个事实。不过这个事实在零假设里很难发生，备择假设里也无法发生。零假设我们拒绝了某人是中国人，那么根据 NHST，他不是中国人。但问题是人大代表一定要是中国人，此时就会出现事实跟NHST矛盾的情况。在此类问题里，NHST 永远无法认定稀有事件，也就是功效永远不足，并会给出错误答案。
这个问题本质上是多数人在使用 p 值时搞混了条件概率，拿上面人大代表的例子来说，我们的假设 H0 在面对张三这个数据 D 时给出了拒绝 p(H0|D)=0p(H0|D)=0，这个决定是构建在假设 H0 成立时出现 D 的概率太低（即 p(D|H0)p(D|H0)）之上，也就是说 NHST 下，我们默认下面的概率是成立的：
p(D|H0)=p(H0|D)p(D|H0)=p(H0|D)如果你修过任何基础的统计学课程都会知道这两个概率之间差了一个贝叶斯公式。通过使用贝叶斯定理，在新数据出现后原有概率是要被更新而不是直接拒绝掉的。p 值给的是前者，要想知道随机生成的概率，需要知道零假设为真的概率。通俗点说就是 NHST 属于革命派，不认可就打倒你；贝叶斯属于改良派，用新的证据更新原有理论。这个问题的本质就是把假设下的事实与事实下的假设搞混导致的，这是 NHST 的一个致命问题，然而致命问题可不止这一个。
过去的一百年，测量方法的精度是在不断提高的，而精度其实又会影响研究结果，很不幸，也是通过 NHST 来进行的。其实 NHST 在实验物理学里用的还是好好的，例如我去检测一个物理量，只有数据出现在其理论预测下数值四五个标准差以外才会对理论产生实质作用。此时，测量精度越高，由于测量误差导致的对原有理论的冲击就会越少，因为物理学的预测性要比化学生物等学科要好不少且此时 NHST 检测的原有理论是比较真实的。但在其他学科，特别是心理学跟医学的控制实验里，在实验开始前你几乎就可以确定零假设是不成立的，要不然你也没必要分组，此时你去搞 NHST ，几乎一定可以找到差异，此时测量精度如果不断上升，那么你会识别到一系列差异，但这些差异的效果是无法体现在p值里的，p值可能非常小，但效应却属于明显但很微弱，这样的结果也许可以发表，但对实际问题的解决几乎没有贡献。更极端的情况是如果你加大了样本量来提高统计功效，你总是能发现差异的，也就是你的零假设里原有学科理论为真也是会被方法学进步给推翻的。总结下就是 Meehl 在60年代就提出的悖论：方法学的进步与增大样本数对于相对硬（理论根基深厚）的学科证伪是正面的，但对相对软（理论比较模糊）的学科则是弱化。方法学悖论的根基其实是应用学科与基础学科的矛盾，基础学科用 NHST 检验观察事实中的理论，但应用学科用 NHST 来检验的是实验设计预测下的事实，此时实验设计的那个假设与 NHST 的零假设并不对应，而 NHST 先天弱化零假设的问题就凸显了。</description>
    </item>
    
    <item>
      <title>处理glm.fit: fitted probabilities numerically 0 or 1 occurred</title>
      <link>/blog/process-glm-logistic-warning/</link>
      <pubDate>Sun, 26 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/process-glm-logistic-warning/</guid>
      <description>原文：https://www.statology.org/glm-fit-fitted-probabilities-numerically-0-or-1-occurred/
 在建立逻辑回归模型时遇到这个警告：
Warning message: glm.fit: fitted probabilities numerically 0 or 1 occurred 当拟合逻辑回归模型，且数据框中一个或多个观测值的预测概率与0或1难以区分时，会出现此警告。
值得注意的是，这是一个警告消息，而不是一个错误。即使你收到这个错误，你的逻辑回归模型仍然是合适的，但是可能值得分析原始数据框，看看是否有任何异常值导致此警告消息出现。
本教程将分享如何在实践中处理此警告消息。
重复警告 假设我们将logistic回归模型拟合到R中的以下数据框：
#create data frame df &amp;amp;lt;- data.frame(y = c(0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1), x1 = c(3, 3, 4, 4, 3, 2, 5, 8, 9, 9, 9, 8, 9, 9, 9), x2 = c(8, 7, 7, 6, 5, 6, 5, 2, 2, 3, 4, 3, 7, 4, 4)) #fit logistic regression model model &amp;amp;lt;- glm(y ~ x1 &#43; x2, data=df, family=binomial) #view model summary summary(model) Warning message: glm.</description>
    </item>
    
    <item>
      <title>解决conda报错：Module _sysconfigdata_x86_64_conda_linux_gnu not found</title>
      <link>/blog/conda-error-sysconfigdata-not-found/</link>
      <pubDate>Tue, 14 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/conda-error-sysconfigdata-not-found/</guid>
      <description>最新可能是受conda update conda的影响，发现使用conda涉及Python的操作时一直出现问题， 报错：
ModuleNotFoundError: No module named &amp;amp;#39;_sysconfigdata_x86_64_conda_linux_gnu&amp;amp;#39; 例如：
$ pip list Traceback (most recent call last): File &amp;amp;#34;~/miniconda3/bin/pip&amp;amp;#34;, line 7, in &amp;amp;lt;module&amp;amp;gt; from pip._internal.cli.main import main File &amp;amp;#34;~/miniconda3/lib/python3.9/site-packages/pip/_internal/cli/main.py&amp;amp;#34;, line 9, in &amp;amp;lt;module&amp;amp;gt; from pip._internal.cli.autocompletion import autocomplete File &amp;amp;#34;~/miniconda3/lib/python3.9/site-packages/pip/_internal/cli/autocompletion.py&amp;amp;#34;, line 10, in &amp;amp;lt;module&amp;amp;gt; from pip._internal.cli.main_parser import create_main_parser File &amp;amp;#34;~/miniconda3/lib/python3.9/site-packages/pip/_internal/cli/main_parser.py&amp;amp;#34;, line 8, in &amp;amp;lt;module&amp;amp;gt; from pip._internal.cli import cmdoptions File &amp;amp;#34;~/miniconda3/lib/python3.9/site-packages/pip/_internal/cli/cmdoptions.py&amp;amp;#34;, line 23, in &amp;amp;lt;module&amp;amp;gt; from pip._internal.cli.parser import ConfigOptionParser File &amp;amp;#34;~/miniconda3/lib/python3.9/site-packages/pip/_internal/cli/parser.py&amp;amp;#34;, line 12, in &amp;amp;lt;module&amp;amp;gt; from pip.</description>
    </item>
    
    <item>
      <title>Rcpp：什么时候使用Rcpp</title>
      <link>/blog/when-use-rcpp/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/when-use-rcpp/</guid>
      <description>来源：https://teuder.github.io/rcpp4everyone_en/010_Rcpp_merit.html
什么时候使用  后面的迭代依赖于前面的迭代的循环操作。 需要访问向量/矩阵的每个元素。 在循环中循环调用函数。 动态更改向量的大小。 需要高级数据结构和算法的操作。  怎么配置 除了Windows需要安装Rtools，其他系统中一般已经装好了。
如果我们要自定义C&#43;&#43;的配置，如更改编译器，需要使用到配置文件.R/Makevars。
下面是一个示例：
CC=/opt/local/bin/gcc-mp-4.7 CXX=/opt/local/bin/g&#43;&#43;-mp-4.7 CPLUS_INCLUDE_PATH=/opt/local/include:$CPLUS_INCLUDE_PATH LD_LIBRARY_PATH=/opt/local/lib:$LD_LIBRARY_PATH CXXFLAGS= -g0 -O2 -Wall MAKE=make -j4  包括编译器位置、头文件位置、动态库位置、编译参数等。
 安装Rcpp install.packages(&amp;amp;#34;Rcpp&amp;amp;#34;) </description>
    </item>
    
    <item>
      <title>Rcpp：基本用法</title>
      <link>/blog/rcpp-basic-usage/</link>
      <pubDate>Mon, 13 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/rcpp-basic-usage/</guid>
      <description>来源：https://teuder.github.io/rcpp4everyone_en/030_basic_usage.html
使用Rcpp函数只需要3步：
 编写Rcpp源代码。 编译代码。 执行函数。  编写Rcpp代码 下面是一个对向量求和的Rcpp函数：
//sum.cpp #include &amp;amp;lt;Rcpp.h&amp;amp;gt;using namespace Rcpp; // [[Rcpp::export]] double rcpp_sum(NumericVector v){ double sum = 0; for(int i=0; i&amp;amp;lt;v.length(); &#43;&#43;i){ sum &#43;= v[i]; } return(sum); } Rcpp函数定义格式 下面是定义一个Rcpp函数的基本格式：
#include&amp;amp;lt;Rcpp.h&amp;amp;gt;using namespace Rcpp; // [[Rcpp::export]] RETURN_TYPE FUNCTION_NAME(ARGUMENT_TYPE ARGUMENT){ //do something  return RETURN_VALUE; }  #include&amp;amp;lt;Rcpp.h&amp;amp;gt;：这个句子允许你使用Rcpp包定义的类和函数。 // [[Rcpp::export]]：这个句子下面定义的函数可以从R中访问。 你需要把这个句子附加到你想从R中使用的每个函数中。 using namespace Rcpp;：这个句子是可选的。但是如果你没有写这个句子， 你必须添加前缀Rcpp::来指定由Rcpp定义的类和函数。(例如：Rcpp::NumericVector) RETURN_TYPE FUNCTION_NAME(ARGUMENT_TYPE ARGUMENT){}：你需要指定函数和参数的数据类型。 return RETURN_VALUE;：如果函数将返回一个值，return语句是强制性的。 然而，如果你的函数没有返回值（即RETURN_TYPE是无效的），返回语句可以省略。  编译代码 函数Rcpp::sourceCpp()将编译你的源代码，并将定义的函数加载到R。
library(Rcpp) sourceCpp(&amp;amp;#39;sum.cpp&amp;amp;#39;) 使用函数 像正常R函数一样调用它就可以了。</description>
    </item>
    
    <item>
      <title>在VSCode中使用ipython执行代码</title>
      <link>/blog/use-ipython-exe-code-in-vscode/</link>
      <pubDate>Thu, 09 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/use-ipython-exe-code-in-vscode/</guid>
      <description>参考Stack Overflow这篇回答进行配置，将下面的内容加入settings.json.
&amp;amp;#34;python.terminal.launchArgs&amp;amp;#34;: [ &amp;amp;#34;-m&amp;amp;#34;, &amp;amp;#34;IPython&amp;amp;#34;, &amp;amp;#34;--no-autoindent&amp;amp;#34;, ], Python的数据科学社区都将编辑的重心放到了Jupyter notebook上，忽视了对.py文本本身的支持。 这个事情不知道是好是坏，但对于我这个R使用比较严重的人来说使用起来是不舒服的。 Jupyter的记录格式太笨重了，如果只是单独完成分析任务，代码加注释完全够用了。 R里面有RMarkdown格式，相比更加轻松。更重要的是，我在GitHub上2年前提的需求说 VSCode中Python文件不支持按块执行，现在也没有实现。只能找到上面的办法缓解这种不适。
我之前的帖子在：https://github.com/microsoft/vscode-python/issues/8851
我在issue中也看到了一些类似的提问和讨论，但皆不了了之。
其实还有一个痛点我没说，在执行完一句代码后，VSCode插件不会自动向下移动光标。
整个源代码编辑交互式分析的体验Python还没有编辑器能比过RStudio。</description>
    </item>
    
    <item>
      <title>云服务器清理病毒kdevtmpfsi</title>
      <link>/blog/clean-virus-in-cloud-server/</link>
      <pubDate>Wed, 08 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/clean-virus-in-cloud-server/</guid>
      <description>云服务器一段时间没使用就经常能发现存在挖矿病毒，也不知道怎么进来的。 也不知道各种云服务商干啥去了，一般能本地使用电脑还是不建议使用云服务器。
本文记录下清理病毒的大体流程，防止脑子总是忘记，又不是需要它。
查看占CPU任务名 这种一般top就可以了。
$ top -H top - 19:47:30 up 72 days, 4:01, 1 user, load average: 3.46, 3.17, 3.12 Threads: 357 total, 5 running, 352 sleeping, 0 stopped, 0 zombie %Cpu(s): 98.7 us, 1.3 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st KiB Mem : 1882016 total, 69388 free, 579844 used, 1232784 buff/cache KiB Swap: 2097148 total, 2002172 free, 94976 used. 1106516 avail Mem PID USER PR NI VIRT RES SHR S %CPU %MEM TIME&#43; COMMAND 24665 zd 20 0 710928 265496 672 R 98.</description>
    </item>
    
    <item>
      <title>mlr3（三）模型优化</title>
      <link>/blog/mlr3-model-optimization/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/mlr3-model-optimization/</guid>
      <description>来源：https://mlr3book.mlr-org.com/optimization.html
模型优化
机器学习算法为其超参数设置了默认值。不管怎样，用户需要更改这些超参数，以在给定的数据集上实现最佳性能。不建议手动选择超参数值，因为这种方法很少能获得最佳性能。为了证实所选超参数（=调优）的有效性，建议进行数据驱动的优化。为了优化机器学习算法，必须指定（1）搜索空间，（2）优化算法(又称调优方法)，（3）评估方法，即重采样策略，（4）性能度量。
总而言之，关于调优的小节介绍：
 进行经验超参数选择 选择优化算法 简洁地指定搜索空间 触发调优 自动调优  本小节还需要包mlr3tuning，这是一个支持超参数调优的扩展包。
特征选择
本章的第二部分介绍特征选择，也称为变量选择。特征选择是寻找数据相关特征子集的过程。执行选择的一些原因：
 增强模型的可解释性 加速模型拟合 通过降低数据中的噪声来提高学习性能  在本文中，我们主要集中在最后一个方面。有不同的方法来识别相关的特征。在特征选择的分章中，我们强调了三种方法：
 运用过滤算法根据分数独立地选择特征 根据变量重要性过滤选择特征 包装器方法迭代地选择特性以优化性能度量  注意，过滤器不需要学习器。变量重要性过滤器需要一个学习器，该学习器在训练时可以计算特征的重要性值。获得的重要值可用于数据子集，然后可用于训练学习器。包装器方法可以用于任何学习器，但需要对学习器进行多次训练。
嵌套重采样
为了更好地估计泛化性能并避免数据泄漏，外部（性能）和内部（调优/特征选择）重采样过程都是必要的。本章将讨论以下特点：
 嵌套重采样中的内重采样和外重采样策略 嵌套重采样的执行 执行重采样迭代的评估  本小节将提供如何实现嵌套重采样的说明，包括mlr3中的内重采样和外重采样。
超参数调优 超参数是机器学习模型的二阶参数，虽然在模型估计过程中往往没有明确优化，但会对模型的结果和预测性能产生重要影响。通常，超参数在训练模型之前是固定的。但是，由于模型的输出可能对超参数的规范很敏感，因此通常建议对哪些超参数设置可以产生更好的模型性能做出明智的决定。在许多情况下，超参数设置可能是预先选择的，但在将模型拟合到训练数据上之前，尝试不同的设置可能是有利的。这个过程通常被称为模型“调优”。
超参数调优是通过mlr3tuning扩展包支持的。下面是这个过程的说明：
mlr3tuning的核心是R6类：
TuningInstanceSingleCrit，TuningInstanceMultiCrit：这两个类描述调优问题并存储结果。
Tuner：这个类是调优算法实现的基类。
TuningInstance* 类 下面的小节审查了皮马印度糖尿病数据集上的简单分类树的优化。
library(&amp;amp;quot;mlr3verse&amp;amp;quot;) task = tsk(&amp;amp;quot;pima&amp;amp;quot;) print(task) ## &amp;amp;lt;TaskClassif:pima&amp;amp;gt; (768 x 9) ## * Target: diabetes ## * Properties: twoclass ## * Features (8): ## - dbl (8): age, glucose, insulin, mass, pedigree, pregnant, pressure, ## triceps 我们使用rpart中的分类树，并选择我们想要调优的超参数的子集。这通常被称为“调优空间”。</description>
    </item>
    
    <item>
      <title>mlr3（二）基础</title>
      <link>/blog/mlr3-basics/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/mlr3-basics/</guid>
      <description>来源：https://mlr3book.mlr-org.com/basics.html
本文将教你基本的mlr3知识，以及它的R6类和操作以用于机器学习。典型的机器学习工作流是这样的:
 Figure 1: 机器学习流程 source: https://mlr3book.mlr-org.com/images/ml_abstraction.svg  mlr3将数据封装在任务中，并将其分解为互不重叠的训练集和测试集。由于我们感兴趣的模型外推到新的数据，而不仅仅是记忆训练数据，独立的测试数据允许客观地评估模型的泛化。训练数据被提供给一个机器学习算法，在mlr3中我们称之为learner。learner利用训练数据建立输入特征与输出目标值之间关系的模型。然后使用该模型对测试数据进行预测，并将其与参考真值进行比较，以评估模型的质量。mlr3提供了许多不同的度量方法，根据预测值和实际值之间的差异来量化模型的执行情况。通常这是一个数字分数。
将数据分割为训练集和测试集、建立模型并对其进行评估的过程可能会重复多次，每次从原始数据中重新采样不同的训练集和测试集。多重重采样迭代允许我们对特定类型的模型获得更好、更一般化的性能估计，因为它是在不同的条件下测试的，而且由于数据重采样的特定方式，它不太容易产生偏差。
在许多情况下，这个简单的工作流不足以处理真实世界的数据，可能需要规范化（标准化）、缺失值的输入或特征选择。我们将在以后介绍更复杂的工作流程。
本文涵盖以下小主题：
任务：
任务用元信息封装数据，比如预测目标列的名称。我们将介绍如何：
 访问预定义的任务 指定一个任务类型 创建一个任务 使用任务的API工作 为任务的行和列分配角色 实施任务mutator 获取存储在任务中的数据  学习器
学习器封装机器学习算法来训练模型并对任务进行预测。它们由R和其他包提供。我们将介绍如何：
 访问随mlr3而来的分类和回归学习器集合，并检索特定的学习器 访问学习器的超参数值集并修改它们  如何修改和扩展学习器涵盖在补充高级技术部分。
训练和预测
关于训练和预测方法的部分说明了如何使用任务和学习器训练模型并对新数据集进行预测。特别地，我们将介绍如何：
 正确设置任务和学习器 为一项任务设置训练和测试分割（集） 在训练集上训练学习器以生成模型 生成测试集的预测 通过比较预测值和实际值来评估模型的性能  重采样
重采样是一种创建训练和测试分割（集）的方法。我们将介绍：
 访问和选择重采样策略 通过应用重采样实例化分割到训练集和测试集 执行重采样以获得结果  关于重采样的附加信息可以在嵌套重采样部分和模型优化一章中找到。
基准测试
基准测试用于比较不同模型的性能，例如不同学习器训练的模型，不同任务训练的模型，或不同重采样方法训练的模型。我们介绍如何
 创建一个基准设计 执行设计并汇总结果 将基准测试对象转换为重采样对象  二分类
二值分类是分类的一种特殊情况，预测的目标变量只有两个可能的值。在这种情况下，还需要考虑其他因素。特别是：
 ROC曲线和预测一个类和另一个类的阈值 阈值调整  在详细介绍如何使用mlr3进行机器学习之前，我们先简要介绍一下R6，因为它是R相对较新的一部分。mlr3严重依赖于R6，它提供的所有基本构造都是R6类：
 任务 task 学习器 learner 测量 measure 重采样 resamplings  快速R6入门介绍 R6是R最新的面向对象编程(OO)方言之一。它解决了R中早期OO实现的缺点，比如我们在mlr中使用的S3。如果你以前做过面向对象编程，那么R6应该很熟悉。我们关注的是R6的部分，你需要知道在这里使用mlr3。</description>
    </item>
    
    <item>
      <title>R小技巧：分组应用和排序去重的应用与比较</title>
      <link>/blog/r-tricks-remove-duplicates-after-ordering/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/r-tricks-remove-duplicates-after-ordering/</guid>
      <description>问题与方案 假设我们有这样一个数据集：
df &amp;amp;lt;- data.frame( c1 = c(&amp;amp;quot;a&amp;amp;quot;, &amp;amp;quot;a&amp;amp;quot;, &amp;amp;quot;a&amp;amp;quot;, &amp;amp;quot;b&amp;amp;quot;, &amp;amp;quot;b&amp;amp;quot;, &amp;amp;quot;c&amp;amp;quot;), c2 = c(1, 3, 2, 1, 4, 2) ) df out c1 c2 out 1 a 1 out 2 a 3 out 3 a 2 out 4 b 1 out 5 b 4 out 6 c 2 如果我们想保留每个c1分类和分类下的最大值，你会怎么操作？
思考一分钟。
如果使用惯了tidyverse套装，我们脑子里容易冒出来的是这样的解法：使用分组应用。
library(dplyr) df |&amp;amp;gt; group_by(c1) |&amp;amp;gt; summarize(c2 = max(c2, na.rm = TRUE)) out # A tibble: 3 × 2 out c1 c2 out &amp;amp;lt;chr&amp;amp;gt; &amp;amp;lt;dbl&amp;amp;gt; out 1 a 3 out 2 b 4 out 3 c 2 在数据不是特别大的时候，使用这种策略没有任何问题。但如果分组有成千上万，分组的时间代价就很高了。有没有其他的方式可以解决该问题呢？</description>
    </item>
    
    <item>
      <title>forestmodel给多水平变量添加整体p值</title>
      <link>/blog/forestmode-set-overall-pva-for-variable-with-multiple-levels/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/forestmode-set-overall-pva-for-variable-with-multiple-levels/</guid>
      <description>前段时间收到来信：
Hi Shixiang I am writing to you about the forestmodel package in R. Thank you so much for the wonderful package that you created. I was wondering if there is a way to display the wald test p-value which is important for variables that have more than two levels. I tried to work around the code but did not find a way out. Best Aniket 我不是作者，搞错了人，问我干啥呢～自个提问嘛
Hi Aniket, I am not the author of forestmodel, you can see from https://github.</description>
    </item>
    
    <item>
      <title>mlr3（一）快速入门</title>
      <link>/blog/mlr3-quickstart/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/mlr3-quickstart/</guid>
      <description>来源：https://mlr3book.mlr-org.com/quickstart.html
安装包：
install.packages(&amp;amp;quot;mlr3&amp;amp;quot;) 作为一个30秒的介绍性示例，我们将在虹膜数据集的前120行训练决策树模型，并对最后30行进行预测，测量训练模型的准确性。
library(&amp;amp;quot;mlr3&amp;amp;quot;) task = tsk(&amp;amp;quot;iris&amp;amp;quot;) learner = lrn(&amp;amp;quot;classif.rpart&amp;amp;quot;) # 为任务的一个子集（前120行）训练这个学习器的模型 learner$train(task, row_ids = 1:120) # 决策树模型 learner$model ## n= 120 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 120 70 setosa (0.41666667 0.41666667 0.16666667) ## 2) Petal.Length&amp;amp;lt; 2.45 50 0 setosa (1.00000000 0.00000000 0.00000000) * ## 3) Petal.Length&amp;amp;gt;=2.45 70 20 versicolor (0.00000000 0.71428571 0.28571429) ## 6) Petal.</description>
    </item>
    
    <item>
      <title>PR曲线与AUC</title>
      <link>/blog/pr-curve-and-auc-value/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/pr-curve-and-auc-value/</guid>
      <description>这里直接使用ROCR包提供的数据作为示例：
library(ROCR) data(ROCR.simple) pred &amp;amp;lt;- prediction(ROCR.simple$predictions, ROCR.simple$labels) perf &amp;amp;lt;- performance(pred,&amp;amp;quot;tpr&amp;amp;quot;,&amp;amp;quot;fpr&amp;amp;quot;) plot(perf) ## precision/recall curve (x-axis: recall, y-axis: precision) perf1 &amp;amp;lt;- performance(pred, &amp;amp;quot;prec&amp;amp;quot;, &amp;amp;quot;rec&amp;amp;quot;) plot(perf1, xlim = c(0, 1), ylim = c(0, 1)) 使用 PRROC 包获取PR AUC值并且绘图：
pr &amp;amp;lt;- PRROC::pr.curve(ROCR.simple$predictions, weights.class0 = ROCR.simple$labels, curve = TRUE) pr ## ## Precision-recall curve ## ## Area under curve (Integral): ## 0.7815038 ## ## Area under curve (Davis &amp;amp;amp; Goadrich): ## 0.7814246 ## ## Curve for scores from 0.</description>
    </item>
    
    <item>
      <title>《R语言数据科学导论》笔记</title>
      <link>/blog/note-for-r-data-science-intro/</link>
      <pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/note-for-r-data-science-intro/</guid>
      <description>原始资料来源：https://github.com/leovan/data-science-introduction-with-r
特征工程 特征工程是将原始数据转换成特征的过程。更通俗地说，特征工程就是人工设计模型的输入变量 x的过程。
主要分为：
 数据预处理 特征提取和选择 特征变换和编码 特征监控  数据预处理 对赃数据进行清洗，包括括缺失，噪声，不一致等等一系列问题数据。
剔除处理：
 样本去重。同一个ID出现多次重复记录。 特征去重。例如月收入和年收入，它们都是用于表征收入特征，关系只差常数倍。 常量特征剔除。即常量或方差近似为0的特征。caret包中的nearZeroVar()可以帮助我们识别该类特征。  缺失值处理：
 探索缺失值：mice包的md.pattern()，VIM包的aggr()/marginplot()。 处理：  删除法，可以直接使用na.omit()。 插补法，如果该特征对最终的预测结果影响较小，则我们可以直接删除该特征；相反如果该特征对预测结果影响较大，直接删除会对模型造成较大的影 响，此时我们需要利用其它的方法对该特征的缺失值进行填补。其中最简单的方式是利用均值，中位数或众数等统计量对其进行简单插补。这种插补方法是建立在完全随机缺失的前提假设下，同时会造成变量方差变小。    异常值是指样本中存在的同样本整体差异较大的数据。
分为2类：
采样是一种常见的预处理技术。
 随机采样。每个样本单位被抽中的概率相等，样本的每个单位完全独立，彼此间无一定的关联性和排斥性。 分层采样。将抽样单位按某种特征或某种规则划分为不同的层，然后从不同的层中独立、随机地抽取样本。从而保证样本的结构与总体的结构比较相近，从而提高估计的精度。可以利用sampling::strata()。 欠采样和过采样。我们经常会碰到不同分类的样本比例相差较大的问题，这种问题会对我们构建模型造成很大的影响，因此从数据角度出发，我们可以利用欠采样或过采样处理这种现象。可以利用ROSE::ovun.sample()。  特征变换和编码 无量纲化 通过归一化，我们可以消除不同量纲下的数据对最终结果的影响。
normalize &amp;amp;lt;- function(x) { # 计算极值 x_min &amp;amp;lt;- min(x) x_max &amp;amp;lt;- max(x) # 归一化 x_n &amp;amp;lt;- (x - x_min) / (x_max - x_min) # 将极值作为结果的属性 attr(x_n, &amp;amp;#39;min&amp;amp;#39;) &amp;amp;lt;- x_min attr(x_n, &amp;amp;#39;max&amp;amp;#39;) &amp;amp;lt;- x_max # 返回归一化后结果 x_n } 标准化。</description>
    </item>
    
    <item>
      <title>解决由于网络问题导致的stringi安装失败问题</title>
      <link>/blog/install-stringi-when-bad-acess-to-gh/</link>
      <pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/install-stringi-when-bad-acess-to-gh/</guid>
      <description>这是一个我每次重装R，或者在新的系统上使用R进行包安装，都会遇到的问题。
stringi是tidyverse的一个核心包，基本上必装。但由于gayhub经常访问有问题，这个包安装时所需要的依赖文件会下载不了。 解决的办法是手动下载，然后进行安装：
wget https://github.com/gagolews/stringi/archive/master.zip -O stringi.zip # 如果上面github的链接无法下载，尝试： # wget https://download.fastgit.org/gagolews/stringi/archive/master.zip -O stringi.zip unzip stringi.zip sed -i &amp;amp;#39;/\/icu..\/data/d&amp;amp;#39; stringi-master/.Rbuildignore R CMD build stringi-master R CMD INSTALL stringi*.tar.gz 参考：https://stackoverflow.com/questions/31942322/how-to-install-stringi-from-local-file-absolutely-no-internet-access#</description>
    </item>
    
    <item>
      <title>CentOS/Redhat R包使用最新的gcc编译</title>
      <link>/blog/use-new-gcc-on-centos-for-r/</link>
      <pubDate>Wed, 25 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/use-new-gcc-on-centos-for-r/</guid>
      <description>R包在Linux下编译不通过，原因是gcc版本太低怎么办？
一些有C&#43;&#43;代码的R包可能会用到一些新的C&#43;&#43;特性，需要C&#43;&#43;11或者C&#43;&#43;14。这个问题通常在CentOS/红帽系统上出现，因为系统稳定的要求，这个系列的系统它的C&#43;&#43;版本很低。 但请读者前往注意了别自己编译新版本的gcc，然后替换掉系统的。这种操作我试过几次，系统基本上就崩掉了。
正确的解决方式是安装独立的gcc，通过环境变量引用和使用它。
在Root用户下操作：
yum install centos-release-scl yum install devtoolset-9 然后在你使用R的用户下操作：
# If you use your non-root account to install packages,  # change /root to /home/your_id in the following command mkdir -p /root/.R vi /root/.R/Makevars 将下面的内容写入打开的文件，然后保存：
CXX11=/opt/rh/devtoolset-9/root/usr/bin/g&#43;&#43; -std=c&#43;&#43;11 -fPIC CXX14=/opt/rh/devtoolset-9/root/usr/bin/g&#43;&#43; -std=c&#43;&#43;14 -fPIC 如果没有root权限，可以通过conda来安装新版本的gcc,c&#43;&#43;等：
conda install gcc_linux-64 conda install gxx_linux-64  加上conda-forge通道也可以。
 另外可以一键安装常用的编译器：
conda install -c conda-forge compilers 安装完成后记得添加环境变量：
export CC=/path/to/anaconda/bin/x86_64-conda_cos6-linux-gnu-gcc export CXX=/path/to/anaconda/bin/x86_64-conda_cos6-linux-gnu-g&#43;&#43; 如果是R包编译，修改前面提到的Makevars文件即可。
如果使用的是miniconda，这个文件的内容可能就是这样的了：
CXX11=~/miniconda3/bin/x86_64-conda-linux-gnu-g&#43;&#43; -std=c&#43;&#43;11 -fPIC CXX14=~/miniconda3/bin/x86_64-conda-linux-gnu-g&#43;&#43; -std=c&#43;&#43;14 -fPIC 这样就可以愉快地安装包了：</description>
    </item>
    
    <item>
      <title>使用modules包来组织R的函数集合</title>
      <link>/blog/use-modules-to-organize-r-functions/</link>
      <pubDate>Tue, 24 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/use-modules-to-organize-r-functions/</guid>
      <description>接触过Python的朋友肯定对模块很熟悉，R的代码组织方式以包为主。但基于文件的模块形式也是可以实现的，modules 包提供了这种支持。
安装和使用 直接从CRAN下载即可：
install.packages(&amp;amp;#34;modules&amp;amp;#34;) 使用了解2个函数的使用就可以了。
一是import()，用于替换library()加载包。
&amp;amp;gt; library(modules) &amp;amp;gt; gp = import(&amp;amp;#39;ggplot2&amp;amp;#39;) Masking (modules:ggplot2): `Position` from: base &amp;amp;gt; args(gp$ggplot) function (data = NULL, mapping = aes(), ..., environment = parent.frame()) NULL &amp;amp;gt; args(ggplot) function (data = NULL, mapping = aes(), ..., environment = parent.frame()) NULL 这样我们可以直接使用这个函数，也可以通过gp这个对象去访问可用的函数。
如果不想要在全局直接访问包内的函数，在导入时设定attach=FALSE。
&amp;amp;gt; dp &amp;amp;lt;- import(dplyr, attach = FALSE) Masking (modules:dplyr): `intersect` from: base `setdiff` from: base `setequal` from: base `union` from: base &amp;amp;gt; select 错误: 找不到对象&amp;amp;#39;select&amp;amp;#39; &amp;amp;gt; dp$select function (.</description>
    </item>
    
    <item>
      <title>Shell：工具工作技巧备忘</title>
      <link>/blog/unix-working-tricks/</link>
      <pubDate>Wed, 18 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/unix-working-tricks/</guid>
      <description>远程传输和备份文件（夹） 使用scp无法续传，使用rsync更好。
rsync -avLr --progress huaxi:/remote_dir/ ./local_dir 从bed文件指定的区域提取bam结果 samtools view -@ 4 -bhL ../regions_to_check_in_bam.bed /public/home/zhaoxxx.bam &amp;amp;gt; xxx.bam </description>
    </item>
    
    <item>
      <title>RNAseq原理与分析课程笔记</title>
      <link>/blog/rnaseq-cource-notes/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/rnaseq-cource-notes/</guid>
      <description>很久之前在腾讯课堂购买了孟浩巍的RNAseq课程，讲解的挺细致完备的。这里主要是很少做这样的分析，所以一方面扫盲，一方面记录下其中重要的点（自己不知道的）或者值得拓展了解的知识点。
参考基因组与基因分布 拼装失败的地方大都是rRNA位置区域，与后面的内容对应。
rRNA 这里有几个点：
 一个DNA分子是比一个氨基酸大的 rRNA是集中分布的 rRNA在全部RNA中的占比很高，达80%左右。所以mRNA测序要么富集poly-A，要么去掉rRNA。  测序的概念 双端测序涉及到两端的转换。补个截图增强理解。</description>
    </item>
    
    <item>
      <title>Shell：使用数组</title>
      <link>/blog/shell-array/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/shell-array/</guid>
      <description>之前使用Shell编程很少使用到数组，最近尝试使用后发现它在某些情况下非常有用。 这里简单介绍如何生成和使用数组。
生成数组 我们只要将一组空格分隔的序列用括号括起来，就生成了一个数组。
array=(a b c d e f g) 使用数组 输出数组 使用${array[*]}或${array[@]}输出全部元素：
bash-3.2$ echo ${array[*]} a b c d e f g bash-3.2$ echo ${array[@]} a b c d e f g 在array前加#输出元素个数：
bash-3.2$ echo ${#array[*]} 7 获取元素将*改成索引，记得是从0开始：
bash-3.2$ echo ${array[1]} b bash-3.2$ echo ${array[2]} c 注意在非引用的情况下不需要美元符号和花括号，例如修改元素：
bash-3.2$ array[2]=ff bash-3.2$ echo ${array[2]} ff 迭代数组：
#for i in &amp;amp;#34;${!array[@]}&amp;amp;#34;; do  # printf &amp;amp;#34;%s\t%s\n&amp;amp;#34; &amp;amp;#34;$i&amp;amp;#34; &amp;amp;#34;${array[$i]}&amp;amp;#34; #done bash-3.2$ for i in &amp;amp;#34;${!</description>
    </item>
    
    <item>
      <title>关于区间突变概率计算理解的纠正</title>
      <link>/blog/adjustment-for-mutaiton-prob-in-a-region/</link>
      <pubDate>Thu, 12 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/adjustment-for-mutaiton-prob-in-a-region/</guid>
      <description>资料来源：https://www.huber.embl.de/msmb/Chap-Generative.html，我把资料中的情景更改了下。
 假设50个样本有100个碱基，单个位点有百分之一的突变率，汇总50个样本，我们期望在任何给定位置，50个样本观测到突变次数的总和服从具有参数为0.5的泊松分布。
一个随机的图如下：
现在让我们假设实际观测的图如下：
这种情况的概率是多少呢？
让我们先看一下至少出现7次的概率： $$ \begin{equation*} P(X\geq 7)= \sum_{k=7}^\infty P(X=k). \end{equation*} $$ 它可以转变为1减去出现少于7次的概率。
在R里面可以计算：
我们假设我们最后要计算得到的概率为$\epsilon$： $$ \begin{equation*} \epsilon=P(X\geq 7)=1-P(X\leq 6)\simeq10^{-6}. \end{equation*} $$
停！上面是错误的计算！
上面我们看了100个位置，寻找最大值并发现它是7，这种情况下出现7的概率比单个位置出现7的概率要大！
这里我们使用极端值分析，先对每个位置出现的次数排序，然后重新命名。
那么最大值出现至少7次的概率可以采用互补计算： $$ \begin{equation*} \begin{aligned} P(x_{(100)}\geq 7) &amp;amp;amp;=&amp;amp;amp;1-P(x_{(100)} \leq 6)\\
&amp;amp;amp;=&amp;amp;amp;1-P(x_{(1)}\leq 6 )\times P(x_{(2)}\leq 6 )\times \cdots \times P(x_{(100)} \leq 6 )\\
&amp;amp;amp;=&amp;amp;amp;1-P(x_1\leq 6 )\times P(x_2\leq 6 )\times \cdots \times P(x_{100}\leq 6 )\\
&amp;amp;amp;=&amp;amp;amp;1-\prod_{i=1}^{100} P(x_i \leq 6 ).\end{aligned} \end{equation*} $$ 由于100个事件是独立的，所以转换为： $$ \begin{equation*} \prod_{i=1}^{100} P(x_i \leq 6)= \left(P(x_i \leq 6)\right)^{100}= \left(1-\epsilon\right)^{100}.</description>
    </item>
    
    <item>
      <title>深度学习数学基础</title>
      <link>/blog/math-basics-for-dl/</link>
      <pubDate>Wed, 11 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/math-basics-for-dl/</guid>
      <description>本文来自《动手学习深度学习》附录。很久之前就摘录了，总觉得某一天用得上，不厌其烦地搬运了。 参考https://blog.csdn.net/xm_ovo/article/details/107536132一文对公式进行了正确的换行处理，以保持与简书相同的阅读效果。
  本文总结了本书中涉及的有关线性代数、微分和概率的基础知识。
线性代数 下面分别概括了向量、矩阵、运算、范数、特征向量和特征值的概念。
向量 本书中的向量指的是列向量。一个$n$维向量$\boldsymbol{x}$的表达式可写成
$$ \boldsymbol{x} = \begin{bmatrix} x_{1} \\
x_{2} \\
\vdots \\
x_{n} \end{bmatrix}, $$
其中$x_1, \ldots, x_n$是向量的元素。我们将各元素均为实数的$n$维向量$\boldsymbol{x}$记作$\boldsymbol{x} \in \mathbb{R}^{n}$或$\boldsymbol{x} \in \mathbb{R}^{n \times 1}$。
矩阵 一个$m$行$n$列矩阵的表达式可写成
$$ \boldsymbol{X} = \begin{bmatrix} x_{11} &amp;amp;amp; x_{12} &amp;amp;amp; \dots &amp;amp;amp; x_{1n} \\
x_{21} &amp;amp;amp; x_{22} &amp;amp;amp; \dots &amp;amp;amp; x_{2n} \\
\vdots &amp;amp;amp; \vdots &amp;amp;amp; \ddots &amp;amp;amp; \vdots \\
x_{m1} &amp;amp;amp; x_{m2} &amp;amp;amp; \dots &amp;amp;amp; x_{mn} \end{bmatrix}, $$</description>
    </item>
    
    <item>
      <title>ggplot构建新图形元素</title>
      <link>/blog/ggplot-build-new-object/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/ggplot-build-new-object/</guid>
      <description>2021/05/24 19:35
 原文：https://bookdown.org/rdpeng/RProgDA/building-new-graphical-elements.html
 由ggplot构造的数据图中的关键元素包括geoms（几何对象）和stats（统计变换）。事实上，ggplot2包具有强大的功能，允许用户制作各种有趣而丰富的数据图形。这些图形可以通过组合调用各种geom_*和stat_*函数(以及其他类函数)来实现。
为什么要构造新的图形元素？
  实现ggplot2目前不存在的特性。
  简化复杂的工作流。如果你总是发现自己在用重复的代码绘制类似的图形元素。
  创建新的geoms和stats可以简化代码，让用户轻松调整情节的某些元素，而不必每次都费劲地处理整个代码。
构造一个geom ggplot2中的新geoms继承自一个名为Geom的顶级类，并使用两步流程构造：
  ggproto()函数用于构造一个与新的geom对应的新类。这个新类指定了许多属性和函数，这些属性和函数描述了如何在图上绘制数据。
  geom_*函数被构造为标准函数。这个函数返回一个层，该层可以添加到使用ggplot()函数创建的plot中。
  新的geom类的基本设置如下所示：
GeomNEW &amp;amp;lt;- ggproto(&amp;amp;#34;GeomNEW&amp;amp;#34;, Geom, required_aes = &amp;amp;lt;a character vector of required aesthetics&amp;amp;gt;, default_aes = aes(&amp;amp;lt;default values for certain aesthetics&amp;amp;gt;), draw_key = &amp;amp;lt;a function used to draw the key in the legend&amp;amp;gt;, draw_panel = function(data, panel_scales, coord) { ## 返回一个grid grob对象的函数 ## 是绘图真正工作的地方 } ) 所需的美学映射应该很简单——例如，如果你的新geom生成了一种特殊的散点图，那么你可能需要x和y映射。美学映射的默认值可以包括绘图符号如形状、颜色等内容。</description>
    </item>
    
    <item>
      <title>grid 1：图形对象grobs</title>
      <link>/blog/grobs/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/grobs/</guid>
      <description>学习材料：https://bookdown.org/rdpeng/RProgDA/the-grid-package.html#grobs
grobs 是 grid 绘图系统中图形对象的表示，即 graphics &#43; objects = grobs
grobs 的创建通常使用以 Grob 作为后缀的函数们，包括circleGrob, linesGrob, polygonGrob, rasterGrob, rectGrob, segmentsGrob, legendGrob, xaxisGrob, and yaxisGrob 等等。 gridExtra 包定义了更多的图形对象。
下面是一个绘圆的示例：
library(grid) my_circle &amp;amp;lt;- circleGrob(x = 0.5, y = 0.5, r = 0.5, gp = gpar(col = &amp;amp;#34;gray&amp;amp;#34;, lty = 3)) 每个图形对象有自带的参数，比如圆有它的中心，半径。而不同的对象有相同的一些参数设定，包括颜色、线型、大小等等，这通过 gpar 函数进行设定。
再创建图形对象后，使用 grid.draw() 将图形绘制出来。
grid.draw(my_circle) 多个图形对象可以叠加，而且叠加后可以修改原来的图形对象。
my_circle &amp;amp;lt;- circleGrob(name = &amp;amp;#34;my_circle&amp;amp;#34;, x = 0.5, y = 0.5, r = 0.5, gp = gpar(col = &amp;amp;#34;gray&amp;amp;#34;, lty = 3)) grid.</description>
    </item>
    
    <item>
      <title>grid 2：视图 viewports</title>
      <link>/blog/viewports/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/viewports/</guid>
      <description>原文：https://bookdown.org/rdpeng/RProgDA/the-grid-package.html#viewports
 视图是什么 视图是绘图窗口，我们可以将其移进或移出，以方便使用grid包定制绘图。我们可以导航到其中一个视图，进行一些更改，然后弹出并导航到另一个视图。简而言之，视图提供了一种在绘图（可以想象为一个画板）的不同子空间（不同的层，如果了解ggplot2，采用图层概念理解它）中导航和工作的方法。
下面是一个例子，我们在整个图形的右上角绘制一个棒棒图：
# 默认，grid会初始化第一个视图 grid.draw(rectGrob()) # 创建一个小的视图 # 指定新视图的位置，X为0.5，Y为0.5，视图高宽都为0.5 sample_vp &amp;amp;lt;- viewport(x = 0.5, y = 0.5, width = 0.5, height = 0.5, just = c(&amp;amp;#34;left&amp;amp;#34;, &amp;amp;#34;bottom&amp;amp;#34;)) # 位置校正参数 # 导航视图：上面只是定义了一个视图对象 # 我们可以想象为一个大画板是一张白纸 # 我们上面准备了一张小的白纸 # 而 push 的目的就是把小的白纸放到大白纸的对应位置 # 接下来的绘图动作就是在小的白纸上进行的 pushViewport(sample_vp) grid.draw(roundrectGrob()) grid.draw(lollipop) # 弹出最上层的视图 popViewport()  棒棒图的绘制见「R」grid 图形对象 grobs一文末尾。
 视图的just参数 值得注意的是这里的just参数：它定义了新的视图是如何在旧（大） 的视图上摆放的。这里的c(&amp;amp;quot;left&amp;amp;quot;, &amp;amp;quot;bottom&amp;amp;quot;)指定了小的视图的左侧和下侧与其坐标单位0.5, 0.5对齐。我们看看修改下该参数的效果：
grid.draw(rectGrob()) sample_vp &amp;amp;lt;- viewport(x = 0.5, y = 0.5, width = 0.</description>
    </item>
    
    <item>
      <title>grid 3：图形坐标系统</title>
      <link>/blog/coords/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/coords/</guid>
      <description>绘图时，我们需要坐标才能准确地对要绘制的对象进行定位。在 grid 包中，有多种绘图坐标系统，选择一个合适的加以利用可以帮助我们更好地绘制图形。
坐标系统不同的单位：
  native 单位：根据数据值。
  n pc单位：将整个（笛卡尔）坐标系缩放为0到1范围。
  实际尺寸单位：包括英寸、厘米、毫米。
  例如，下面的绘图同时用到了前两者：
ex_vp &amp;amp;lt;- viewport(x = 0.5, y = 0.5, # npc 单位 just = c(&amp;amp;#34;center&amp;amp;#34;, &amp;amp;#34;center&amp;amp;#34;), height = 0.8, width = 0.8, xscale = c(0, 100), yscale = c(0, 10)) # 设定native单位 pushViewport(ex_vp) grid.draw(rectGrob()) grid.draw(circleGrob(x = unit(20, &amp;amp;#34;native&amp;amp;#34;), y = unit(5, &amp;amp;#34;native&amp;amp;#34;), # 根据native单位绘图 r = 0.1, gp = gpar(fill = &amp;amp;#34;lightblue&amp;amp;#34;))) grid.draw(circleGrob(x = unit(85, &amp;amp;#34;native&amp;amp;#34;), y = unit(8, &amp;amp;#34;native&amp;amp;#34;), r = 0.</description>
    </item>
    
    <item>
      <title>grid 4：gridExtra包</title>
      <link>/blog/grid-4-gridextra%E5%8C%85/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/grid-4-gridextra%E5%8C%85/</guid>
      <description>gridExtra包提供了grid系统的有用拓展，包括对grobs对象的操作方法和其他一些grobs对象。
grid.arrange()可以绘制多个对象：
library(gridExtra) grid.arrange(lollipop, circleGrob(), rectGrob(), lollipop, ncol = 2) 由于ggplot2也是基于grid系统的，所以我们可以使用该函数对ggplot对象组合排序：
time_vs_shots &amp;amp;lt;- ggplot(worldcup, aes(x = Time, y = Shots)) &#43; geom_point() player_positions &amp;amp;lt;- ggplot(worldcup, aes(x = Position)) &#43; geom_bar() grid.arrange(time_vs_shots, player_positions, ncol = 2) 使用layout_matrix选项可以进行更自定义控制：
grid.arrange(time_vs_shots, player_positions, layout_matrix = matrix(c(1, 2, 2), ncol = 3)) 包括图形填充：
grid.arrange(time_vs_shots, player_positions, layout_matrix = matrix(c(1, NA, NA, NA, 2, 2), byrow = TRUE, ncol = 3)) 使用tableGrob()函数，我们可以在图中嵌入表格。下面是一个略微复杂的例子：
worldcup_table &amp;amp;lt;- worldcup %&amp;amp;gt;% filter(Team %in% c(&amp;amp;#34;Germany&amp;amp;#34;, &amp;amp;#34;Spain&amp;amp;#34;, &amp;amp;#34;Netherlands&amp;amp;#34;, &amp;amp;#34;Uruguay&amp;amp;#34;)) %&amp;amp;gt;% group_by(Team) %&amp;amp;gt;% dplyr::summarize(`Average time` = round(mean(Time), 1), `Average shots` = round(mean(Shots), 1)) %&amp;amp;gt;% tableGrob() `summarise()` ungrouping output (override with `.</description>
    </item>
    
    <item>
      <title>Rcpp：数据结构</title>
      <link>/blog/rcpp-data-structure/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/rcpp-data-structure/</guid>
      <description>RObject类 RObject类在Rcpp类系统中占核心地位。虽然它不是面向用户的，但为接下来的所有类提供了公共的数据结构，它是构建Rcpp API的基础类。
每一个RObject类实例都封装了一个R对象，而每个对象在内部可以表示为一个SEXP：一个指向S表达式对象的指针。
基于它的用户可见（可使用）的类：
  IntegerVector对应整型向量。
  NumericVector对应数值向量。
  LogicalVector对应逻辑值向量。
  CharacterVector对应Character向量。
  GenericVector对应List类型的泛型向量。
  ExpressionVector对应表达式类型向量。
  RawVector对应raw类型向量。
  对于整型和数值型，我们还有IntegerMatrix和NumericMatrix对应R中的数值矩阵。
下面我们通过整型向量来了解它们。
IntegerVector类 模板函数as&amp;amp;lt;&amp;amp;gt;()用于从R转换到C&#43;&#43;，而wrap()函数的方向相反。
 实际大多数使用情况下，我们已经不需要显式地进行转换处理，该过程会在底层自动完成。
 示例：返回完美数 什么是完美数：
https://baike.baidu.com/item/完全数/370913?fromtitle=完美数&amp;amp;amp;fromid=871560&amp;amp;amp;fr=aladdin
代码：
library(Rcpp) library(inline) src &amp;amp;lt;- &amp;amp;#39; Rcpp::IntegerVector epn(4); epn[0] = 6; epn[1] = 14; epn[2] = 496; epn[3] = 8182; return epn; &amp;amp;#39; func &amp;amp;lt;- cxxfunction(signature(), src, plugin = &amp;amp;#34;Rcpp&amp;amp;#34;) 调用：
&amp;amp;gt; func() [1] 6 14 496 8182 示例：使用输入 求乘积。</description>
    </item>
    
    <item>
      <title>Unix：Bash编程</title>
      <link>/blog/unix-bash-programming/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/unix-bash-programming/</guid>
      <description>原文：https://seankross.com/the-unix-workbench/bash-programming.html
 数学 创建math.sh：
#!/usr/bin/env bash # File: math.sh expr 5 &#43; 2 expr 5 - 2 expr 5 \* 2 # 转义 expr 5 / 2 保存然后运行：
$ bash math.sh 7 3 10 2 ⚠️注意，bash使用整除法。求余使用%符号。
如果我们想要进行更为复杂的数学计算，使用bc命令。
创建文件bigmath.sh：
#!/usr/bin/env bash # File: bigmath.sh echo &amp;amp;#34;22 / 7&amp;amp;#34; | bc -l echo &amp;amp;#34;4.2 * 9.15&amp;amp;#34; | bc -l echo &amp;amp;#34;(6.5 / 0.5) &#43; (6 * 2.2)&amp;amp;#34; | bc -l 结果：</description>
    </item>
    
    <item>
      <title>Unix：Make</title>
      <link>/blog/unix-make/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/unix-make/</guid>
      <description>2021/05/26 22:01
 原文：https://seankross.com/the-unix-workbench/working-with-unix.html#make
 从前没有网页浏览器、文件浏览器、开始菜单或搜索栏。当有人启动电脑时，他们得到的是一个shell提示符，他们做的所有工作都是从这个提示符开始的。那时候，人们仍然喜欢共享软件，但总是存在软件应该如何安装的问题。make程序是解决这个问题的最好的尝试，make的优雅使它至今仍被广泛使用。make的指导设计目标是为了安装一些新软件:
  将所有的依赖下载到一个目录。
  cd进入目录。
  运行make。
  这是通过指定一个名为makefile的文件来实现的，该文件描述了不同文件和程序之间的关系 。除了安装程序之外，make对于自动创建文档也很有用。让我们构建一个makefile，它创建一个readme.txt文件，该文件自动填充有关当前目录的一些信息。
首先进入目录并创建一个文件：
▶ cd ~/Documents/test ▶ nano makefile 填入如下内容：
draft_journal_entry.txt: touch draft_journal_entry.txt 上面简单的makefile说明了一个规则，它的一般格式如下：
[target]: [dependencies...] [commands...] 在这个简单的示例中，我们创建了以draft_journal_entry.txt为目标的文件，该文件是作为命令的结果创建的。需要注意的是，目标下的任何命令都必须用Tab缩进。如果我们不使用Tab 来缩进命令，那么make将失败。让我们保存并关闭makefile，然后我们可以在控制台中运行以下命令：
▶ ls makefile 然后使用下面的操作查看make使用方式：
▶ make draft_journal_entry.txt touch draft_journal_entry.txt ▶ ls draft_journal_entry.txt makefile 在我们为draft_journal_entry.txt目标定义的规则下缩进的命令已经执行，所以现在draft_journal_entry.txt存在！让我们再次运行相同的make命令：
▶ make draft_journal_entry.txt make: `draft_journal_entry.txt&amp;amp;#39; is up to date. 因为目标文件已经存在，所以没有采取任何操作，相反，我们被告知，draft_journal_entry.txt的规则是“最新的”(没有什么需要做的)。
如果我们看一下我们之前草拟的一般规则格式，我们可以看到我们没有为该规则指定任何依赖项。依赖项是目标在构建时所依赖的文件。如果自上次为目标运行make以来，依赖项已经更新，则目标不是“最新的”。这意味着下次为该目标运行make时将运行该目标的命令。通过这种方式，对依赖项的更改被合并到目标中。为了避免不必要地运行命令，这些命令只在依赖项改变时运行，或者当目标根本不存在时运行。
让我们更新makefile，以包含自动生成的readme.txt。首先，让我们添加文件：
▶ echo &amp;amp;#34;1. 2017-06-15-In-Boston&amp;amp;#34; &amp;amp;gt; toc.txt makefile修改为：
draft_journal_entry.txt: touch draft_journal_entry.</description>
    </item>
    
    <item>
      <title>使用Circle-Map Realign鉴定环形DNA</title>
      <link>/blog/use-circle-map-for-detecting-eccdna/</link>
      <pubDate>Mon, 09 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/use-circle-map-for-detecting-eccdna/</guid>
      <description>2021/07/23 19:39
 原文：https://github.com/iprada/Circle-Map/wiki/Tutorial:-Identification-of-circular-DNA-using-Circle-Map-Realign
 这是一个教程，一步一步地解释如何从原始数据(fastq文件)到一个可解释的，标签分离的bed文件指示染色体坐标的DNA环。为了制作教程，我们模拟了Illumina读取来自人类基因组未知区域的环形DNA。本教程的目的是使用Circle-Map来提取环形DNA的来源。
依赖   GNU/Linux
  BWA
  samtools
  Circle-Map
  conda install -c bioconda bwa samtools pip3 install Circle-Map pip3 install biopython==1.77 # 不安装这个版本目前使用会报错 第一步：准备和下载数据 下载原始数据 直接克隆仓库：
git clone https://github.com/iprada/Circle-Map 下载和准备参考基因组 wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz gunzip -d hg38.fa.gz bwa index hg38.fa samtools faidx hg38.fa 第二步：比对到参考基因组 cd Circle-Map/tutorial/ bwa mem -q ~/data/refs/hg38.fa unknown_circle_reads_1.fastq unknown_circle_reads_2.fastq &amp;amp;gt; unknown_circle.sam 这就产生了一个SAM文件，其中包含了关于reads在哪里以及如何与基因组对齐的信息。
  我们使用 -q 选项（query name sorted）为BWA中的拆分读对比对分配独立的映射质量分数 。这可以改进Circle-Map概率模型中断点图权值的估计。</description>
    </item>
    
    <item>
      <title>Jupyter Notebook 插件配置</title>
      <link>/blog/jupyter-notebook-plugin-configs/</link>
      <pubDate>Sun, 08 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/jupyter-notebook-plugin-configs/</guid>
      <description>安装命令：
pip3 install -U jupyter_contrib_nbextensions jupyter contrib nbextension install --user pip3 install -U jupyter_nbextensions_configurator jupyter nbextensions_configurator enable --user pip3 install -U qgrid # 动态操作 DataFrame jupyter nbextension enable --py --sys-prefix qgrid jupyter nbextension enable --py --sys-prefix widgetsnbextension 如果要修改主题：
pip3 install -U jupyterthemes 网址：https://github.com/dunovank/jupyter-themes
激活的插件：</description>
    </item>
    
    <item>
      <title>Slurm使用技巧</title>
      <link>/blog/slurm-tricks/</link>
      <pubDate>Fri, 06 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/slurm-tricks/</guid>
      <description>Slurm的官方文档在：https://slurm.schedmd.com/。
这篇文章记录下我在工作中使用经常用到，但是一般的初级文档不会涉及的内容、操作。
系统节点资源概览 一般我们输入sinfo，只能看到非常精简的节点信息：
$ sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST cn up infinite 1 mix cn03 cn up infinite 2 alloc cn[01-02] cn up infinite 15 idle cn[04-18] fat up infinite 1 idle fat01 gpu* up infinite 1 idle gpu01 我们可以添加几个选项来获得一个更为完备的信息：
$ sinfo -Nel Fri Aug 6 15:54:17 2021 NODELIST NODES PARTITION STATE CPUS S:C:T MEMORY TMP_DISK WEIGHT AVAIL_FE REASON cn01 1 cn allocated 80 80:1:1 771000 0 1 (null) none cn02 1 cn allocated 80 80:1:1 771000 0 1 (null) none cn03 1 cn mixed 80 80:1:1 771000 0 1 (null) none cn04 1 cn idle 80 80:1:1 771000 0 1 (null) none cn05 1 cn idle 80 80:1:1 771000 0 1 (null) none cn06 1 cn idle 80 80:1:1 771000 0 1 (null) none cn07 1 cn idle 80 80:1:1 771000 0 1 (null) none cn08 1 cn idle 80 80:1:1 771000 0 1 (null) none cn09 1 cn idle 80 80:1:1 771000 0 1 (null) none cn10 1 cn idle 80 80:1:1 771000 0 1 (null) none cn11 1 cn idle 80 80:1:1 771000 0 1 (null) none cn12 1 cn idle 80 80:1:1 771000 0 1 (null) none cn13 1 cn idle 80 80:1:1 771000 0 1 (null) none cn14 1 cn idle 80 80:1:1 771000 0 1 (null) none cn15 1 cn idle 80 80:1:1 771000 0 1 (null) none cn16 1 cn idle 80 80:1:1 771000 0 1 (null) none cn17 1 cn idle 80 80:1:1 771000 0 1 (null) none cn18 1 cn idle 80 80:1:1 771000 0 1 (null) none fat01 1 fat idle 80 80:1:1 296649 0 1 (null) none gpu01 1 gpu* idle 40 40:1:1 772000 0 1 (null) none 全部的选项介绍在文档sinfo中。</description>
    </item>
    
    <item>
      <title>LeetCode：0001-两数之和</title>
      <link>/blog/leetcode-0001-two-sum/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/leetcode-0001-two-sum/</guid>
      <description>难度：简单。
参考：
  https://leetcode-cn.com/problems/two-sum
  https://books.halfrost.com/leetcode/ChapterFour/0001~0099/0001.Two-Sum/
  代码仓库：https://github.com/ShixiangWang/LeetCode
问题 给定一个整数数组nums和一个整数目标值target，请你在该数组中找出 和为目标值target 的那两个整数，并返回它们的数组下标。
你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。
示例：
输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] &#43; nums[1] == 9 ，返回 [0, 1] 。 题解 顺序扫描数组，对每一个元素，在map中找能组合给定值的另一半数字，如果找到了，直接返回2个数字的下标即可。如果找不到，就把这个数字存入map中，等待扫到“另一半”数字的时候，再取出来返回结果。
这种解法将数据扫描一遍必然得到结果，所以时间复杂度是O(n)。
Go package main import &amp;amp;#34;fmt&amp;amp;#34; func twoSum(nums []int, target int) []int { m := make(map[int]int) for i := 0; i &amp;amp;lt; len(nums); i&#43;&#43; { another := target - nums[i] if _, ok := m[another]; ok { return []int{m[another], i} } m[nums[i]] = i } return nil } func main() { fmt.</description>
    </item>
    
    <item>
      <title>旧文存档</title>
      <link>/blog/blog-archives/</link>
      <pubDate>Thu, 05 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/blog-archives/</guid>
      <description> GitHub Home 博客：https://shixiangwang.github.io/home 语雀：https://www.yuque.com/shixiangwang 我来：https://www.wolai.com/sCHoE8HyUFnDEuCrth7PZG  </description>
    </item>
    
    <item>
      <title>RMarkdown template from my reference blog</title>
      <link>/blog/questions-answers-and-reprexes/</link>
      <pubDate>Tue, 16 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>/blog/questions-answers-and-reprexes/</guid>
      <description>I stumbled into a niche YouTube genre: Web Development online instructors (a.k.a content creators) challenge each other to “CSS Battles”.
Turns out they are fascinating videos where experienced programmers talk and fumble their way through their coding.
As they talked through their thought processes, I thought about the questions we ask when our code goes wrong and the answers we get when we reprex.
 CSS Battle with The CSS King     I challenged The CSS King to a CSS Battle on YouTube   I’ve thought a lot about this video since I watched it.</description>
    </item>
    
  </channel>
</rss>