[
  {
    "objectID": "posts/2023-04-08-yiyi-with-book-by-y/index.html",
    "href": "posts/2023-04-08-yiyi-with-book-by-y/index.html",
    "title": "晒晒娃和Y叔R书的合照",
    "section": "",
    "text": "买了一本Y叔最新出的新书，出版社的张编辑又送了一本，这里晒一下崽崽和图书的照片。\n\n\n摄影自我媳妇"
  },
  {
    "objectID": "posts/2023-05-12-start-a-new-research-direction/index.html",
    "href": "posts/2023-05-12-start-a-new-research-direction/index.html",
    "title": "Start a new research direction",
    "section": "",
    "text": "Copied from a slide (Liulab?).\n\n\nIs the new direction is important?\n\nI can explain its importance to laymen\nThere are basic science, translational and industry interests\nI am willing to devote the next 5-10 years working on it\n\nDo we have any advantage working on this?\nWhat can we actually do (expertise)?"
  },
  {
    "objectID": "posts/2023-05-02-random/index.html",
    "href": "posts/2023-05-02-random/index.html",
    "title": "随想",
    "section": "",
    "text": "越长大，突然明白这个世界都是在玩一场资源分配的游戏。\n卷不是目的，只是无能为力。\n超脱者永远是少数。\n所谓，求仙不得仙，只能老实做人。"
  },
  {
    "objectID": "posts/2023-06-14-use-just/index.html",
    "href": "posts/2023-06-14-use-just/index.html",
    "title": "使用 Just 重复构建",
    "section": "",
    "text": "Just（https://just.systems/）是一个命令行运行容器，它只是简单的运行你输入的命令，以方便重复构建。\n它没有 Make 那么复杂，这真是我喜欢它的原因。很久之前我很想学习 Make/Makefile，但一直未能成行。Just 让我意识到，我可能根本不需要学习 Make，也能开始很好地学会使用构建。\n我把 Just 的文档看完后发现，其实核心要点就在 README 的图里，而复杂的特征需求往后再说，可能一直都用不上。\n\n\n来源：https://github.com/casey/just"
  },
  {
    "objectID": "posts/2023-05-31-scrna-seq_online-00/index.html",
    "href": "posts/2023-05-31-scrna-seq_online-00/index.html",
    "title": "scRNA-seq_online 00：包准备",
    "section": "",
    "text": "参考资料：https://hbctraining.github.io/scRNA-seq_online/"
  },
  {
    "objectID": "posts/2023-05-31-scrna-seq_online-00/index.html#安装包",
    "href": "posts/2023-05-31-scrna-seq_online-00/index.html#安装包",
    "title": "scRNA-seq_online 00：包准备",
    "section": "安装包",
    "text": "安装包\n安装 CRAN 包：\ninstall.packages(\"BiocManager\")\nBiocManager::install(c(\"tidyverse\", \"Matrix\", \"RCurl\", \"scales\", \"cowplot\", \"Seurat\", \"metap\"))\n安装 Bioc 包：\n# options(BioC_mirror=\"https://mirrors.tuna.tsinghua.edu.cn/bioconductor\")\nBiocManager::install(c(\"AnnotationHub\", \"ensembldb\", \"multtest\", \"glmGamPoi\"))"
  },
  {
    "objectID": "posts/2023-05-31-scrna-seq_online-00/index.html#测试包的安装",
    "href": "posts/2023-05-31-scrna-seq_online-00/index.html#测试包的安装",
    "title": "scRNA-seq_online 00：包准备",
    "section": "测试包的安装",
    "text": "测试包的安装\n\nlibrary(Seurat)\n## Attaching SeuratObject\nlibrary(tidyverse)\n## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n## ✔ dplyr     1.1.2     ✔ readr     2.1.4\n## ✔ forcats   1.0.0     ✔ stringr   1.5.0\n## ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n## ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n## ✔ purrr     1.0.1\n## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## ✖ dplyr::filter() masks stats::filter()\n## ✖ dplyr::lag()    masks stats::lag()\n## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nlibrary(Matrix)\n## \n## Attaching package: 'Matrix'\n## \n## The following objects are masked from 'package:tidyr':\n## \n##     expand, pack, unpack\nlibrary(RCurl)\n## \n## Attaching package: 'RCurl'\n## \n## The following object is masked from 'package:tidyr':\n## \n##     complete\nlibrary(scales)\n## \n## Attaching package: 'scales'\n## \n## The following object is masked from 'package:purrr':\n## \n##     discard\n## \n## The following object is masked from 'package:readr':\n## \n##     col_factor\nlibrary(cowplot)\n## \n## Attaching package: 'cowplot'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     stamp\nlibrary(AnnotationHub)\n## Loading required package: BiocGenerics\n## \n## Attaching package: 'BiocGenerics'\n## \n## The following objects are masked from 'package:lubridate':\n## \n##     intersect, setdiff, union\n## \n## The following objects are masked from 'package:dplyr':\n## \n##     combine, intersect, setdiff, union\n## \n## The following objects are masked from 'package:stats':\n## \n##     IQR, mad, sd, var, xtabs\n## \n## The following objects are masked from 'package:base':\n## \n##     anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n##     colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n##     get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n##     match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n##     Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n##     table, tapply, union, unique, unsplit, which.max, which.min\n## \n## Loading required package: BiocFileCache\n## Loading required package: dbplyr\n## \n## Attaching package: 'dbplyr'\n## \n## The following objects are masked from 'package:dplyr':\n## \n##     ident, sql\nlibrary(ensembldb)\n## Loading required package: GenomicRanges\n## Loading required package: stats4\n## Loading required package: S4Vectors\n## \n## Attaching package: 'S4Vectors'\n## \n## The following objects are masked from 'package:Matrix':\n## \n##     expand, unname\n## \n## The following objects are masked from 'package:lubridate':\n## \n##     second, second&lt;-\n## \n## The following objects are masked from 'package:dplyr':\n## \n##     first, rename\n## \n## The following object is masked from 'package:tidyr':\n## \n##     expand\n## \n## The following objects are masked from 'package:base':\n## \n##     expand.grid, I, unname\n## \n## Loading required package: IRanges\n## \n## Attaching package: 'IRanges'\n## \n## The following object is masked from 'package:lubridate':\n## \n##     %within%\n## \n## The following objects are masked from 'package:dplyr':\n## \n##     collapse, desc, slice\n## \n## The following object is masked from 'package:purrr':\n## \n##     reduce\n## \n## Loading required package: GenomeInfoDb\n## Loading required package: GenomicFeatures\n## Loading required package: AnnotationDbi\n## Loading required package: Biobase\n## Welcome to Bioconductor\n## \n##     Vignettes contain introductory material; view with\n##     'browseVignettes()'. To cite Bioconductor, see\n##     'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n## \n## \n## Attaching package: 'Biobase'\n## \n## The following object is masked from 'package:AnnotationHub':\n## \n##     cache\n## \n## \n## Attaching package: 'AnnotationDbi'\n## \n## The following object is masked from 'package:dplyr':\n## \n##     select\n## \n## Loading required package: AnnotationFilter\n## \n## Attaching package: 'ensembldb'\n## \n## The following object is masked from 'package:dplyr':\n## \n##     filter\n## \n## The following object is masked from 'package:stats':\n## \n##     filter"
  },
  {
    "objectID": "posts/2023-05-31-scrna-seq_online-00/index.html#查看会话信息",
    "href": "posts/2023-05-31-scrna-seq_online-00/index.html#查看会话信息",
    "title": "scRNA-seq_online 00：包准备",
    "section": "查看会话信息",
    "text": "查看会话信息\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nRunning under: macOS Big Sur ... 10.16\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib\nLAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib\n\nlocale:\n[1] zh_CN.UTF-8/zh_CN.UTF-8/zh_CN.UTF-8/C/zh_CN.UTF-8/zh_CN.UTF-8\n\nattached base packages:\n[1] stats4    stats     graphics  grDevices utils     datasets  methods  \n[8] base     \n\nother attached packages:\n [1] ensembldb_2.22.0        AnnotationFilter_1.22.0 GenomicFeatures_1.50.4 \n [4] AnnotationDbi_1.60.2    Biobase_2.58.0          GenomicRanges_1.50.2   \n [7] GenomeInfoDb_1.34.9     IRanges_2.32.0          S4Vectors_0.36.2       \n[10] AnnotationHub_3.6.0     BiocFileCache_2.6.1     dbplyr_2.3.2           \n[13] BiocGenerics_0.44.0     cowplot_1.1.1           scales_1.2.1           \n[16] RCurl_1.98-1.12         Matrix_1.5-4.1          lubridate_1.9.2        \n[19] forcats_1.0.0           stringr_1.5.0           dplyr_1.1.2            \n[22] purrr_1.0.1             readr_2.1.4             tidyr_1.3.0            \n[25] tibble_3.2.1            ggplot2_3.4.2           tidyverse_2.0.0        \n[28] SeuratObject_4.1.3      Seurat_4.3.0           \n\nloaded via a namespace (and not attached):\n  [1] utf8_1.2.3                    spatstat.explore_3.2-1       \n  [3] reticulate_1.28               tidyselect_1.2.0             \n  [5] RSQLite_2.3.1                 htmlwidgets_1.6.2            \n  [7] grid_4.2.2                    BiocParallel_1.32.6          \n  [9] Rtsne_0.16                    munsell_0.5.0                \n [11] codetools_0.2-19              ica_1.0-3                    \n [13] future_1.32.0                 miniUI_0.1.1.1               \n [15] withr_2.5.0                   spatstat.random_3.1-5        \n [17] colorspace_2.1-0              progressr_0.13.0             \n [19] filelock_1.0.2                knitr_1.43                   \n [21] rstudioapi_0.14               ROCR_1.0-11                  \n [23] tensor_1.5                    listenv_0.9.0                \n [25] MatrixGenerics_1.10.0         GenomeInfoDbData_1.2.9       \n [27] polyclip_1.10-4               bit64_4.0.5                  \n [29] parallelly_1.36.0             vctrs_0.6.2                  \n [31] generics_0.1.3                xfun_0.39                    \n [33] timechange_0.2.0              R6_2.5.1                     \n [35] bitops_1.0-7                  spatstat.utils_3.0-3         \n [37] cachem_1.0.8                  DelayedArray_0.24.0          \n [39] promises_1.2.0.1              BiocIO_1.8.0                 \n [41] gtable_0.3.3                  globals_0.16.2               \n [43] goftest_1.2-3                 rlang_1.1.1                  \n [45] splines_4.2.2                 rtracklayer_1.58.0           \n [47] lazyeval_0.2.2                spatstat.geom_3.2-1          \n [49] BiocManager_1.30.20           yaml_2.3.7                   \n [51] reshape2_1.4.4                abind_1.4-5                  \n [53] httpuv_1.6.11                 tools_4.2.2                  \n [55] ellipsis_0.3.2                RColorBrewer_1.1-3           \n [57] ggridges_0.5.4                Rcpp_1.0.10                  \n [59] plyr_1.8.8                    progress_1.2.2               \n [61] zlibbioc_1.44.0               prettyunits_1.1.1            \n [63] deldir_1.0-9                  pbapply_1.7-0                \n [65] zoo_1.8-12                    SummarizedExperiment_1.28.0  \n [67] ggrepel_0.9.3                 cluster_2.1.4                \n [69] magrittr_2.0.3                data.table_1.14.8            \n [71] scattermore_1.1               lmtest_0.9-40                \n [73] RANN_2.6.1                    ProtGenerics_1.30.0          \n [75] fitdistrplus_1.1-11           matrixStats_0.63.0           \n [77] hms_1.1.3                     patchwork_1.1.2              \n [79] mime_0.12                     evaluate_0.21                \n [81] xtable_1.8-4                  XML_3.99-0.14                \n [83] gridExtra_2.3                 compiler_4.2.2               \n [85] biomaRt_2.54.1                KernSmooth_2.23-21           \n [87] crayon_1.5.2                  htmltools_0.5.5              \n [89] later_1.3.1                   tzdb_0.4.0                   \n [91] DBI_1.1.3                     MASS_7.3-60                  \n [93] rappdirs_0.3.3                cli_3.6.1                    \n [95] parallel_4.2.2                igraph_1.4.3                 \n [97] pkgconfig_2.0.3               GenomicAlignments_1.34.1     \n [99] sp_1.6-0                      plotly_4.10.1                \n[101] spatstat.sparse_3.0-1         xml2_1.3.4                   \n[103] XVector_0.38.0                digest_0.6.31                \n[105] sctransform_0.3.5             RcppAnnoy_0.0.20             \n[107] spatstat.data_3.0-1           Biostrings_2.66.0            \n[109] rmarkdown_2.21                leiden_0.4.3                 \n[111] uwot_0.1.14                   restfulr_0.0.15              \n[113] curl_5.0.0                    shiny_1.7.4                  \n[115] Rsamtools_2.14.0              rjson_0.2.21                 \n[117] lifecycle_1.0.3               nlme_3.1-162                 \n[119] jsonlite_1.8.4                viridisLite_0.4.2            \n[121] fansi_1.0.4                   pillar_1.9.0                 \n[123] lattice_0.21-8                KEGGREST_1.38.0              \n[125] fastmap_1.1.1                 httr_1.4.6                   \n[127] survival_3.5-5                interactiveDisplayBase_1.36.0\n[129] glue_1.6.2                    png_0.1-8                    \n[131] BiocVersion_3.16.0            bit_4.0.5                    \n[133] stringi_1.7.12                blob_1.2.4                   \n[135] memoise_2.0.1                 irlba_2.3.5.1                \n[137] future.apply_1.11.0"
  },
  {
    "objectID": "posts/2023-05-31-scrna-seq_online-00/index.html#阅读",
    "href": "posts/2023-05-31-scrna-seq_online-00/index.html#阅读",
    "title": "scRNA-seq_online 00：包准备",
    "section": "阅读",
    "text": "阅读\n接下来就可以阅读以下两份非常好的材料了解单细胞的预备知识。\n\nIntroduction to single-cell RNA-seq\nGeneration of count matrix"
  },
  {
    "objectID": "posts/2023-05-23-nextflow-singularity-no-space-left/index.html",
    "href": "posts/2023-05-23-nextflow-singularity-no-space-left/index.html",
    "title": "nextflow 运行 singularity no space left",
    "section": "",
    "text": "运行 nextflow 时报错没有空间：\nError executing process &gt; 'NFCORE_CIRCDNA:CIRCDNA:CNVKIT_BATCH (N87-TR)'\n\nCaused by:\n  Process `NFCORE_CIRCDNA:CIRCDNA:CNVKIT_BATCH (N87-TR)` terminated with an error exit status (255)\n\nCommand executed:\n\n  cnvkit.py \\\n      batch \\\n      N87-TR.bam \\\n       \\\n      --reference GRCh38_cnvkit_filtered_ref.cnn \\\n      --processes 4 \\\n      --method wgs\n\n  cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n  \"NFCORE_CIRCDNA:CIRCDNA:CNVKIT_BATCH\":\n      cnvkit: $(cnvkit.py version | sed -e \"s/cnvkit v//g\")\n  END_VERSIONS\n\nCommand exit status:\n  255\n\nCommand output:\n  (empty)\n\nCommand error:\n  INFO:    Converting SIF file to temporary sandbox...\n  FATAL:   while extracting /data3/wsx/nf-core-circdna-dev/workflow/../singularity-images/depot.galaxyproject.org-singularity-cnvkit-0.9.9--pyhdfd78af_0.img: root filesystem extraction failed: extract command failed: WARNING: passwd file doesn't exist in container, not updating\n  WARNING: group file doesn't exist in container, not updating\n  WARNING: Skipping mount /etc/hosts [binds]: /etc/hosts doesn't exist in container\n  WARNING: Skipping mount /etc/localtime [binds]: /etc/localtime doesn't exist in container\n  WARNING: Skipping mount proc [kernel]: /proc doesn't exist in container\n  WARNING: Skipping mount /data3/wsx/miniconda3/var/singularity/mnt/session/tmp [tmp]: /tmp doesn't exist in container\n  WARNING: Skipping mount /data3/wsx/miniconda3/var/singularity/mnt/session/var/tmp [tmp]: /var/tmp doesn't exist in container\n  WARNING: Skipping mount /data3/wsx/miniconda3/var/singularity/mnt/session/etc/resolv.conf [files]: /etc/resolv.conf doesn't exist in container\n\n  Write on output file failed because No space left on device\n\n  FATAL ERROR:writer: failed to write file /image/root/.singularity.d/startscript\n  Parallel unsquashfs: Using 48 processors\n  29783 inodes (35465 blocks) to write\n\n  : exit status 1\n\nWork dir:\n  /data3/wsx/nxf_wgs/work/e9/902f1684d95d41a61ae28ef3e529b3\n\nTip: when you have fixed the problem you can continue the execution adding the option `-resume` to the run command line\n我查了下 /tmp 缺失很少或没有了，尝试删除我自己产生的临时目录再次运行还是报错。原因是 singularity 镜像在 /tmp 目录下解压空间不够导致的。\n实际上我在 ~/.bashrc 下是设置过临时目录和singularity的缓存的：\nexport TMPDIR=$HOME/TEMPDIR\nexport TEMP=$HOME/TEMPDIR\nexport TMP=$HOME/TEMPDIR\n\nexport NXF_SINGULARITY_CACHEDIR=$HOME/NXF_singularity\nexport SINGULARITY_TMPDIR=$HOME/singularity-env\nexport SINGULARITY_CACHEDIR=$HOME/singularity-env\n这就相当纳闷了。在 Github 发现一个帖子，讨论说这个问题是 nextflow 没有正常地读取和设置 singularity 的临时目录磁盘挂载。\n想一想，确实是这个问题。为了确定，我在报错的 work 目录下查看和调试了 .command.run 文件，当设置挂载后确实是可以正常工作的。\n于是按照下面进行了 nextflow.config 的配置：\nsingularity {\nenabled = true\nautoMounts = true\nrunOptions = '-B $SINGULARITY_TMPDIR:/tmp -B $SINGULARITY_TMPDIR:/scratch'\n//Used to allow Singularity to access bashrc variables\nenvWhitelist = ['SINGULARITY_TMPDIR']\n}\n还是不行，后面对比时猛然发现在 nextflow.config 中 autoMounts 的写法是 singularity.autoMounts = true。 所以我添加了前缀：\nsingularity.autoMounts = true\nsingularity.runOptions = '-B $SINGULARITY_TMPDIR:/tmp'\nsingularity.envWhitelist = ['SINGULARITY_TMPDIR']\n这样就没有问题了。"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!"
  },
  {
    "objectID": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html",
    "href": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html",
    "title": "ChatGPT 帮助我理解 Rust 的所有权",
    "section": "",
    "text": "这篇文章以我和 ChatGPT 的问答进行。"
  },
  {
    "objectID": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#怎么理解rust的所有权",
    "href": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#怎么理解rust的所有权",
    "title": "ChatGPT 帮助我理解 Rust 的所有权",
    "section": "怎么理解rust的所有权？",
    "text": "怎么理解rust的所有权？\nRust的所有权是其独特的内存管理系统，用于动态内存分配和释放。它基于以下原则：\n\n每个值在任何时刻只能有一个所有者。\n所有权只能转移，不能共享。\n默认情况下，所有权的转移是按值而不是按引用进行的。\n\n这种所有权系统有助于避免内存泄漏、空引用和数据竞争等常见的错误。在Rust中，通过为每个值分配一个所有者来跟踪内存中的数据是如何使用的，每一个所有者离开作用域时，它拥有的值都会被销毁。因此，Rust所有权系统能够自动管理内存，而且无需额外的垃圾回收机制。这一特性让Rust成为一个高效、安全和可靠的编程语言。需要注意的是，所有权机制对于Rust的初学者来说是一个相对复杂的概念，需要一定的理解和实践才能更好地应用它并编写出高质量的Rust代码。"
  },
  {
    "objectID": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#怎么理解-所有权的主要目的就是为了管理堆数据",
    "href": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#怎么理解-所有权的主要目的就是为了管理堆数据",
    "title": "ChatGPT 帮助我理解 Rust 的所有权",
    "section": "怎么理解 所有权的主要目的就是为了管理堆数据？",
    "text": "怎么理解 所有权的主要目的就是为了管理堆数据？\nRust中的所有权确实是用于管理堆数据的，因为堆数据是在运行时动态分配的，并且在内存中存在一定的问题，如内存泄漏、数据竞争等。因此需要一种机制来管理堆数据并解决这些问题。所有权机制可以确保在动态分配内存的情况下，内存得到正确的释放，每个heap上的数据有且仅有一个所有者对其进行管理，从而避免了数据竞争和其他常见的错误。\n此外，Rust还使用采取堆管理的方式为其他类型的数据分配内存。例如，当一个Vector或String类型增长时，它们必须重新分配和复制所有的数据，同时保证新的数据具有唯一的所有者。这使得Rust的动态内存分配和释放机制非常高效、可靠和安全，在许多应用程序场景中可以提供非常好的性能。"
  },
  {
    "objectID": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#为什么所有权不是管理栈数据的呢",
    "href": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#为什么所有权不是管理栈数据的呢",
    "title": "ChatGPT 帮助我理解 Rust 的所有权",
    "section": "为什么所有权不是管理栈数据的呢？",
    "text": "为什么所有权不是管理栈数据的呢？\n所有权的设计之所以没有涉及栈，是因为栈上的数据是随着当前作用域结束而自动释放的。栈上的数据的分配和释放是编译时完成的，编译器可以自己进行管理，从而不需要特别的内存管理机制。当在栈上分配数据时，其生命周期的结束是在编译器处理的代码块退出前的最后一个语句执行后就会自动发生。此时编译器会自动释放它们的空间，没有必要使用所有权。因此，所有权机制主要应用于需要在运行时动态分配内存的场景，例如堆上存储的大型数据结构、字符串、闭包等。相比于栈上的数据，堆上的数据需要更加灵活和高级的处理方式，所以所有权体系的设计针对的是动态内存的管理，而不是栈上的处理。"
  },
  {
    "objectID": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#所有权造成的操作障碍",
    "href": "posts/2023-03-29-chatgpt-help-me-understand-rust-ownership/index.html#所有权造成的操作障碍",
    "title": "ChatGPT 帮助我理解 Rust 的所有权",
    "section": "所有权造成的操作障碍",
    "text": "所有权造成的操作障碍\n由于所有权的问题，Rust 变得安全有效，但对于开发者也造成了一些理解和操作问题。 例如，同一个堆数据（栈数据不会受到影响，因为它们在编译时已经被决定了，堆数据在运行时是可变的，所以才会有内存问题）不能指向两个不同的变量，当新的变量出现时，所有权被转移，之前的变量就无效无法使用了。\nfn main() {\n    let s1 = String::from(\"hello\");\n    let s2 = s1;\n\n    println!(\"{}, world!\", s1);\n}\n\n为了生成两个拷贝，需要用到克隆。\nfn main() {\n    let s1 = String::from(\"hello\");\n    let s2 = s1.clone();\n\n    println!(\"s1 = {}, s2 = {}\", s1, s2);\n}\n这个 Rust 的 trait 设计也有关系（我理解就是实际的实现）\n\nRust 有一个叫做 Copy trait 的特殊注解，可以用在类似整型这样的存储在栈上的类型上。如果一个类型实现了 Copy trait，那么一个旧的变量在将其赋值给其他变量后仍然可用。\nRust 不允许自身或其任何部分实现了 Drop trait 的类型使用 Copy trait。如果我们对其值离开作用域时需要特殊处理的类型使用 Copy 注解，将会出现一个编译时错误。\n那么哪些类型实现了 Copy trait 呢？你可以查看给定类型的文档来确认，不过作为一个通用的规则，任何一组简单标量值的组合都可以实现 Copy，任何不需要分配内存或某种形式资源的类型都可以实现 Copy 。如下是一些 Copy 的类型：\n所有整数类型，比如 u32。 布尔类型，bool，它的值是 true 和 false。 所有浮点数类型，比如 f64。 字符类型，char。 元组，当且仅当其包含的类型也都实现 Copy 的时候。比如，(i32, i32) 实现了 Copy，但 (i32, String) 就没有。\n\n变量的所有权总是遵循相同的模式：将值赋给另一个变量时移动它。当持有堆中数据值的变量离开作用域时，其值将通过 drop 被清理掉，除非数据被移动为另一个变量所有。\n\n参考：\n\nhttps://kaisery.github.io/trpl-zh-cn/ch04-01-what-is-ownership.html"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html",
    "title": "Single-cell sequencing protocols",
    "section": "",
    "text": "From https://www.sc-best-practices.org/introduction/scrna_seq.html\nTypes:"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#基于微流控装置",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#基于微流控装置",
    "title": "Single-cell sequencing protocols",
    "section": "基于微流控装置",
    "text": "基于微流控装置\n基于微流控芯片的单细胞策略可以将细胞固定在水凝胶液滴内部，实现将其分隔到单细胞反应室中。最广泛使用的协议有inDrop [Klein等人，2015]、Drop-seq [Macosko等人，2015]和商业可用的10x Genomics Chromium [Zheng等人，2017]，能够每秒生成数千个这样的液滴。这种大规模并行的过程以相对较低的成本生成非常多的液滴。虽然这三个协议在细节上有所不同，但始终设计了包含携带PCR处理程序、细胞条形码和4-8个碱基对长的唯一分子标识符（UMI）和poly-T尾巴的专门微珠来封装细胞的纳升级液滴，以便同时捕获微珠和细胞。封装过程是使用具有on-bead引物的专门微珠进行的，这些引物包含一个PCR处理程序、一个细胞条形码和一个4-8 bp长的唯一分子标识符（UMI）和一个poly-T尾巴。在裂解后，细胞的mRNA立即被释放，并被附加在微珠上的带条形码的寡核苷酸捕获。接下来，收集液滴并将其打破以释放连接到微粒（STAMPs）上的单个细胞转录组。然后进行PCR和反转录以捕获和扩增转录物。最后进行标签切割，其中转录本被随机切割并附加测序适配器。该过程产生了用于测序的测序文库，如上所述。在基于微流控芯片的协议中，细胞的约10%的转录本会被检测到[Islam等人，2014]。值得注意的是，这种低测序量已足以稳健地识别细胞类型。\n所有三种基于微流控芯片的方法都会产生特定的偏差。所使用的微珠材料在协议之间存在差异。Drop-seq使用脆性树脂制成的微珠，因此微珠被以泊松分布封装，而InDrop和10X Genomics微珠是可变形的，导致微珠的占用率超过80%[Zhang等人，2019]。此外，Drop-Seq中使用表面固定引物可能会影响捕获效率。InDrop使用光解引物释放，而10X genomics则溶解微珠。这种差异也影响了反转录过程的位置。在Drop-seq中，反转录发生在微珠从液滴中释放后，而在InDrop和10X genomics协议中，反转录发生在液滴内部[Zhang等人，2019]。\n2019年张等人的比较发现，就微珠质量而言，10X Genomics比inDrop和Drop-seq表现更好，因为前两个系统中的细胞条形码存在明显的不匹配。此外，从有效条形码中产生的读数比例对于10X Genomics是75%，而对于InDrop和Drop-seq仅分别为25%和30%。\n在敏感性方面，10X Genomics也表现出类似的优势。他们的比较显示，平均而言，10X Genomics捕获了来自3000个基因的约17000份转录本，而Drop-seq只有来自2500个基因的约8000份转录本，InDrop则只有来自1250个基因的约2700份转录本。技术噪音最小的是10X Genomics，其次是Drop-seq和InDrop[Zhang等人，2019]。\n实际生成的数据展示了大量的协议偏差。10X Genomics更倾向于捕获和扩增长度较短的基因和GC含量较高的基因，而相比之下，Drop-seq更倾向于GC含量较低的基因。尽管10X Genomics在各个方面的表现都超过了其他协议，但它每个细胞的成本也是其他协议的两倍左右。此外，除了微珠以外，Drop-seq是开源的，协议可以更容易地进行适应性调整。InDrop完全是开放源代码的，甚至可以在实验室中制造和修改微珠。因此，InDrop是三种协议中最灵活的一种。\n\n优点：\n\n可以以成本效益的方式对大量细胞进行测序，以识别组织的总体组成并表征罕见的细胞类型。\n可以加入唯一分子标识符（UMIs）。\n\n限制：\n\n与其他方法相比，转录本检测率较低。\n只能捕获3’端而非全长转录本，因为细胞条形码和PCR处理程序仅添加到转录本的末端。"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#基于板",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#基于板",
    "title": "Single-cell sequencing protocols",
    "section": "基于板",
    "text": "基于板\n基于板的协议通常将细胞物理地分离到微孔板中。第一步涉及通过荧光激活细胞分选（FACS）等方法对细胞进行分选，其中根据特定的细胞表面标记对细胞进行排序；或通过微型移液进行。然后将所选细胞放入含有细胞裂解缓冲液的单个孔中，随后进行反转录。这样可以在单次实验中分析数百个细胞，并每个细胞捕获5000至10000个基因。基于板的测序协议包括但不限于SMART-seq2、MARS-seq、QUARTZ-seq和SRCB-seq。总的来说，这些协议在其多重复合能力方面存在差异。例如，MARS-seq允许三个条形码级别，即分子、细胞和板级标签，以实现强大的多重复合功能。相反，SMART-seq2不允许早期多重复合，从而限制了细胞数目。Mereu等人在2020年进行的系统比较显示，与SMART-seq2、MARS-seq或SRCB-seq相比，QUARTZ-seq2能够捕获更多的基因[Mereu等人，2020]，这意味着QUARTZ-seq2能够很好地捕获细胞类型特异性标记基因，从而实现可靠的细胞类型注释。\n\n优点：\n\n每个细胞可以恢复许多基因，从而进行深入的表征。\n可能在库制备之前收集信息，例如通过FACS排序将细胞大小和任何使用的标记的强度与孔座标关联起来。\n允许完整的转录本恢复。\n\n限制：\n\n基于板的实验规模受到其单个处理单元较低的吞吐量的限制。\n断裂步骤会消除链特异性信息[Hrdlickova等人，2017]。\n根据协议不同，基于板的协议可能是人力密集型的，需要许多必需的移液步骤，导致潜在的技术噪声和批次效应。"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#fluidigm-c1",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#fluidigm-c1",
    "title": "Single-cell sequencing protocols",
    "section": "Fluidigm C1",
    "text": "Fluidigm C1\n商业的Fluidigm C1系统是一种微流控芯片，可以自动将细胞加载和分离到小反应室中。 CEL-seq2和SMART-seq（版本1）协议在其工作流程中使用Fluidigm C1芯片，允许RNA提取和文库制备步骤同时进行，从而减少所需的手动劳动。但是，Fluidigm C1需要相对均匀的细胞混合物，因为细胞基于其大小会到达微流控芯片上的不同位置，可能会引入潜在的空间偏差。由于扩增步骤是在单个孔中进行的，因此可以实现全长测序，有效地减少了许多其他单细胞RNA测序协议的3’偏差。该协议通常也更昂贵，因此主要用于特定细胞群的广泛检查。\n\n优点：\n\n允许全长转录本覆盖。\n可以恢复剪接变异和T/B细胞受体多样性。\n\n限制：\n\n仅允许对最多800个细胞进行测序[Fluidigm，2022]。\n每个细胞的成本比其他协议更高。"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#纳米孔单细胞转录组测序",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#纳米孔单细胞转录组测序",
    "title": "Single-cell sequencing protocols",
    "section": "纳米孔单细胞转录组测序",
    "text": "纳米孔单细胞转录组测序\n长读单细胞测序方法很少使用UMI [Singh等人，2019]或未执行UMI校正[Gupta等人，2018]，因此将新的UMI读取分配给新的UMI。由于长读测序器的较高测序误差率，这会导致严重问题[Lebrigand等人，2020]。Lebigrand等人引入了ScNaUmi-seq（带有UMI的单细胞Nanopore测序），它将Nanopore测序与细胞条形码和UMI分配相结合。通过比较在Nanopore读取中发现的细胞条形码序列和从同一区域或基因恢复的 Illumina读取中发现的序列，利用Illumina数据指导条形码分配[Lebrigand等人，2020]。然而，这实际上需要两个单细胞库。scCOLOR-seq使用在整个条形码长度上互补的核苷酸对计算识别无误差的条形码。然后使用这些条形码作为指南来纠正其余的错误条形码[Philpott等人，2021]。修改过的UMI-tools定向网络方法可以纠正UMI序列重复。\n\n优点：\n\n恢复剪接和序列异质性信息\n\n缺点：\n\nNanopore试剂昂贵。\n细胞条形码恢复错误率高。\n根据协议不同，条形码分配使用Illumina数据进行指导，需要两个测序试验。 只有约10%的提取细胞被捕获，使该协议不适用于罕见细胞类型或低输入。\n使用的阵列仅捕获特定的细胞大小，可能会引入偏差。"
  },
  {
    "objectID": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#总结",
    "href": "posts/2023-05-12-single-cell-sequencing-protocols/index.html#总结",
    "title": "Single-cell sequencing protocols",
    "section": "总结",
    "text": "总结\n总的来说，我们强烈建议湿实验室和干实验室的科学家根据研究目的选择测序协议。是希望对特定细胞类型人群进行深入表征吗？在这种情况下，其中一种基于板的方法可能更适合。相反，基于液滴的测定会更好地捕获异质性混合物，从而允许更广泛的细胞表征。此外，如果预算是一个限制因素，则应选择更具成本效益和稳健性的协议。在分析数据时，请注意测序试验特异性偏差。为了全面比较所有单细胞测序协议，我们建议参考Mereu等人的“Benchmarking single-cell RNA-sequencing protocols for cell atlas projects”论文[Mereu等人，2020]。"
  },
  {
    "objectID": "posts/2023-06-01-likelihood/index.html",
    "href": "posts/2023-06-01-likelihood/index.html",
    "title": "概率与似然——摘要",
    "section": "",
    "text": "概率(probability)和似然(Likelihood)是两个经常被混淆使用的概念。\n概率质量函数（PMF）是用于描述离散随机变量的概率分布的函数。对于给定的随机变量取值，概率质量函数给出了该取值发生的概率。概率质量函数的输入是随机变量的取值，输出是对应取值的概率。\n似然函数（Likelihood Function）是用于估计模型参数的函数。对于给定的模型参数值，似然函数衡量了观测数据出现该参数值的可能性。似然函数的输入是模型参数的取值，输出是在给定参数下观测数据出现的可能性。\n概率质量函数和似然函数的区别在于它们关注的对象不同。概率质量函数是给定参数值时，计算随机变量的取值的概率；而似然函数是给定观测数据时，评估参数值的可能性。因此，似然函数通常用于参数估计，而概率质量函数用于描述随机变量的分布。\n简而言之，当我们有一个固定参数集的模型并且我们对可能生成的数据类型感兴趣时，通常会考虑概率。相反，当我们已经观察到数据并且我们想要检查某些模型参数的可能性时，就会使用似然。\n概率质量函数（PMF）：参数已知，观察数据。\n似然函数(Likelihood Function)：数据已知，评估参数。\n\n来源：\n\n一文了解最大似然估计(Maximum Likelihood Estimation)"
  },
  {
    "objectID": "posts/2023-05-29-tccia/index.html",
    "href": "posts/2023-05-29-tccia/index.html",
    "title": "TCCIA：第一个连接CircRNA和免疫治疗的数据库",
    "section": "",
    "text": "CircRNA是一种形成闭环结构的RNA类型，已被证明在调节基因表达方面发挥重要作用。 然而，CircRNA在免疫治疗中的作用和标志物潜能仍未被充分研究。我们很高兴地向大家介绍TCCIA（The Cancer CircRNA Immunome Atlas），它是一个广泛且用户友好的数据库，用于研究接受针对CTLA4，PD-1或PDL-1等靶点的免疫治疗的18个临床队列中的circRNA表达和分析。这一创新资源提供了cohorts和molecule级别上circRNAs、临床表型和免疫特征/浸润的系统比较。该数据库包括3700多个癌症样本，来自5种不同类型的癌症患者，这些患者单独或与化疗、靶向治疗以及其他免疫治疗药物联合使用免疫检查点抑制剂（ICI）进行治疗。\nTCCIA代表了研究circRNA及其作为预测免疫治疗响应生物标志物的潜力以及它们在癌症中的广泛功能的独特机会。无论你是研究circRNA的研究人员还是对改善新治疗策略以提高患者预后感兴趣的临床医生，TCCIA都提供了有价值的信息，可以深入了解circRNA、免疫系统和癌症之间复杂的相互作用。\n\nTCCIA 分为多个不同的页面，它以仓库页面的数据查阅和筛选为起点，允许用户从多个不同的层面和角度探索CircRNA在免疫治疗中的价值。\n\nTCCIA 是一次成功的线上科研协作，来自国内外各地的研究者贡献自己的数据和知识、资源与技术。我们旨在提供一个有用且好用的CircRNA在线研究平台，希望CircRNA领域的研究者能从中获益。目前，TCCIA正式进入开放测试阶段，它可以通过 http://biotrainee.vip:18888/TCCIA/ 和 https://shiny.hiplot.cn/TCCIA 进行在线访问。我们欢迎大家进行试用并提供宝贵的反馈意见，任何使用不便或者新功能特性的提议请发送到王诗翔（shixiang1994wang@gmail.com）或周建国（jianguo.zhou@zmu.edu.cn），我们团队将尽力回复和解决，我们期待大家的参与，一起为该领域的进步添砖加瓦。\n\n下面是一些功能的预览图："
  },
  {
    "objectID": "posts/2023-06-12-prepare-for-validation/index.html",
    "href": "posts/2023-06-12-prepare-for-validation/index.html",
    "title": "准备好应对验证",
    "section": "",
    "text": "个人感觉有效地验证的必要的，但过度的验证是次要的。但作为一个生信从业者，当文章投出去之后，总会遇到认为全世界的数据（包括原始数据）都供你可用的，任意挥霍的评审人。我并不认为这是真正的专家，因为它把一个次要的问题当作major concern，由此来否决你整个方法研究的科学性。\n事实是，文章总能在由高到低的期刊列表的评审过程中发出去。真正付出的代价是研究者的时间。 这也是预印本为什么正在走向科学发表主流的根本原因，人们越来越没有耐心了，专业化让人们产生越来越大的隔阂，但评审在大部分情况下并没有快捷有效地提升文章的真正质量。实际上，大部分的审稿人可能并不具备评估某一篇文章的能力，邀请审稿大多数只是一种资历。\n我比较痛恨当前科学研究赢者通吃的规则。eLife 等期刊的新理念或许是一个好的方向。更让我思考和困惑的是，如何不在所谓大树的阴凉下，有且做好独立创新，并能够追求内心倾向、自由探索的学术。\n可能当前阶段，我的想法过于理想和幼稚。但如果我不想和尝试做做，我的人生也总归不会交到别人的手上。"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Write programs that do one thing and do it well.",
    "section": "",
    "text": "第一/二类型错误\n\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nR 设置编译选项\n\n\n\n\n\n\n\nLinux\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nMar 20, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nsnakemake 教程精要\n\n\n\n\n\n\n\nsnakemake\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nCan I run VS Code Server on older Linux distributions\n\n\n\n\n\n\n\nLinux\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nuse singularity\n\n\n\n\n\n\n\nnote\n\n\ncontainer\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\ntelescope installation\n\n\n\n\n\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n回广州路上的感想\n\n\n\n\n\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nshiny plot dpi\n\n\n\n\n\n\n\npersonal\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nFeb 12, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n免疫\n\n\n\n\n\n\n\nnote\n\n\n免疫\n\n\n\n\n\n\n\n\n\n\n\nJan 22, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n解决 GitHub 代码拉取和推送问题\n\n\n\n\n\n\n\nnote\n\n\nLinux\n\n\ngit\n\n\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nR 针对数据进行惰性加载\n\n\n\n\n\n\n\nR\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n随便聊两句，可能不一定有意义\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n诗歌存档\n\n\n\n\n\n\n\n诗\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n少年时\n\n\n\n\n\n\n\n诗\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nThe world is not all sunshine and rainbows\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nE pluribus unum: From Complexity, Universality\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n从 Faker 夺冠讨论竞争与热爱\n\n\n\n\n\n\n\nthought\n\n\n\n\n\n\n\n\n\n\n\nNov 21, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n谈谈使用过的代码编辑器\n\n\n\n\n\n\n\nR\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nNov 15, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n随想\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nSRA 测序数据下载和处理三部曲\n\n\n\n\n\n\n\nnote\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nNov 13, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\ndont forget\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n南京行记 两首\n\n\n\n\n\n\n\n诗\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n聊聊开源协议：我们可以免费使用开源软件吗？\n\n\n\n\n\n\n\nthought\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n结合 quarto 出版系统和 webr 技术\n\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023 年诺贝尔生理或医学奖授予 KK\n\n\n\n\n\n\n\nacademic\n\n\nnews\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nShowMeAI 速查表\n\n\n\n\n\n\n\nmachine-learning\n\n\nPython\n\n\nresource\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nShiny 开发学习路径\n\n\n\n\n\n\n\nshiny\n\n\nR\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n检查 CPU 的使用量\n\n\n\n\n\n\n\nLinux\n\n\nbioinformatics\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nMAPQ（映射质量）与-AS（比对得分）\n\n\n\n\n\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\ndiscourse on the method\n\n\n\n\n\n\n\nacademic\n\n\nnote\n\n\nthought\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\ncircRNA鉴定流程-使用手册\n\n\n\n\n\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\npnas\n\n\n\n\n\n\n\nrandom\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nnormalize data 和 scale data 的差异\n\n\n\n\n\n\n\nnote\n\n\nstats\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\ncaddyfile\n\n\n\n\n\n\n\nnote\n\n\nLinux\n\n\n\n\n\n\n\n\n\n\n\nAug 11, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nDuplication Rate 经验值\n\n\n\n\n\n\n\nnote\n\n\nbioinformatics\n\n\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nSingle-cell Workshop 2021 - 03 - 细胞相互作用分析\n\n\n\n\n\n\n\nnote\n\n\nscRNA-seq\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nSingle-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定\n\n\n\n\n\n\n\nnote\n\n\nscRNA-seq\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 26, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n小记（往昔）\n\n\n\n\n\n\n\n诗\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-25\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 25, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n学术的价值观\n\n\n\n\n\n\n\nacademic\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n你认同生物信息学不属于科学吗？\n\n\n\n\n\n\n\nthought\n\n\nacademic\n\n\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-20\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 20, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-18\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nSingle-cell Workshop 2021 - 01 - scRNASeq 分析基础\n\n\n\n\n\n\n\nnote\n\n\nscRNA-seq\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-13\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-12\n\n\n\n\n\n\n\njournal\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nscrna 核心工具\n\n\n\n\n\n\n\nnote\n\n\nscRNA-seq\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-10\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nmight & right\n\n\n\n\n\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-06\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n生物医学能离开生物信息学吗？\n\n\n\n\n\n\n\nthought\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n博客 links\n\n\n\n\n\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-07-02\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-06-28\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-06-27\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n2023-06-26\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nlmtest 似然比检验Likelihood Ratio Test 比较两个线性模型\n\n\n\n\n\n\n\nstats\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n使用 Just 重复构建\n\n\n\n\n\n\n\nLinux\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n应该写写日记\n\n\n\n\n\n\n\njournal\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n准备好应对验证\n\n\n\n\n\n\n\nbioinformatics\n\n\nthought\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n概率与似然——摘要\n\n\n\n\n\n\n\nstats\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nscRNA-seq_online 00：包准备\n\n\n\n\n\n\n\nbioinformatics\n\n\nscRNA-seq\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nTCCIA：第一个连接CircRNA和免疫治疗的数据库\n\n\n\n\n\n\n\nbioinformatics\n\n\nshiny\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nnextflow 运行 singularity no space left\n\n\n\n\n\n\n\ndebug\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nStart a new research direction\n\n\n\n\n\n\n\nacademic\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nSingle-cell sequencing protocols\n\n\n\n\n\n\n\nbioinformatics\n\n\nnote\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n晒晒娃和Y叔R书的合照\n\n\n\n\n\n\n\npersonal\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\n随想\n\n\n\n\n\n\n\npersonal\n\n\n\n\n\n\n\n\n\n\n\nApr 8, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT 帮助我理解 Rust 的所有权\n\n\n\n\n\n\n\nRust\n\n\nAI\n\n\n\n\n\n\n\n\n\n\n\nMar 29, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2023\n\n\nShixiang Wang\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resource.html",
    "href": "resource.html",
    "title": "Resource",
    "section": "",
    "text": "一些比较有用的 R 包都列入了 r-universe，无论是否发布在 CRAN。这里也进行了一些罗列。\n\n\n\n\nloon - 不再维护\n\n\n\n\n\nsync-deploy\nR Search Extension"
  },
  {
    "objectID": "resource.html#软件",
    "href": "resource.html#软件",
    "title": "Resource",
    "section": "",
    "text": "一些比较有用的 R 包都列入了 r-universe，无论是否发布在 CRAN。这里也进行了一些罗列。\n\n\n\n\nloon - 不再维护\n\n\n\n\n\nsync-deploy\nR Search Extension"
  },
  {
    "objectID": "resource.html#图书教程",
    "href": "resource.html#图书教程",
    "title": "Resource",
    "section": "图书/教程 📖",
    "text": "图书/教程 📖\n\n交互的Python：数据分析入门 - 阅读地址\n极客R：数据分析之道\n\nhttps://github.com/ShixiangWang/geek-r-tutorial\n\nCookbook for R 中文版- 阅读地址\n\nhttps://github.com/openbiox/Cookbook-for-R-Chinese\n\n生信爱好者周刊"
  },
  {
    "objectID": "posts/2023-06-14-write-journal/index.html",
    "href": "posts/2023-06-14-write-journal/index.html",
    "title": "应该写写日记",
    "section": "",
    "text": "吴军在《阅读与写作讲义》中写道工作日记、读书心得以及对特殊经历的记录值得人写下来。\n我很认同这个看法。回望过去一到两年的时间，写东西的次数少了以后，我竟常常难以思考出来过去一段时间做了什么，是浪费了时间还是利用了时间都搞不清楚。\n这让我联想到博士后期间做的工作，我不禁反思：是否是平常太过随意，导致工作和论文写作不严谨，所以论文评审才常常收到负面意见？心态可以放松，思想应当解放，但平时无论是工作或是生活中都要严谨，这样可能才能做好真正的科学研究工作。做人首先还是要对自己负责，毕竟生命只有一次啊！\n很多东西真的不重要，很多东西真的很重要。"
  },
  {
    "objectID": "posts/2023-06-26-lmtest-lrt/index.html",
    "href": "posts/2023-06-26-lmtest-lrt/index.html",
    "title": "lmtest 似然比检验Likelihood Ratio Test 比较两个线性模型",
    "section": "",
    "text": "library(lmtest)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n## with data from Greene (1993):\n## load data and compute lags\ndata(\"USDistLag\")\nusdl &lt;- na.contiguous(cbind(USDistLag, lag(USDistLag, k = -1)))\ncolnames(usdl) &lt;- c(\"con\", \"gnp\", \"con1\", \"gnp1\")\n\nfm1 &lt;- lm(con ~ gnp + gnp1, data = usdl)\nfm2 &lt;- lm(con ~ gnp + con1 + gnp1, data = usdl)\n\n## various equivalent specifications of the LR test\n## 下面4种操作方法是等价的\nlrtest(fm2, fm1)\n\nLikelihood ratio test\n\nModel 1: con ~ gnp + con1 + gnp1\nModel 2: con ~ gnp + gnp1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   5 -56.069                         \n2   4 -65.871 -1 19.605  9.524e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(fm2, 2) # Remove variable with idx 2\n\nLikelihood ratio test\n\nModel 1: con ~ gnp + con1 + gnp1\nModel 2: con ~ gnp + gnp1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   5 -56.069                         \n2   4 -65.871 -1 19.605  9.524e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(fm2, \"con1\") # Remove variable cond 1\n\nLikelihood ratio test\n\nModel 1: con ~ gnp + con1 + gnp1\nModel 2: con ~ gnp + gnp1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   5 -56.069                         \n2   4 -65.871 -1 19.605  9.524e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlrtest(fm2, . ~ . - con1)  # Remove con1 by formula\n\nLikelihood ratio test\n\nModel 1: con ~ gnp + con1 + gnp1\nModel 2: con ~ gnp + gnp1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   5 -56.069                         \n2   4 -65.871 -1 19.605  9.524e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSurvival 模型交互项：\n\nlibrary(survival)\n\nfit2 = coxph(Surv(time, status) ~ age + sex*factor(ph.ecog), data = lung)\nlmtest::lrtest(fit2, . ~ . - sex:factor(ph.ecog))\n\nLikelihood ratio test\n\nModel 1: Surv(time, status) ~ age + sex * factor(ph.ecog)\nModel 2: Surv(time, status) ~ age + sex + factor(ph.ecog)\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)\n1   7 -728.93                     \n2   5 -729.05 -2 0.2263      0.893\n\nsummary(fit2)\n\nCall:\ncoxph(formula = Surv(time, status) ~ age + sex * factor(ph.ecog), \n    data = lung)\n\n  n= 227, number of events= 164 \n   (1 observation deleted due to missingness)\n\n                          coef exp(coef)  se(coef)      z Pr(&gt;|z|)  \nage                   0.009667  1.009714  0.009601  1.007   0.3140  \nsex                  -0.641345  0.526583  0.385890 -1.662   0.0965 .\nfactor(ph.ecog)1      0.335017  1.397964  0.609291  0.550   0.5824  \nfactor(ph.ecog)2      0.618092  1.855384  0.686960  0.900   0.3683  \nfactor(ph.ecog)3      1.941570  6.969687  1.033409  1.879   0.0603 .\nsex:factor(ph.ecog)1  0.063350  1.065400  0.451918  0.140   0.8885  \nsex:factor(ph.ecog)2  0.221865  1.248403  0.507016  0.438   0.6617  \nsex:factor(ph.ecog)3        NA        NA  0.000000     NA       NA  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n                     exp(coef) exp(-coef) lower .95 upper .95\nage                     1.0097     0.9904    0.9909     1.029\nsex                     0.5266     1.8990    0.2472     1.122\nfactor(ph.ecog)1        1.3980     0.7153    0.4235     4.615\nfactor(ph.ecog)2        1.8554     0.5390    0.4827     7.131\nfactor(ph.ecog)3        6.9697     0.1435    0.9195    52.827\nsex:factor(ph.ecog)1    1.0654     0.9386    0.4394     2.583\nsex:factor(ph.ecog)2    1.2484     0.8010    0.4621     3.372\nsex:factor(ph.ecog)3        NA         NA        NA        NA\n\nConcordance= 0.636  (se = 0.025 )\nLikelihood ratio test= 31.09  on 7 df,   p=6e-05\nWald test            = 30.72  on 7 df,   p=7e-05\nScore (logrank) test = 33.92  on 7 df,   p=2e-05\n\n\n除了用 likelihood ratio test，还可以使用 anova() 去对比。\n但如果数据不一致时，上述模型无法见效。这时，应把不同的对比作为变量构建模型，然后可以采用 aov() 进行方差分析，看变量是否显著以评估是否有组间差异。"
  },
  {
    "objectID": "posts/2023-06-26-journal/index.html",
    "href": "posts/2023-06-26-journal/index.html",
    "title": "2023-06-26",
    "section": "",
    "text": "疫情之后抗感冒能力确实变得更加糟糕了，上一次二阳（本以为是重感冒）十多天之后在端午之前终于感觉到差不多好了，端午去了趟深圳回家又感觉不好了，特别是前天晚上睡觉时突然感觉喉咙很痒，频繁咳嗽，把自己咳起来去喝水。\n工作后老是感觉时间少，做什么都不够用似的，而且身体在变形和走下坡路。所以，成年人需要合理地管理自己的时间：身体、工作、生活、家人（陪伴）、休闲娱乐。其实，人生存在很强的悖论，要保持理性追求职业上升的同时，又要把关自身的感性的体态面貌健康/快乐渴望，很难做到，真的很难做到。\n我总是在想起我自己常想的一句话：我们是看着别人在过自己的人生。这也是非常矛盾的，合理的把握至关重要。在平衡这方面，我们国家的哲学理论实在是一言蔽之——阴阳！"
  },
  {
    "objectID": "posts/2023-06-27-journal/index.html",
    "href": "posts/2023-06-27-journal/index.html",
    "title": "2023-06-27",
    "section": "",
    "text": "今天记录在《经济学讲义》中看到的一句话：\n\n你拥有你的生命，但是你的生命是怎么度过的，你的职业是怎样选择的，很大程度上是由社会上其他人决定的。"
  },
  {
    "objectID": "posts/2023-06-28-journal/index.html",
    "href": "posts/2023-06-28-journal/index.html",
    "title": "2023-06-28",
    "section": "",
    "text": "坚持学习，坚持同一种内容的学习。\n工作、生活越来越让时间碎片化，我常常在回家后忘记想要做的工作和学习内容，在上班的空闲时间也常忘记家里要处理的杂事，比如购置一些缺少的生活用品。可见，这种碎片化非常影响记忆，自然就影响日常的学习进度了。\n为了抵抗这种情况，就要自律，不论各种事情，只要定下了近期的学习目标，在一两天内必须要有跟进。如果放到一周后再来做，可能要做什么都要回想半个小时。之前写 Python 基础教程书期间我就有半年因为这种事情而焦虑。\n人最可贵的是思考并付诸于行动，不是吗？"
  },
  {
    "objectID": "posts/2023-07-02-links/index.html",
    "href": "posts/2023-07-02-links/index.html",
    "title": "博客 links",
    "section": "",
    "text": "之前在构建个人域名网站 &lt;shixiangwang.work&gt; 时，把浏览器书签的所有博客链接都进行了整理。\n由于云服务器的时间有限，运维实际成本想了想还是蛮高的，没多久后也没考虑再维护了。 这里把之前整理的链接备份一下，以方便查找复用，如果对一些读者有帮助也是极好的了。\n点击查看链接"
  },
  {
    "objectID": "posts/2023-07-02-journal/index.html",
    "href": "posts/2023-07-02-journal/index.html",
    "title": "2023-07-02",
    "section": "",
    "text": "今天摘录从书中看到的一句话。\n\n我们应该利用自己有限的时间、有限的精力，在自己所能涉及的所有领域、所有活动、所有选项中， 根据边际平衡的规律来分配时间、金钱、精力和其他资源，从而使总效用达到最大。\n\n这概括了生命一生的所有努力。"
  },
  {
    "objectID": "posts/2023-07-06-journal/index.html",
    "href": "posts/2023-07-06-journal/index.html",
    "title": "2023-07-06",
    "section": "",
    "text": "顾神昨天公众号吐槽他的海外优青没有上会，我深表遗憾。\n我根据这几年逐步对学术界的了解和领会，分析可能有以下一些原因：\n\n缺乏大子刊及以上的一作和通讯；\n国内缺乏生物医学各种山头的硬的合作和关系；\n纯生信基础设施的研究工作不被评审人认可，我自己申请国自然基金时就感觉生信工作如果扯不上“分子机制”就被认为不是“自然科学”；\n基金本子没有突出优势，上会之前应该是有多个评审人基于不同的方面评估等级，所以很可能写的本子不对国内的“口味”。\n\n国内的学术条件确实在越来越好，这吸引了越来越多海外优秀的青年回国发展。 但目前国内长期不正常的学术土壤为祸尤烈，希望未来在政府的引导下、有识之士的建设下 朝着良好的方向前进。学术是创新探索，也是资源的分配，希望以后资源能够更主动地分配 到不同领域的各个角落，而不是各大门派山头。"
  },
  {
    "objectID": "posts/2023-07-06-can-biology-live-without-bioinformatics/index.html",
    "href": "posts/2023-07-06-can-biology-live-without-bioinformatics/index.html",
    "title": "生物医学能离开生物信息学吗？",
    "section": "",
    "text": "不能。\n整个生物医学领域，已经由实验科学进入了数据科学的层次，而这一点无法再倒退。 生物信息学者/家（bioinformatian/computational biologist），已经成为了该领域不可或缺的存在。无论是狭义的组学，还是更广义的分子结构计算/图形处理等，生物信息学者已经成为了科学发现/技术创造的中间力量。这也正是为什么从事生物信息学的人能从致谢名单中走到作者列表，走到第一/通讯作者的根本原因。\n如果你是一个纯湿实验的研究者，你还在心里鄙夷这个行业，你就应该反思了，时代的进步不会等待你的落后。"
  },
  {
    "objectID": "posts/2023-07-10-journal/index.html",
    "href": "posts/2023-07-10-journal/index.html",
    "title": "2023-07-10",
    "section": "",
    "text": "今天终于看完了《癌生物学》，具体而言，是「珠江肿瘤」公众号整理的中文版《癌生物学》的内容。\n当年阅读计划单上终于可以划去一项了，稍稍松了一口气。这几年，虽然我读完了不少“闲书”和网文小说，但阅读，特别是 读完的工作领域相关图书几乎是没有的。教科书类字少，但理解、思考却非常费时费力，充满挑战。 这里需要感谢公众号的作者的悉心整理，当然也特别感谢图书的作者和译者。\n\n\n公众号癌生物学汇总帖\n癌生物学PDF.pdf\n\n\n很多内容读的很浅，有些内容也跳读了，但翻完了，心里好像“靴子落地”了。这也给了我自信和时间去关注和完成其他的事情。\n这两年，我在生活和工作中都遇到了心里难以言表的困难，这种困难来自感受，也部分来自物质。切实地融入生活，才发生生活有那么多的琐事，很多事情需要耗费心力，却感受上“憋”的很。我常常会想在心里找个角落躲起来。也有时会在游戏、小说和心灵的孤独中放逐自我。 人不是机器，它不能一直产生正能量。我们都是大大宇宙中小小地球上一个个活着的卑微的人，我们想要诉求伟大，但跌入的是统计的深渊。\n不管怎样，我们的道路，就是要完成一个一个想做的事情，然后继续向前。"
  },
  {
    "objectID": "posts/2023-07-10-might-and-right/index.html",
    "href": "posts/2023-07-10-might-and-right/index.html",
    "title": "might & right",
    "section": "",
    "text": "摘录一句话。\n能力：might\n权利：right\n能力取决于自己能够占有多少，而权利取决于社会上其他人愿意给你多少。\n\n所以，该追求哪种呢？"
  },
  {
    "objectID": "posts/2023-07-10-scrna-recom/index.html",
    "href": "posts/2023-07-10-scrna-recom/index.html",
    "title": "scrna 核心工具",
    "section": "",
    "text": "Repo {scrna-recom} 单细胞综述文章关于单细胞转录组分析的流程、软件、安装等的一些说明。这里我主要是拷贝下其中关于核心工具的整理，防止迷失。\n\n\n\n\n\n\n\n\nPackage\n\n\nTutorial\n\n\n\n\n\n\nRaw Data Processing\n\n\nCell Ranger\n\n\nhttps://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger\n\n\n\n\nQuality Control\n\n\nDoubletFinder\n\n\nhttps://github.com/chris-mcginnis-ucsf/DoubletFinder\n\n\n\n\nSeurat\n\n\nhttps://satijalab.org/seurat/articles/pbmc3k_tutorial.html\n\n\n\n\nSoupX\n\n\nhttps://rawcdn.githack.com/constantAmateur/SoupX/204b602418df12e9fdb4b68775a8b486c6504fe4/inst/doc/pbmcTutorial.html\n\n\n\n\nNormalization\n\n\nSeurat\n\n\nhttps://satijalab.org/seurat/articles/pbmc3k_tutorial.html#normalizing-the-data-1\n\n\n\n\nIntegration\n\n\nSeurat:CCA,RPCA\n\n\nhttps://satijalab.org/seurat/articles/integration_rpca.html\n\n\n\n\nLiger\n\n\nhttps://github.com/welch-lab/liger\n\n\n\n\nHarmony\n\n\nhttps://github.com/immunogenomics/harmony\n\n\n\n\nDimensional Reduction\n\n\nSeurat:PCA,UMAP\n\n\nhttps://satijalab.org/seurat/articles/pbmc3k_tutorial.html#perform-linear-dimensional-reduction-1\n\n\n\n\nClustering\n\n\nSeurat\n\n\nhttps://satijalab.org/seurat/articles/pbmc3k_tutorial.html#cluster-the-cells-1\n\n\n\n\nCell type annotation\n\n\nSingleR\n\n\nhttps://github.com/dviraran/SingleR\n\n\n\n\nscCATCH\n\n\nhttps://github.com/ZJUFanLab/scCATCH\n\n\n\n\nRegulon analysis\n\n\nSCENIC\n\n\nhttps://pyscenic.readthedocs.io/en/latest/\n\n\n\n\nTrajectory inference\n\n\nMonocle3\n\n\nhttps://cole-trapnell-lab.github.io/monocle3/\n\n\n\n\nscVelo\n\n\nhttps://github.com/theislab/scvelo\n\n\n\n\nCell communication\n\n\nCellChat\n\n\nhttps://github.com/sqjin/CellChat\n\n\n\n\nMetabolic analysis\n\n\nscMetabolism\n\n\nhttps://github.com/wu-yc/scMetabolism\n\n\n\n\nscFEA\n\n\nhttps://github.com/changwn/scFEA\n\n\n\n\n\nWorkflow:\n\n下图展示了从测序read比对参考基因组根据barcode和UMI产生Count matrix的过程。\n\n\nscRNA-seq利用PCR技术对cDNA分子进行指数级扩增，而UMIs技术可以帮助用户识别并消除在扩增过程中可能产生的重复序列，从而降低技术噪音。值得注意的是，UMI中的测序错误可以人为地提高基因表达，因为应该被消除的重复序列被视为了不同的分子。相反，不同的分子可能会被错误地标记为具有相同的UMI序列，从而被视为同一个分子。\n\nFASTQ 预处理工具：\n\n\nCellRanger、DropEst、Kallisto-BUStools、UMI-Tools、STARSolo和Alevin都是可选的read处理方法，\n\nCell QC is commonly performed based on three QC covariates:\nthe number of counts per barcode (count depth)：每个细胞的测序深度，gene count数总和\nthe number of genes per barcode：每个细胞内表达基因的数目\nthe fraction of counts from mitochondrial genes per barcode：每个细胞中线粒体基因的比例\n标准的 R 处理流程大概如下：\nSeurat_preprocessing &lt;- function(counts, project = \"Scissor_Single_Cell\", min.cells = 400, min.features = 0,\n                                 normalization.method = \"LogNormalize\", scale.factor = 10000,\n                                 selection.method = \"vst\", resolution = 0.6,\n                                 dims_Neighbors = 1:10, dims_TSNE = 1:10, dims_UMAP = 1:10,\n                                 verbose = TRUE){\n    library(Seurat)\n    data &lt;- CreateSeuratObject(counts = counts, project = project, min.cells = min.cells, min.features = min.features)\n    data &lt;- NormalizeData(object = data, normalization.method = normalization.method, scale.factor = scale.factor, verbose = verbose)\n    data &lt;- FindVariableFeatures(object = data, selection.method = selection.method, verbose = verbose)\n    data &lt;- ScaleData(object = data, verbose = verbose)\n    data &lt;- RunPCA(object = data, features = VariableFeatures(data), verbose = verbose)\n    data &lt;- FindNeighbors(object = data, dims = dims_Neighbors, verbose = verbose)\n    data &lt;- FindClusters( object = data, resolution = resolution, verbose = verbose)\n    data &lt;- RunTSNE(object = data, dims = dims_TSNE)\n    data &lt;- RunUMAP(object = data, dims = dims_UMAP, verbose = verbose)\n\n    return(data)\n}\n\n来源：https://github.com/sunduanchen/Scissor/blob/master/R/Seurat_preprocessing.R\n\n\n在过去的几年里，UMAP已经取代t-SNE成为scRNA-seq数据的默认可视化方法。与图聚类类似，UMAP生成细胞的最近邻图，根据相似度的强弱对每个细胞间的连接进行加权，然后将图形嵌入到二维空间中。还可以使用PAGA图初始化UMAP，以生成连续开发数据集的高度精确可视化。在实践中，人们发现UMAP在可视化数据集的局部结构方面表现得和t-SNE一样好，包括分离密切相关的细胞类型，同时在可视化数据的全局属性方面表现得更好。因此，对于大多数用户来说，UMAP是一个非常有用的默认可视化选项。对UMAP和t-SNE的其他测试表明，初始化这些方法的方式对它们的整体性能非常重要。实际上，在使用PCA初始化时，t-SNE和UMAP在保存全局结构方面表现得同样好。"
  },
  {
    "objectID": "posts/2023-07-12-journal/index.html",
    "href": "posts/2023-07-12-journal/index.html",
    "title": "2023-07-12",
    "section": "",
    "text": "在看文献的时候发现一个自监督学习，我看英文有个 supervise，以为这是带标签的学习，了解后发现原来并非如此。这里记录下 ChatGPT 关于自监督学习和监督学习的回答。以后有机会实践深入了解下。\n自监督学习和监督学习是机器学习中两种不同的方法。\n监督学习是一种常见的机器学习方法，其中模型通过使用带有标签的训练数据进行训练。在监督学习中，输入和相应的输出之间存在一个明确的映射关系。训练数据包含了输入样本及其对应的预期输出标签。模型根据这些标签来调整自身的参数，以使预测结果尽可能接近于真实标签。例如，在图像分类任务中，监督学习会使用带有标签的图像来训练模型，使其能够根据输入图像正确地预测其所属的类别。\n自监督学习则是一种无监督学习的形式，其中模型在没有人工标签的情况下进行训练。相反，自监督学习利用输入数据中的某种自动生成的标签或信息来进行训练。这些自动生成的标签可以是输入数据的某种变换、部分信息或其他方式生成的。例如，在自我对比学习中，模型会将输入数据分为两个视图，并鼓励模型将它们视为相似。通过最大化这两个视图之间的相似性度量，模型可以学习到数据的有用表示。自监督学习的目标是通过学习数据中的内在结构或模式来训练模型，而无需人工标签。\n总的来说，监督学习依赖于带有人工标签的训练数据，而自监督学习则通过利用数据中的某种自动生成的标签或信息来进行训练。自监督学习在无法获得大量带有标签的数据时尤为有用，并且已经在多个领域中取得了显著的研究进展。"
  },
  {
    "objectID": "posts/2023-07-13-journal/index.html",
    "href": "posts/2023-07-13-journal/index.html",
    "title": "2023-07-13",
    "section": "",
    "text": "今天还是摘要书中看到的话。\n\n我们面临的，就是两个分离的世界：一个世界是我们真实的世界、看得见摸得着的物理世界。 在这里，一切都按照自然规律有条不紊地进行着。另一个是看不见、摸不着的观念世界。 在那里，人们时刻在形成、比较、交换和修正对未来的预期。\n\n\n每个人生活在世界上，都至少面临四种最基本的约束，那就是： 东西不够、时间有限、相互依赖、需要协调。\n\n——《薛兆丰经济学讲义》"
  },
  {
    "objectID": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html",
    "href": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html",
    "title": "Single-cell Workshop 2021 - 01 - scRNASeq 分析基础",
    "section": "",
    "text": "学习资料：https://holab-hku.github.io/Fundamental-scRNA/"
  },
  {
    "objectID": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#准备",
    "href": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#准备",
    "title": "Single-cell Workshop 2021 - 01 - scRNASeq 分析基础",
    "section": "准备",
    "text": "准备\n软件包\n载入下面的包，如果没有就先安装下。\n\nlibrary(Seurat)\n\nAttaching SeuratObject\n\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(hdf5r)\n\n数据\n下载数据：\n\nData used in this material is a 10k PBMC data getting from 10x Genomics website\nhttps://github.com/holab-hku/Fundamental-scRNA/blob/master/data/10k_PBMC.h5"
  },
  {
    "objectID": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#单细胞技术介绍",
    "href": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#单细胞技术介绍",
    "title": "Single-cell Workshop 2021 - 01 - scRNASeq 分析基础",
    "section": "单细胞技术介绍",
    "text": "单细胞技术介绍\n技术流程\n单细胞RNA测序（scRNA-seq）包括一系列技术，以产生许多单个细胞的全基因组表达数据。\n\n细胞分离\n根据技术主要可以分为两类：Droplet-based 和 Non droplet-based。\nDroplet-based：\n\n组织样品必须解离成悬浮液\n细胞将被单独封装成一个油包水滴\n高通量，低成本\n相关技术：Drop-seq (Macosko et al. 2015), inDrop (Klein et al. 2015), Chromium 10X (Zheng et al. 2017)\n\nNon droplet-based：\n\nSmart-seq2 (Ramsköld et al. 2012)：用微型毛细管移液器手工细胞取样\nCEL-seq (Hashimshony et al. 2012)：单个细胞被添加到试管中；第一个介绍了条形码和RNA聚合\nMARS-seq (Jaitin et al. 2014)是第一个使用FACS将单细胞分离到单孔中的方法，优化版MARS-seq2 (Keren-Shaul et al. 2019)的推出成本更低，可重复性更高，井间污染更少\n\n下图显示了一些常见的单细胞分离技术（Hwang, Lee, and Bang 2018）\n\n\n\nSingle Cell Isolation (modified from Hwang, Lee, and Bang 2018)\n\n\n条形码和唯一分子标识符（UMI）\n双端测序输出两个 fastq 文件，分别对应测序的5′和3′方向。使用这种测序技术，配对的第一个read总是与引物的细胞（条形码+UMI）部分一致。\n\n\n\nBiased paired-end reads (David Tse et.al)\n\n\n根据获得的细胞条形码、UMI 和 cDNA 的 reads，我们可以估计转录物的丰度。这允许 mapping 算法区分哪些序列是条形码，哪些是转录序列。因此，在确定细胞条形码和 UMI 条形码序列的长度和位置时，识别用于测序的文库制备化学方法非常重要。\n为了获得 UMIs 的计数，我们可以首先通过细胞条形码对 reads 进行分组，然后对齐 cDNA reads 并使用 UMIs 对每个细胞每个基因的独特分子进行计数。\n\n\n\nGrouping barcodes to assign reads to cells (modified from David Tse et.al)\n\n\n对细胞条形码和 UMIs 的分析包括在校准过程中，我们将在下一节中介绍更多内容。\n广泛应用的scRNA-seq技术汇总\n\n\nMethods\nTranscript coverage\nUMI possibility\nStrand specific\nReferences\n\n\n\nTang method\nNearly full-length\nNo\nNo\nTang et al. (2009)\n\n\nQuartz-Seq\nFull-length\nNo\nNo\nSasagawa et al. (2013)\n\n\nSUPeR-seq\nFull-length\nNo\nNo\nX. Fan et al. (2015)\n\n\nSmart-seq\nFull-length\nNo\nNo\nRamsköld et al. (2012)\n\n\nSmart-seq2\nFull-length\nNo\nNo\nPicelli et al. (2013)\n\n\nMATQ-seq\nFull-length\nYes\nYes\nSheng et al. (2017)\n\n\nSTRT-seq STRT/C1\n5′-only\nYes\nYes\nIslam et al. (2011)\n\n\nCEL-seq\n3′-only\nYes\nYes\nHashimshony et al. (2012)\n\n\nCEL-seq2\n3′-only\nYes\nYes\nHashimshony et al. (2016)\n\n\nMARS-seq\n3′-only\nYes\nYes\nJaitin et al. (2014)\n\n\nCytoSeq\n3′-only\nYes\nYes\nH. C. Fan, Fu, and Fodor (2015)\n\n\nDrop-seq\n3′-only\nYes\nYes\nMacosko et al. (2015)\n\n\nInDrop\n3′-only\nYes\nYes\nKlein et al. (2015)\n\n\nChromium\n3′-only\nYes\nYes\nZheng et al. (2017)\n\n\nSPLiT-seq\n3′-only\nYes\nYes\nRosenberg et al. (2018)\n\n\nsci-RNA-seq\n3′-only\nYes\nYes\nCao et al. (2017)\n\n\nSeq-Well\n3′-only\nYes\nYes\nGierahn et al. (2017)\n\n\nDroNC-seq\n3′-only\nYes\nYes\nHabib et al. (2017)\n\n\nQuartz-Seq2\n3′-only\nYes\nYes\nSasagawa et al. (2018)"
  },
  {
    "objectID": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#预处理和质量控制",
    "href": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#预处理和质量控制",
    "title": "Single-cell Workshop 2021 - 01 - scRNASeq 分析基础",
    "section": "预处理和质量控制",
    "text": "预处理和质量控制\nFASTQ 文件\n原始 RNA 测序数据可能在 FASTQ 文件中。它是一种基于文本的格式，用于存储由单字母代码表示的读序列。 FASTQ 文件中的序列以@符号开头的readID开始，然后是序列数据行，一个简单的加号+分隔符和碱基质量分数。\n它以以下格式表示:\n@ReadID\nREAD SEQUENCE\n+\nSEQUENCING QUALITY SCORES\n一般来说，fastq 文件是使用质量控制工具（如 FastQC）进行预处理的。这将输出一系列评估序列 reads 质量的指标。\nCell Ranger\nCell Ranger 是一组分析管道，用于处理 Chromium 单细胞数据以对齐 reads，生成特征条形码矩阵，执行聚类和其他二次分析等等。 它帮助我们生成 RNA reads 计数矩阵，我们将在学习中使用。\n一些概念：\n\nGEM 孔（以前称为 GEM 组）：来自单个 10x Chromium™ 芯片通道的分隔单元（凝胶颗粒悬浮液）集合。可以从一个 GEM 孔中获得一个或多个测序文库。\n文库（或测序文库）：从单个 GEM 孔中制备的带有 10x 条形码的测序文库。借助特征条形码或 V(D)J 分析，可以从同一个 GEM 孔中创建多个文库。文库类型可能包括基因表达、抗体捕获、CRISPR 引导捕获、TCR 富集等。\n测序 Run（或 Flowcell）: A flowcell containing data from one sequencing instrument run.（这个从英文直译上很难理解，通俗的说就是一次上机测序得到的数据流）\n\n通用流程\n从 Chromium 10X 管道获得的单细胞数据可以使用 cellranger 通过以下工作流程进行处理。\n\n\n\nCellranger Workflow (taken from the cellranger website)\n\n\n输出文件\n下面是我们将从 cellranger 获得的输出文件夹。outs 文件夹包含最终的管道输出文件，其中包括我们需要用于下游分析的内容。\n\n\nOverview of the folder generate from cellranger\n\n\n\nOverview of the outs folder\n\n以上是我们可以在outs文件夹中找到的内容。它包含了一些测序数据的总结信息，注释的读取序列，以及我们通常工作的基因表达矩阵。下面是我们想看的一些重要的输出文件。\n\nMatrices\nWeb Summary .html\nSecondary Analysis CSV\nBAM\nMolecule Info (h5)\nLoupe File (.cloupe)\n\n矩阵\n测序时，Chromium 10X 不仅对转录组进行测序，还对任何可能的分子进行测序。这就导致了背景条形码的存在。细胞相关条形码是cellranger 认为标记来自真实细胞的转录组而不是背景的条形码。\n对于不同版本的cellranger，使用不同的算法来检测细胞相关条形码。一般的想法是，细胞的条形码应该比背景条形码有更多的转录本计数。更多信息请访问：https://kb.10xgenomics.com/hc/en-us/articles/115003480523-How-are-barcodes-classified-as-cell-associated-。\ncellranger管道输出两种类型的特征条形码矩阵：\n\n未经过滤的特征条形码矩阵存储在raw_feature_bc_matrix（1-1）文件夹下。它包含了来自已知良好条形码序列固定列表的每个条形码，其中至少有一个读取，这意味着它包括背景和与细胞相关的条形码。\n经过过滤的特征条形码矩阵存储在filtered_feature_bc_matrix（1-2）文件夹下。它只包括已检测到的与细胞相关的条形码。\n\nraw_feature_bc_matrix和filtered_feature_bc_matrix文件夹中都包含三个文件。\n\nmatrix.mtx.gz文件以稀疏矩阵的形式存储 reads 计数，其中每一行表示scRNA-seq数据中的一个基因，每一列表示一个细胞。行索引的信息存储在features.tsv.gz文件中，而列索引的信息存储在barcodes.tsv.gz文件中。\nfeatures.tsv.gz文件对应于行索引。在scRNA-seq数据中，“features”实际上指的是基因。features.tsv.gz文件包含三列：feature ID：参考GTF文件的注释字段中的gene_id，表示特征的ID。 name：参考GTF文件的注释字段中的gene_name，如果参考GTF中没有gene_name字段，则基因名称等同于基因ID。 type of feature：描述特征类型，可以是Gene Expression、Antibody Capture、CRISPR或CUSTOM之一。对于scRNA-seq数据，它将是Gene Expression（基因表达）。\nbarcodes.tsv.gz对应于列索引，它包含了每个列的条形码。有关条形码序列格式的更多详细信息，请参考条形码BAM部分。\n\nWeb Summary .html\n一个概要的HTML文件包含了摘要指标和自动化二次分析结果。如果在流程运行期间检测到问题，将在此页面上显示警报。\n该HTML文件包括两个部分，SUMMARY（摘要）和ANALYSIS（分析）。您还可以点击每个仪表板顶部的“?”以获取有关每个指标的更多信息。\n\nSUMMARY（摘要）指标描述了测序质量和检测到的细胞的各种特征。在页面顶部附近醒目地显示了检测到的细胞数量、每个细胞的平均reads数以及每个细胞检测到的基因的中位数。\n\n\n\nCells dashboard (modified from the cellranger website)\n\n\n在“Cells”仪表板下的“Barcode Rank Plot”显示了条形码计数的分布以及被推断与细胞相关的条形码。y轴表示每个条形码映射到的UMI计数，x轴表示低于该值的条形码数量。陡峭的下降表示细胞关联的条形码与与空分区相关的条形码之间有良好的分离。条形码可以通过其UMI计数或其RNA配置文件确定为与细胞相关，因此图表的某些区域可能同时包含细胞关联和背景关联的条形码。图表的颜色表示与细胞关联的条形码的局部密度。\n\n\n\nSequencing and Mapping dashboards (modified from the cellranger website)\n\n其他用于评估的指标：\n\n\nEstimated Number of Cells: 500-10,000\n\nMean Reads per Cell: 20,000 reads/cell minimum recommended reads per cell around 10,000\n\nValid barcodes: greater than 75%\n\nQ30 bases in RNA read: ideally greater than 65%\n\nReads mapped confidently to transcriptome: ideally greater than 30%\n\nReads mapped antisense to gene: ideally smaller than 10%\n\n\n分析（ANALYSIS）指标包括以下自动化的二次分析：\n\n\n降维分析：将细胞投影到二维空间（t-SNE），以展示它们之间的关系。\n自动聚类分析：将具有相似表达特征的细胞分组在一起，形成聚类。\n差异表达基因列表：列出在所选聚类之间表达差异显著的基因。\n显示测序深度降低对观察到的文库复杂性的影响的图表。\n显示测序深度降低对每个细胞检测到的平均基因数的影响的图表。\n\nBAM\nBAM文件以二进制压缩格式保存了关于 mapping reads 的信息。它由可选的头部部分和对齐部分组成。如果存在头部部分，它将通过第一列中的@与对齐部分区分，并位于对齐部分之前。\n当解压缩成SAM文件时，信息以制表符分隔的表格形式存储，其中包含一些标准列和由 Cell Ranger 软件生成的特定列。Cell Ranger特定的列包含与 BAM 条形码、BAM 对齐和特征条形码相关的信息。标准列对应以下内容：\nQNAME : read name (generally will include UMI barcode if applicable)\nFLAG : number tag indicating the “type” of alignment, link to explanation of all possible “types”\nRNAME : reference sequence name (i.e. chromosome read is mapped to).\nPOS : leftmost mapping position\nMAPQ : Mapping quality\nCIGAR : string indicating the matching/mismatching parts of the read (may include soft-clipping).\nRNEXT : reference name of the mate/next read\nPNEXT : POS for mate/next read\nTLEN : Template length (length of reference region the read is mapped to)\nSEQ : read sequence\nQUAL : read quality\n可以使用 SAMtools 查看 BAM 文件：\nsamtools view output.bam\n为了进行基于 RNA 速率的轨迹分析，需要 bam 文件。\n分子信息（h5）\n它是一个 HDF5 文件，包含有效条形码和有效 UMI 的所有分子的每分子信息，并以高置信度分配给基因或特征条形码。该 HDF5 文件包含与观察到的分子相对应的数据，以及有关所使用的库和特征集的数据。这个文件的结构是：\n\n\nHDF5 File Hierarchy(taken from the cellranger website)\n\n二级分析 CSV 文件\n包含自动辅助分析结果的几个 CSV 文件。它包含有关降维、t-SNE、UMAP、聚类和差分表达的信息。它也通过Web Summary.html文件在ANALYSIS度量中可视化。\nLoupe 文件 (.cloupe)\nLoupe Browser 是一款桌面应用程序，提供与10x Genomics解决方案的数据进行交互式可视化分析的功能。它可以帮助寻找感兴趣的细胞、发现重要基因、识别细胞类型、探索细胞亚结构、研究细胞亚型、集成基因表达和V(D)J分析，并共享结果。\n通过 Loupe Browser 可以查看 Loupe 文件，该文件包含以下信息：\n\n样本中细胞的基因表达信息。\n细胞的各种基于基因表达的聚类信息，包括t-SNE和UMAP投影以及差异基因表达情况。\n来自转录组参考的基因注释信息。\nSTARsolo\nSTARsolo（Kaminow，Yunusov和Dobin 2021）是一个专为液滴式单细胞RNA测序数据（例如10X Genomics Chromium系统）设计的分析工具，直接内嵌在STAR代码中。STARsolo的输入是一个FASTQ文件，它可以以与Cell Ranger几乎相同的格式输出基因计数，但速度约快10倍。\nSTARsolo 程序输出大量反映 reads 比对过程细节的文件。在这里，我们只讨论其中一些关键文件。\n\n\nSTARsolo Output\n\n\nBAM文件包含有关比对 reads 的信息，与通过Cell Ranger生成的BAM文件非常相似。\n比对摘要文件Features.stats和Summary.csv包含有关基本比对细节的信息。这可以作为对比对过程进行简单初步质量控制检查的便捷工具。\n特征矩阵文件matrix.mtx包含每个个体细胞中映射的基因计数信息。列名对应于每个个体细胞的条形码，行名对应于所有注释的基因。由于该数据的规模较大，它以稀疏矩阵的形式存储。\n辅助文件 barcodes.tsv 和 features.tsv 提供了下游分析所需的额外元数据。这些文件连同矩阵文件在功能上扮演着与Cell Ranger输出中的 Matrices 部分相同的角色，并且在金标准的 scRNA-seq 数据分析软件 seurat 中进行分析时是必需的。\nDoublets\n双细胞是指虽然设计为由一个细胞生成，但却是由两个细胞生成的人工文库。通常这是由于细胞分选或捕获过程中的错误引起的。\n可以使用几种实验策略来去除双细胞：\n\n基于不同供体个体之间的自然遗传变异（Kang等人，2018）。\n使用与不同寡核苷酸结合的抗体标记一组细胞（例如，来自一个样本的所有细胞）。在混合后，观察到具有不同寡核苷酸的文库被视为双细胞并移除（Bach等人，2017）。\n仅基于表达谱的计算方法（如模拟）推断双细胞并进行去除。"
  },
  {
    "objectID": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#使用-seurat-进行分析",
    "href": "posts/2023-07-13-fundamentals-of-scrnaseq-analysis/index.html#使用-seurat-进行分析",
    "title": "Single-cell Workshop 2021 - 01 - scRNASeq 分析基础",
    "section": "使用 Seurat 进行分析",
    "text": "使用 Seurat 进行分析\n本节的内容是根据Seurat - Guided Clustering Tutorial进行了一些修改和适应。我们使用的数据是从10x Genomics网站获取的10k PBMC数据。\n我们将学习如何读取10X测序数据并将其转换为Seurat对象，进行质控和选择用于进一步分析的细胞，对数据进行归一化处理，识别高可变特征（特征选择），对数据进行缩放，进行线性降维和可视化分析。\nSeurat 对象\nSeurat对象充当一个容器，其中包含单个单细胞数据集的数据（如计数矩阵）和分析（如PCA或聚类结果）。在使用 Seurat 分析scRNA-seq数据之前，我们可以先从这里对Seurat对象有一些基本的了解。\n设置Seurat对象\n\nlibrary(dplyr)\nlibrary(Seurat)\nlibrary(patchwork)\nlibrary(ggplot2)\n\n我们从读入数据开始。Read10X_h5从10X CellRanger hdf5文件读取计数矩阵，返回唯一的分子识别（UMI）计数矩阵。该矩阵中的值表示每个特征（即基因；行）在每个细胞（列）中检测到的表达量。它可用于读取scATAC-seq和scRNA-seq矩阵。\n接下来我们使用计数矩阵来创建一个Seurat对象。\n\n# Load the PBMC dataset\npbmc.data &lt;- Read10X_h5(\"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/10k_PBMC.h5\")\n# Initialize the Seurat object with the raw (non-normalized data).\npbmc &lt;- CreateSeuratObject(counts = pbmc.data, project = \"pbmc10k\", min.cells = 3, min.features = 200)\npbmc\n\nAn object of class Seurat \n22432 features across 10813 samples within 1 assay \nActive assay: RNA (22432 features, 0 variable features)\n\n\n如果我们想直接使用 cellranger 管道从 10X 读取数据，我们可以使用 Read10X()。\n标准预处理流程\n以下步骤包含了 Seurat 中 scRNA-seq 数据的标准预处理工作流程。它们基于我们将从 Cell Ranger 或 STARsolo 输出中获得的 RNA reads 计数矩阵。标准的预处理工作流程代表了基于 QC 指标的细胞选择和过滤，数据归一化和缩放，以及高度可变特征的检测。\nQC 和细胞过滤\nSeurat 允许你根据任何用户定义的标准轻松探索 QC 指标和过滤细胞。社区常用的一些质量控制指标（Ilicic et al. 2016）包括：\n\n在每个细胞中检测到的独特基因的数量\n\n低质量的细胞或空液滴通常只有很少的基因\n细胞双胞胎或多胞胎可能表现出异常高的基因计数\n同样，细胞内检测到的分子总数（与独特的基因密切相关）\n\n\nreads 占到线粒体基因组的百分比\n\n低质量/垂死细胞通常表现出广泛的线粒体污染\n我们使用PercentageFeatureSet()函数计算线粒体 QC 指标，该函数计算源自一组特征的计数百分比\n我们用 MT 开头的所有基因集合作为线粒体基因集合\n\n\n\n\n# The [[ operator can add columns to object metadata. This is a great place to stash QC stats\npbmc[[\"percent.mt\"]] &lt;- PercentageFeatureSet(pbmc, pattern = \"^MT-\")\n\n在 CreateSeuratObject() 过程中自动计算唯一基因和总分子的数量。它们存储在对象元数据中。\n\n# Show QC metrics for the first 5 cells in the control group\nhead(pbmc@meta.data, 5)\n\n                   orig.ident nCount_RNA nFeature_RNA percent.mt\nAAACCCAGTATATGGA-1    pbmc10k        860          350 44.1860465\nAAACCCAGTATCGTAC-1    pbmc10k       1548          729  0.4521964\nAAACCCAGTCGGTGAA-1    pbmc10k       6387         1827 10.4117739\nAAACCCAGTTAGAAAC-1    pbmc10k      16664         3744  5.2808449\nAAACCCAGTTATCTTC-1    pbmc10k       3352         1464 13.8424821\n\n\n我们可以可视化 nFeature_RNA, nCount_RNA 和 percent.mt 作为 QC 指标。\n与 Cell Ranger 输出一样，以下结果中的 feature 表示基因。 nFeature_RNA 是每个细胞中检测到的基因数量。 nCount_RNA 是细胞内检测到的分子总数。 下图中的每个点代表一个细胞。\n\n# Visualize QC metrics as a violin plot\nVlnPlot(pbmc, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), \n        ncol = 3, pt.size = 0.0001)\n\n\n\n\n我们接着使用点图来表示 3 者之间的关系。\n\n# FeatureScatter is typically used to visualize feature-feature relationships, but can be used\n# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.\nplot1 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\") + \n  theme(legend.position=\"none\")\nplot2 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") + \n  theme(legend.position=\"none\")\nplot1 + plot2\n\n\n\n\n在这里，我们过滤掉具有独特特征计数（基因）超过5000或少于200的细胞。我们也过滤掉线粒体数量大于15%的细胞。\n\npbmc &lt;- subset(pbmc, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 5000 & percent.mt &lt; 15)\n\n然后再一次用提琴图查看过滤后的质量图：\n\n# Visualize QC metrics as a violin plot\nVlnPlot(pbmc, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), \n        ncol = 3, pt.size = 0.0001)\n\n\n\nplot1 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\") + \n  theme(legend.position=\"none\")\nplot2 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\") + \n  theme(legend.position=\"none\")\nplot1 + plot2\n\n\n\n\n标准化数据\n从数据集中删除不需要的细胞后，下一步是对数据进行标准化（规范化）。默认情况下，我们使用全局缩放归一化方法“LogNormalize”，该方法通过总表达对每个细胞的特征表达式测量值进行归一化，将其乘以一个比例因子（默认为10,000），并对结果进行对数变换。标准化值存储在pbmc[[\"RNA\"]]@data中。\n\npbmc &lt;- NormalizeData(pbmc, \n          normalization.method = \"LogNormalize\", \n          scale.factor = 10000, verbose = FALSE)\n\n在这里，我们从基因表达矩阵中采样 10,000 个reads计数，分别可视化标准化前后的基因表达分布（不包括零）。\n\n# set seed and put two plots in one figure\nset.seed(123)\npar(mfrow=c(1,2))\n# original expression distribution\nraw_geneExp = as.vector(pbmc[['RNA']]@counts) %&gt;% sample(10000)\n\nWarning in .sparse2v(x): sparse-&gt;dense coercion: allocating vector of size 1.6\nGiB\n\nraw_geneExp = raw_geneExp[raw_geneExp != 0]\nhist(raw_geneExp)\n# expression distribution after normalization\nlogNorm_geneExp = as.vector(pbmc[['RNA']]@data) %&gt;% sample(10000)\n\nWarning in .sparse2v(x): sparse-&gt;dense coercion: allocating vector of size 1.6\nGiB\n\nlogNorm_geneExp = logNorm_geneExp[logNorm_geneExp != 0]\nhist(logNorm_geneExp)\n\n\n\n\n识别高度可变的特征（特征选择）\n接下来，我们计算数据集中表现出高细胞间差异的特征子集（即，它们在一些细胞中高表达，而在其他细胞中低表达）。研究表明(Brennecke et al. 2013)在下游分析中关注这些基因有助于突出单细胞数据集中的生物信号。\nSeurat中的过程详细描述在文献 Stuart et al. 2019 中，FindVariableFeatures()函数中实现直接建模单细胞数据中固有的均值-方差关系来改进以前的版本。默认情况下，Seurat每个数据集返回2000个特征，这些特征将用于下游分析，如PCA。\n\npbmc &lt;- FindVariableFeatures(pbmc, selection.method = \"vst\", nfeatures = 2000, verbose = FALSE)\n\n# Identify the 10 most highly variable genes\ntop10 &lt;- head(VariableFeatures(pbmc), 10)\n\n# plot variable features with and without labels\nplot1 &lt;- VariableFeaturePlot(pbmc) + \n  theme(legend.position=\"top\")\nplot2 &lt;- LabelPoints(plot = plot1, points = top10, repel = TRUE) + \n  theme(legend.position=\"none\")\n\nWhen using repel, set xnudge and ynudge to 0 for optimal results\n\nplot1 + plot2\n\nWarning: Transformation introduced infinite values in continuous x-axis\nTransformation introduced infinite values in continuous x-axis\n\n\n\n\n\n缩放数据\n接下来，我们应用线性变换(“缩放”)，这是在PCA等降维技术之前的标准预处理步骤。函数ScaleData()：\n\n移动每个基因的表达，使细胞间的平均表达为 0。\n缩放每个基因的表达，使细胞间的方差为 1。这一步在下游分析中具有同等的权重，因此高表达基因不会占主导地位。\n结果存储在 pbmc[[\"RNA\"]]@scale.data。\n\n\nall.genes &lt;- rownames(pbmc)\npbmc &lt;- ScaleData(pbmc, features = all.genes, verbose = FALSE)\n\n在使用上述命令时，我们使用所有基因来缩放数据。缩放是 Seurat 工作流程中的重要步骤，但仅限于将用作 PCA 输入的基因。因此，ScaleData()中的默认仅对先前确定的可变特征（默认值为2000）执行缩放。这将使这一步更快。\n在这种情况下，我们的PCA和聚类结果将不受影响。然而，Seurat 热图（用DoHeatmap()生成，如下图所示）需要对热图中的基因进行缩放，以确保高表达的基因不会在热图中占主导地位。我们在本教程中缩放了所有基因。\nScaleData()函数还允许我们从单个单细胞数据集中删除不需要的变异源。例如，我们可以通过回归移除细胞周期阶段或线粒体污染相关的异质性。这个特性可以通过指定vars.to.regress来实现。即：\n# skip here\npbmc &lt;- ScaleData(pbmc, vars.to.regress = \"percent.mt\")\n但是，特别是对于想要使用此功能的高级用户，Seurat 建议使用他们新的标准化工作流 SCTransform()。该方法在 Seurat 论文(Hafemeister and Satija 2019)中进行了描述，并有一个单独的技术文档。与ScaleData() 一样，函数SCTransform()也包含一个vars.to.regress参数。\n执行线性降维\n接下来，我们对缩放后的数据执行 PCA。默认情况下，只使用先前确定的可变特征作为输入，但是如果你希望选择不同的子集，则可以使用 features 参数定义。\n\npbmc &lt;- RunPCA(pbmc, features = VariableFeatures(object = pbmc), verbose = FALSE)\n\nSeurat提供了几种有用的方法来可视化定义 PCA 的细胞和特征，包括VizDimLoadings()、DimPlot() 和 DimHeatmap()。\n\n# Examine and visualize PCA results a few different ways\nprint(pbmc[[\"pca\"]], dims = 1:5, nfeatures = 5)\n\nPC_ 1 \nPositive:  LTB, IL32, TRAC, CD3D, TRBC2 \nNegative:  FCN1, FGL2, CST3, IFI30, TYMP \nPC_ 2 \nPositive:  MS4A1, CD79A, BANK1, IGHM, NIBAN3 \nNegative:  IL32, GZMM, CD3D, CD7, CD247 \nPC_ 3 \nPositive:  GZMB, CLIC3, NKG7, GNLY, KLRD1 \nNegative:  CCR7, LEF1, TRABD2A, TCF7, LTB \nPC_ 4 \nPositive:  CD79B, MS4A1, GNLY, CD79A, LINC00926 \nNegative:  LILRA4, CLEC4C, SERPINF1, TPM2, SCT \nPC_ 5 \nPositive:  CDKN1C, HES4, CTSL, TCF7L2, BATF3 \nNegative:  S100A12, ITGAM, VCAN, CES1, MGST1 \n\n\n\nVizDimLoadings(pbmc, dims = 1:2, reduction = \"pca\")\n\n\n\n\n\nDimPlot(pbmc, reduction = \"pca\")\n\n\n\n\n特别是 `DimHeatmap() 允许轻松地探索数据集中异构的主要来源，并且在试图决定将哪些pc包括在进一步的下游分析中时非常有用。\n\nDimHeatmap(pbmc, dims = 1, cells = 500, balanced = TRUE)\n\n\n\n\n\nDimHeatmap(pbmc, dims = 1:9, cells = 500, balanced = TRUE)\n\n\n\n\n为了克服scRNA-seq数据中任何单个特征中广泛的技术噪声，Seurat基于它们的PCA分数对细胞进行聚类，每个 PC 本质上代表一个“元特征”，该元特征结合了相关特征集中的信息。因此，顶部主成分代表了数据集的鲁棒压缩。在这里，我们选择前20 个 PC。\n细胞聚类\nSeurat v3采用了基于图的聚类方法，基于(Macosko等人，2015)中的初始策略进行改进。重要的是，驱动聚类分析的距离度量（基于先前确定的主成分）保持不变。然而，Seurat在将细胞距离矩阵划分为聚类的方法有了显著改进。这种方法受到最近一些研究的启发，这些研究将基于图的聚类方法应用于scRNA-seq数据（Xu和Su，2015）和CyTOF数据（Levine等人，2015）。简而言之，这些方法将细胞嵌入到一个图结构中，例如K最近邻（KNN）图，其中类似特征表达模式的细胞之间有边连接，然后尝试将该图划分为高度相互连接的“拟团”或“社区”。\n就像PhenoGraph一样，首先根据PCA空间中的欧氏距离构建了一个K最近邻图，然后根据局部邻域中的重叠情况（Jaccard相似性）调整任意两个细胞之间的边权重。这个步骤是使用FindNeighbors()函数完成的，它的输入是之前定义的数据集的维度（前20个主成分）。\n为了对细胞进行聚类，接下来我们应用模块度优化技术，如Louvain算法（默认）或SLM（Blondel等人，2008），以迭代地将细胞分组在一起，目标是优化标准的模块度函数。FindClusters()函数实现了这个过程，并包含一个分辨率参数，用于设置下游聚类的“粒度”，增大的值会导致更多的聚类。我们发现，在大约3K个细胞的单细胞数据集中，将此参数设置在 0.4-1.2 之间通常会得到良好的结果。对于更大的数据集，通常需要增加最佳分辨率。可以使用Idents()函数找到这些聚类。\n\npbmc &lt;- FindNeighbors(pbmc, dims = 1:20, verbose = FALSE)\npbmc &lt;- FindClusters(pbmc, resolution = 0.5, verbose = FALSE)\n\n\n# Look at cluster IDs of the first 5 cells\nhead(Idents(pbmc), 5)\n\nAAACCCAGTATCGTAC-1 AAACCCAGTCGGTGAA-1 AAACCCAGTTAGAAAC-1 AAACCCAGTTATCTTC-1 \n                 2                  4                  2                  8 \nAAACCCAGTTGCCGAC-1 \n                 0 \nLevels: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n\n\n运行非线性降维 UMAP/tSNE\nSeurat提供多种非线性降维技术，例如tSNE和UMAP，用于可视化和探索这些数据集。这些算法的目标是学习数据的潜在流形，以便将相似的细胞放置在低维空间中的一起。在上述基于图的聚类中确定的细胞应在这些降维图中共同定位。作为UMAP和tSNE的输入，我们建议使用与聚类分析相同的主成分作为输入。\n\npbmc &lt;- RunUMAP(pbmc, dims = 1:20, verbose = FALSE)\n\nWarning: The default method for RunUMAP has changed from calling Python UMAP via reticulate to the R-native UWOT using the cosine metric\nTo use Python UMAP via reticulate, set umap.method to 'umap-learn' and metric to 'correlation'\nThis message will be shown once per session\n\n\n然后得到单细胞聚类结果的UMAP图。\n\nDimPlot(pbmc, reduction = \"umap\")\n\n\n\n\n类似地，tSNE。\n\npbmc &lt;- RunTSNE(pbmc, dims = 1:20, verbose = FALSE)\nDimPlot(pbmc, reduction = \"tsne\")\n\n\n\n\n贴上聚类标签：\n\nDimPlot(pbmc, reduction = \"umap\", label = TRUE)\n\n\n\nplot &lt;- DimPlot(object = pbmc)\nLabelClusters(plot = plot, id = 'ident')\n\n\n\n\n是时候保存我们的分析结果了：\n\nsaveRDS(pbmc, \nfile = \"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/pbmc_processed.rds\")"
  },
  {
    "objectID": "posts/2023-07-18-journal/index.html",
    "href": "posts/2023-07-18-journal/index.html",
    "title": "2023-07-18",
    "section": "",
    "text": "Without hard work, nothing grows but weeds."
  },
  {
    "objectID": "posts/2023-07-20-journal/index.html",
    "href": "posts/2023-07-20-journal/index.html",
    "title": "2023-07-20",
    "section": "",
    "text": "今天很是犯困😪，很困。\n可能因为要早起找人签字，中午睡得昏昏沉沉。\n可能今天早上下午都有会议，坐久后的麻痹。\n可能今天精神太过紧绷，站中特别资助没上，又一直在编写和测试分析流程。\n总之，今天确实很犯困。一天看是什么都没做，但好歹有了记录，记录今天大体发生过什么。\n😪"
  },
  {
    "objectID": "posts/2023-07-21-how-do-you-think-bioinformatics-in-science/index.html",
    "href": "posts/2023-07-21-how-do-you-think-bioinformatics-in-science/index.html",
    "title": "你认同生物信息学不属于科学吗？",
    "section": "",
    "text": "刚看到 Zuguang Gu 发的推文，唔，国内生物学术界有这种看法我是生气的，并认为可耻。 因为这里的某个研究机构在收到求职信时，都没有了解过这个领域和它的贡献就妄下评判。 这体现一种自傲与无知。就没有想过世界存在即有理由吗？评估学术求职者考虑的不应该是 学科建设的契合度，求职人的科研态度、经历、成果吗？\n如果对技术的基础探索不属于科学范畴，那当前热门改造世界的伟大技术，比如CRISPR，那些研究、优化和利用它的人是不是也都不是在做科学呢？😄\n那些在实验室搬砖的打工人，那些在提蛋白的结构生物学研究生，那些将被技术/机器取代的科研岗位，他们都不属于科研吗？\n学术，永远是对知识和创新无穷探索的贡献，而无关方式。\n我其实愈加认识到，学术界，特别是国内的，还是人的江湖。关于人，我是既鄙夷又赞美。 我也不知道未来如落叶的我会飘向何方，且行且看，但我越来越感觉，尽力保持一颗“童心”才能不负生命。"
  },
  {
    "objectID": "posts/2023-07-21-academic-culture/index.html",
    "href": "posts/2023-07-21-academic-culture/index.html",
    "title": "学术的价值观",
    "section": "",
    "text": "这篇小文主要是记录、推荐和分享下吴思涵 Lab 的科学价值观条目。\n\n我觉得非常好，也非常佩服吴思涵创建这个条目和对每个进入 Lab 的成员进行宣读。\n有时候，科研工作的环境的喧嚣让人煎熬。在心中悬着一把价值之剑🗡️，我觉得既是告诫，也是指引。 这把剑有离得近和离得远的时候，但如果没有它，我们的心中会缺乏终极目标🎯。 科学的路，是永无止境的，我们一直在寻觅、挣扎、成长，也弯弯曲曲地走向岁月长河的下一步， 但是否在前进、是否在后退，是否能永远前进、是否会永远后退，真的不知道。\n当前，全国进入了“卷”时代，respect science value, keep deep think。"
  },
  {
    "objectID": "posts/2023-07-25-journal/index.html",
    "href": "posts/2023-07-25-journal/index.html",
    "title": "2023-07-25",
    "section": "",
    "text": "为什么事情容易堆积在一起发生？\n为什么越忙的时候越不想忙？忙的时候越容易分散精力？\n这是人类的什么心理呢？\n那些先圣先贤们究竟是如何塑造的呢？\n是什么在压抑着思潮，捶打着自信？\n怎样才能把想做的真正做好？不关注别人？多关注别人？\n哪里是智慧的真谛，生命的妙门？"
  },
  {
    "objectID": "posts/2023-07-25-small-notes/index.html",
    "href": "posts/2023-07-25-small-notes/index.html",
    "title": "小记（往昔）",
    "section": "",
    "text": "以时间倒序的形式备份下「小记」中写的部分诗。\n\n贺妻小记\n山水在回响你的生辰，\n我从中窥探生命的印记。\n缘来了——\n我作亭亭夏日的莲，\n你作微风，\n荡开我湖心。\n\n\n写于二十九\n实岁添九虚添十，\n已望天命瘦肚迟。\n天南地北终须尽，\n人到桥头有诀期。\n\n\n无题\n翠绿叠墨色，\n烟柳入江波。\n排水碧空尽，\n推云眺山郭。\n\n\n无题\n雨骤风萧天易冷，\n寒推漏窗夜捶门。\n鸟叫迟起难行早，\n世事情劫难度成。\n\n\n八月十四与妻登白云山\n山林日曳金明灭，\n乔木深深松涧叠。\n晚起上白云，\n双双汗水淋。\n来时云遮月，\n去时月动人。\n清风频相送，\n人月了无痕。\n\n\n二月\n人道江南好颜色，\n冷风吹起亮春泽。\n近观粉花掩半绿，\n远眺乌波驻高楼。\n骑行频闻鸣翠柳，\n游思偶得壮年愁。\n愿此人间多欢乐，\n笑声归处了白头。"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "",
    "text": "学习资料：GitHub 地址"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#数据下载",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#数据下载",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "数据下载",
    "text": "数据下载\n我们将使用包含大约10K个单细胞的PBMC数据集。这个数据集是由10X基因组公司公开提供的，可以从 https://www.dropbox.com/s/wn4mgwkkzqw2pox/SC3_v3_NextGem_DI_PBMC_10K_filtered_feature_bc_matrix.h5?dl=0 下载。"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#准备",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#准备",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "准备",
    "text": "准备\n这个文章的内容灵感来自多个Seurat 文档。它假定你已经熟悉了最初的质控步骤。我们将从由cellranger生成的数据 x 细胞过滤矩阵开始，这是大多数分析的常见起点。\n我们将专注于单细胞RNA测序分析的两个具体关键任务：聚类和对已识别聚类的注释。"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#安装包",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#安装包",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "安装包",
    "text": "安装包\ninstall.packages(\"Seurat\")\ninstall.packages(\"hdf5r\")\ninstall.packages(\"clustree\")\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"SingleR\")\nBiocManager::install(\"celldex\")\n加载：\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(Seurat)\n\nAttaching SeuratObject\n\nlibrary(patchwork)\nlibrary(clustree)\n\nLoading required package: ggraph\n\n\nLoading required package: ggplot2\n\nlibrary(SingleR)\n\nLoading required package: SummarizedExperiment\n\n\nLoading required package: MatrixGenerics\n\n\nLoading required package: matrixStats\n\n\n\nAttaching package: 'matrixStats'\n\n\nThe following object is masked from 'package:dplyr':\n\n    count\n\n\n\nAttaching package: 'MatrixGenerics'\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    colAlls, colAnyNAs, colAnys, colAvgsPerRowSet, colCollapse,\n    colCounts, colCummaxs, colCummins, colCumprods, colCumsums,\n    colDiffs, colIQRDiffs, colIQRs, colLogSumExps, colMadDiffs,\n    colMads, colMaxs, colMeans2, colMedians, colMins, colOrderStats,\n    colProds, colQuantiles, colRanges, colRanks, colSdDiffs, colSds,\n    colSums2, colTabulates, colVarDiffs, colVars, colWeightedMads,\n    colWeightedMeans, colWeightedMedians, colWeightedSds,\n    colWeightedVars, rowAlls, rowAnyNAs, rowAnys, rowAvgsPerColSet,\n    rowCollapse, rowCounts, rowCummaxs, rowCummins, rowCumprods,\n    rowCumsums, rowDiffs, rowIQRDiffs, rowIQRs, rowLogSumExps,\n    rowMadDiffs, rowMads, rowMaxs, rowMeans2, rowMedians, rowMins,\n    rowOrderStats, rowProds, rowQuantiles, rowRanges, rowRanks,\n    rowSdDiffs, rowSds, rowSums2, rowTabulates, rowVarDiffs, rowVars,\n    rowWeightedMads, rowWeightedMeans, rowWeightedMedians,\n    rowWeightedSds, rowWeightedVars\n\n\nLoading required package: GenomicRanges\n\n\nLoading required package: stats4\n\n\nLoading required package: BiocGenerics\n\n\n\nAttaching package: 'BiocGenerics'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    combine, intersect, setdiff, union\n\n\nThe following objects are masked from 'package:stats':\n\n    IQR, mad, sd, var, xtabs\n\n\nThe following objects are masked from 'package:base':\n\n    anyDuplicated, aperm, append, as.data.frame, basename, cbind,\n    colnames, dirname, do.call, duplicated, eval, evalq, Filter, Find,\n    get, grep, grepl, intersect, is.unsorted, lapply, Map, mapply,\n    match, mget, order, paste, pmax, pmax.int, pmin, pmin.int,\n    Position, rank, rbind, Reduce, rownames, sapply, setdiff, sort,\n    table, tapply, union, unique, unsplit, which.max, which.min\n\n\nLoading required package: S4Vectors\n\n\n\nAttaching package: 'S4Vectors'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    first, rename\n\n\nThe following objects are masked from 'package:base':\n\n    expand.grid, I, unname\n\n\nLoading required package: IRanges\n\n\n\nAttaching package: 'IRanges'\n\n\nThe following objects are masked from 'package:dplyr':\n\n    collapse, desc, slice\n\n\nLoading required package: GenomeInfoDb\n\n\nLoading required package: Biobase\n\n\nWelcome to Bioconductor\n\n    Vignettes contain introductory material; view with\n    'browseVignettes()'. To cite Bioconductor, see\n    'citation(\"Biobase\")', and for packages 'citation(\"pkgname\")'.\n\n\n\nAttaching package: 'Biobase'\n\n\nThe following object is masked from 'package:MatrixGenerics':\n\n    rowMedians\n\n\nThe following objects are masked from 'package:matrixStats':\n\n    anyMissing, rowMedians\n\n\n\nAttaching package: 'SummarizedExperiment'\n\n\nThe following object is masked from 'package:SeuratObject':\n\n    Assays\n\n\nThe following object is masked from 'package:Seurat':\n\n    Assays\n\nlibrary(celldex)\n\n\nAttaching package: 'celldex'\n\n\nThe following objects are masked from 'package:SingleR':\n\n    BlueprintEncodeData, DatabaseImmuneCellExpressionData,\n    HumanPrimaryCellAtlasData, ImmGenData, MonacoImmuneData,\n    MouseRNAseqData, NovershternHematopoieticData"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#读入数据集并创建一个-seurat-对象",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#读入数据集并创建一个-seurat-对象",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "读入数据集并创建一个 Seurat 对象",
    "text": "读入数据集并创建一个 Seurat 对象\n首先，我们要读取10X测序数据并将其转换为 Seurat 对象。Seurat 对象作为一个容器，最初只包含 UMI 计数矩阵。但我们会向其添加更多的分析内容（例如PCA、聚类结果）。\n我们首先通过使用 Read10X_h5 函数读取计数矩阵，该函数从10X CellRanger hdf5文件中读取计数矩阵。层次数据格式（Hierarchical Data Format，HDF5或H5）提供了数据的更压缩的表示形式。Seurat 软件包包含多个读取函数，具体取决于文件格式。\n在读取数据时，我们会应用基本的质量控制，将低质量的细胞丢弃。\n\npbmc.data &lt;- Read10X_h5(filename=\"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/SC3_v3_NextGem_DI_PBMC_10K_filtered_feature_bc_matrix.h5\")\npbmc &lt;- CreateSeuratObject(counts = pbmc.data, project = \"pbmc10k\", min.cells = 3, min.features = 200)\npbmc[[\"percent.mt\"]] &lt;- PercentageFeatureSet(pbmc, pattern = \"^MT-\")\npbmc &lt;- subset(pbmc, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 5000 & percent.mt &lt; 15)\n\n接下来，我们计算线粒体RNA在总RNA含量中的百分比贡献。高线粒体含量可能表明正在经历凋亡的低质量细胞。我们根据线粒体含量和每个细胞特征数的分布再次进行质量控制。特征数过低可能表明空的液滴中存在环境RNA污染。特征数过高可能是由于多个细胞被困在同一个液滴中引起的。"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#数据处理",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#数据处理",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "数据处理",
    "text": "数据处理\n经过质量控制措施以选择继续分析的细胞后，接下来的步骤包括对数据进行归一化、识别高度可变特征以及进行尺度缩放。同时，还会进行主成分分析，因为许多后续分析步骤将在较低维度空间中进行计算。\n\npbmc &lt;- NormalizeData(pbmc)\npbmc &lt;- FindVariableFeatures(pbmc, selection.method = \"vst\", nfeatures = 2000)\nall.genes &lt;- rownames(pbmc)\npbmc &lt;- ScaleData(pbmc, features = all.genes)\n\nCentering and scaling data matrix\n\n\n\npbmc &lt;- RunPCA(pbmc, verbose = FALSE)\n\n前面所有步骤的基本原理都与质量控制相关，在前面的教程中有更详细的介绍。"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#选择主成分的数量",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#选择主成分的数量",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "选择主成分的数量",
    "text": "选择主成分的数量\n首要的重要决策是在后续分析中保留多少个主成分（PCs）。保留的主成分越多，信号量也会增加，但同时也会增加噪音，并且计算需求也会增加。在做出这个选择之前，请检查与每个主成分相关的基因。存在与特定细胞类型相关的基因将表明该主成分具有信息量。而存在与所有不相关基因名称相关的情况则表明相反。在外周血单个核细胞（PBMC）的背景下，我们预期主要的细胞类型将包括T细胞、B细胞、NK细胞、单核细胞等等。\n检查前 10 个 PC，你会看到很多熟悉的基因名称。\n\nprint(pbmc[[\"pca\"]], dims = 1:10, nfeatures = 5)\n\nPC_ 1 \nPositive:  LTB, IL32, TRAC, CD3D, TRBC2 \nNegative:  FCN1, FGL2, CST3, IFI30, TYMP \nPC_ 2 \nPositive:  MS4A1, CD79A, BANK1, IGHM, NIBAN3 \nNegative:  IL32, GZMM, CD3D, CD7, CD247 \nPC_ 3 \nPositive:  GZMB, CLIC3, NKG7, GNLY, KLRD1 \nNegative:  CCR7, LEF1, TRABD2A, TCF7, LTB \nPC_ 4 \nPositive:  CD79B, MS4A1, GNLY, CD79A, LINC00926 \nNegative:  LILRA4, CLEC4C, SERPINF1, TPM2, SCT \nPC_ 5 \nPositive:  CDKN1C, HES4, CTSL, TCF7L2, BATF3 \nNegative:  S100A12, ITGAM, VCAN, CES1, MGST1 \nPC_ 6 \nPositive:  CDKN1C, NRGN, PADI4, CKB, FCGR3A \nNegative:  CRIP1, GBP5, ISG15, NIBAN1, MAF \nPC_ 7 \nPositive:  CLU, ITGB1, NRGN, LIMS1, GP1BB \nNegative:  MALAT1, CCR7, TXK, LINC02446, KLRK1 \nPC_ 8 \nPositive:  CDKN1C, VIM, S100A4, CKB, ITGB1 \nNegative:  MAP3K7CL, SERPING1, GMPR, LGALS2, NEXN \nPC_ 9 \nPositive:  HERC5, IFIT1, RSAD2, IFIT3, MX1 \nNegative:  CLEC10A, FCER1A, ENHO, CD1C, CACNA2D3 \nPC_ 10 \nPositive:  TNFRSF13B, IGHG1, IGHA1, SSPN, IGFBP7 \nNegative:  TCL1A, CD8A, TRGC2, GZMK, CD8B \n\n\n另一种可视化信息的方法是：\n\nVizDimLoadings(pbmc, dims = 1:2, reduction = \"pca\",balanced=TRUE)\n\n\n\n\n这是另一种提供图示表示的方法。细胞和特征根据主成分分析得分进行排序。设置一个细胞数量有助于计算效率，因为它会忽略那些信息较少的极端细胞。强制使其平衡可在正相关和负相关之间获得相等的表示。\n\nDimHeatmap(pbmc, dims = 1, cells = 500, balanced = TRUE)\n\n\n\n\n\nDimHeatmap(pbmc, dims = 1:15, cells = 500, balanced = TRUE)\n\n\n\n\nJackstraw 图用于估计每个主成分所捕获的结构的显著性。它会随机对数据的子集进行排列，以建立空值分布，并根据这个空值分布估计p值。\n⚠️ 这可能需要一些时间来执行。如果时间不够，请跳过这部分，然后只专注于接下来的拐点图。\n\npbmc &lt;- JackStraw(pbmc, num.replicate = 100)\npbmc &lt;- ScoreJackStraw(pbmc, dims = 1:20)\nJackStrawPlot(pbmc, dims = 1:20)\n\nWarning: Removed 28000 rows containing missing values (`geom_point()`).\n\n\n\n\n\n拐点图是经典的计算机科学方法，用于检查通过逐个添加主成分来表示数据中累积变异性。\n\nElbowPlot(pbmc,ndims=50)\n\n\n\n\n根据这些图，我们选择使用20个主成分。但也可以为15到30之间的任何数量提出理由。\n我们使用UMAP来可视化数据集。我们提前计算UMAP，以便在需要时立即使用。\n\npbmc &lt;- RunUMAP(pbmc, dims = 1:20, verbose = FALSE)\n\nWarning: The default method for RunUMAP has changed from calling Python UMAP via reticulate to the R-native UWOT using the cosine metric\nTo use Python UMAP via reticulate, set umap.method to 'umap-learn' and metric to 'correlation'\nThis message will be shown once per session"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#聚类",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#聚类",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "聚类",
    "text": "聚类\nSeurat默认的聚类算法采用基于图的聚类方法。它受到以前的发表文章的启发，这些文章在开发和改进这种方法方面做出了贡献。特别是，《Phenograph》论文强烈推荐给那些希望更好地理解这种方法的人。简要来说，聚类算法首先在PCA空间中计算每个细胞的K个最近邻。然后，根据共享邻居计算细胞之间的Jaccard相似性。使用Louvain算法对信息进行聚合，以便将细胞迭代地分组在一起。\n\npbmc &lt;- FindNeighbors(pbmc, dims = 1:20)\n\nComputing nearest neighbor graph\n\n\nComputing SNN\n\n\n第二个重要的决策是选择聚类的分辨率。较小的数值会生成混合的聚类，而较大的数值会产生过多的聚类，这些聚类可能不太具有意义。\n📝 对于你自己的研究，花一些时间来优化分辨率的选择是很有帮助的。尝试使用多个候选的分辨率选项来进行所有后续分析可能会有帮助。\n在这里，我们尝试三个不同的分辨率值，并使用clustree来研究其影响。在计算完聚类后，我们还保存了pbmc对象。\n\npbmc &lt;- FindClusters(object = pbmc,  resolution = c(0.5, 1, 1.5),  dims.use = 1:10,  save.SNN = TRUE)\n\nWarning: The following arguments are not used: dims.use, save.SNN\n\n\nSuggested parameter: dims instead of dims.use\n\n\nWarning: The following arguments are not used: dims.use, save.SNN\n\n\nSuggested parameter: dims instead of dims.use\n\n\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 9729\nNumber of edges: 356102\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.9145\nNumber of communities: 16\nElapsed time: 1 seconds\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 9729\nNumber of edges: 356102\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.8655\nNumber of communities: 20\nElapsed time: 1 seconds\nModularity Optimizer version 1.3.0 by Ludo Waltman and Nees Jan van Eck\n\nNumber of nodes: 9729\nNumber of edges: 356102\n\nRunning Louvain algorithm...\nMaximum modularity in 10 random starts: 0.8279\nNumber of communities: 22\nElapsed time: 1 seconds\n\n\n\nsaveRDS(pbmc, file = \"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/pbmc_tutorial.rds\")\nclustree(pbmc)\n\n\n\n\n这三行表示每个分辨率值下的细胞分配情况。每行中的节点大小表示该聚类中的细胞数量。行之间的箭头显示随着分辨率的增加，聚类的分配情况如何变化。稳定的聚类可能会更改名称，但是细胞在不同的分辨率下仍然会聚集在一起。一些聚类可能会分裂成两个（或更多）子聚类。这暗示着增加分辨率。但是，如果你看到聚类之间来回跳动的情况很多，那就表示稳定性较差。在你自己的数据中，最好进行过度聚类，然后检查基因，然后手动选择要合并的聚类。\n我们选择分辨率=0.5进行进一步检查：\n\nIdents(pbmc) &lt;- pbmc$RNA_snn_res.0.5\nDimPlot(pbmc, reduction = \"umap\", label=TRUE)"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#聚类富集标记",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#聚类富集标记",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "聚类富集标记",
    "text": "聚类富集标记\n接下来，对于每个聚类，我们想要检查在该聚类中相比其他细胞高度表达的特征（也称为标记或基因）。我们使用’roc’方法来估计每个标记的分类能力（0表示随机，1表示完美）。我们可以逐个聚类地进行这个分析，也可以同时对所有聚类进行分析。\n\ncluster0.markers &lt;- FindMarkers(pbmc, ident.1 = 0, logfc.threshold = 0.25, test.use = \"roc\", only.pos = TRUE)\n\n⚠️ 对所有聚类执行此操作非常耗时。如果你想休息一下。在这样做之前启动它。计算时间可能长达30分钟。如果你在教程时间方面运行晚了，现在跳过这个和下面的热图。\n\ncluster.all.markers0.5 &lt;- FindAllMarkers(pbmc, logfc.threshold = 0.25, test.use = \"roc\", only.pos = TRUE, min.pct = 0.25)\n\nCalculating cluster 0\n\n\nCalculating cluster 1\n\n\nCalculating cluster 2\n\n\nCalculating cluster 3\n\n\nCalculating cluster 4\n\n\nCalculating cluster 5\n\n\nCalculating cluster 6\n\n\nCalculating cluster 7\n\n\nCalculating cluster 8\n\n\nCalculating cluster 9\n\n\nCalculating cluster 10\n\n\nCalculating cluster 11\n\n\nCalculating cluster 12\n\n\nCalculating cluster 13\n\n\nCalculating cluster 14\n\n\nCalculating cluster 15\n\n\n热图为共享高度表达标记的集群提供了很好的图形表示。注意到umap中这些集群之间的关系了吗?\n\ncluster.all.markers0.5  %&gt;%\n    group_by(cluster) %&gt;%\n    top_n(n = 5, wt = avg_log2FC) -&gt; top5\n\nDoHeatmap(pbmc, features = top5$gene) + NoLegend()\n\n\n\n\n接下来，我们选择一些标记，这些标记往往适用于PBMC数据集。其中许多来自cluster.all.markers0.5。\n\nmarkers.to.plot &lt;- c(\"CD3D\", \"HSPH1\", \"SELL\", \"CD14\", \"LYZ\", \"GIMAP5\", \"CACYBP\", \"GNLY\", \"NKG7\", \"CCL5\", \"CD8A\", \"MS4A1\", \"CD79A\", \"FCGR3A\", \"MS4A7\", \"S100A9\", \"HLA-DQA1\",\"GPR183\", \"PPBP\", \"GNG11\", \"TSPAN13\", \"IL3RA\", \"FCER1A\", \"CST3\", \"S100A12\")\n\nDotPlot(pbmc, features = markers.to.plot, cols = c(\"blue\", \"red\"), dot.scale = 8) +RotatedAxis()\n\n\n\n\n\nVlnPlot(pbmc, features = c(\"MS4A1\", \"CD79A\"))\n\n\n\nVlnPlot(pbmc, features = c(\"NKG7\", \"GNLY\"))\n\n\n\nVlnPlot(pbmc, features = c(\"FCGR3A\", \"MS4A7\"))\n\n\n\nVlnPlot(pbmc, features = c(\"PPBP\"))\n\n\n\nVlnPlot(pbmc, features = c(\"FCER1A\", \"CST3\"))   \n\n\n\nVlnPlot(pbmc, features = c(\"CD8A\", \"CD8B\", \"CD3D\"))\n\n\n\n\n\nFeaturePlot(pbmc, features = c(\"MS4A1\", \"GNLY\", \"CD3E\", \"CD14\", \"FCER1A\", \"FCGR3A\", \"LYZ\", \"PPBP\",  \"CD8A\"))"
  },
  {
    "objectID": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#基于参考的聚类注释",
    "href": "posts/2023-07-26-clustering-and-cell-type-identification/index.html#基于参考的聚类注释",
    "title": "Single-cell Workshop 2021 - 02 - 聚类和细胞类型鉴定",
    "section": "基于参考的聚类注释",
    "text": "基于参考的聚类注释\n基于参考数据的聚类注释不在Seurat软件包中进行。为此，我们将使用SingleR，并首先将Seurat数据的信息转换为SingleR的格式。\n\nsce &lt;- GetAssayData(object = pbmc, assay = \"RNA\", slot = \"data\")\n\n我们将使用一篇经典论文中的数据，该论文使用分类细胞对不同的免疫细胞类型进行了比较。基于参考数据的注释允许与高度精选的数据集进行直接比较。然而，缺点是推断的结果取决于参考数据集中定义的内容。\n\nrefMonaco &lt;- MonacoImmuneData()\n\nsnapshotDate(): 2022-10-31\n\n\nsee ?celldex and browseVignettes('celldex') for documentation\n\n\nloading from cache\n\n\nsee ?celldex and browseVignettes('celldex') for documentation\n\n\nloading from cache\n\n\n数据格式在两个层次上显示信息:主要细胞类型和更精细的分辨率信息：\n\nprediction_Monaco_main &lt;- SingleR(test=sce, ref=refMonaco, clusters=Idents(pbmc), labels=refMonaco$label.main)\nprediction_Monaco_fine &lt;- SingleR(test=sce, ref=refMonaco, clusters=Idents(pbmc), labels=refMonaco$label.fine)\n\npredicted_Monaco &lt;- data.frame(cluster=sort(unique(Idents(pbmc))), Monaco_main= prediction_Monaco_main$labels, Monaco_fine= prediction_Monaco_fine$labels)\npredicted_Monaco\n\n   cluster     Monaco_main                  Monaco_fine\n1        0       Monocytes          Classical monocytes\n2        1    CD4+ T cells            Naive CD4 T cells\n3        2       Monocytes          Classical monocytes\n4        3    CD8+ T cells            Naive CD8 T cells\n5        4    CD4+ T cells               Th1/Th17 cells\n6        5         T cells           Non-Vd2 gd T cells\n7        6         B cells  Non-switched memory B cells\n8        7         B cells                Naive B cells\n9        8        NK cells         Natural killer cells\n10       9       Monocytes       Intermediate monocytes\n11      10         T cells                   MAIT cells\n12      11 Dendritic cells      Myeloid dendritic cells\n13      12 Dendritic cells Plasmacytoid dendritic cells\n14      13       Monocytes          Classical monocytes\n15      14     Progenitors             Progenitor cells\n16      15         B cells                 Plasmablasts"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "",
    "text": "学习资料：GitHub 地址"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#尴尬了流程跑不通",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#尴尬了流程跑不通",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "尴尬了，流程跑不通",
    "text": "尴尬了，流程跑不通\n这个教程在运行到 predict_ligand_activities() 就出现了问题。后面就走不下去了。\npredict_ligand_activities(geneset = geneset_oi, background_expressed_genes = background_expressed_genes, ligand_target_matrix = ligand_target_matrix, potential_\n    ligands = potential_ligands)\nError in evaluate_target_prediction(setting, ligand_target_matrix, ligands_position) : \n  all genes have same response\n我自己也懒得折腾了，本身就是想简单学习了解流程，走不通就不走了。如果有读者看到这个文章，感兴趣可以钻研下。\n我查了下，Comparison of methods and resources for cell-cell communication inference from single-cell RNA-Seq data 显示CellChat、CellPhoneDB和SingleCellSignalR，对数据和资源中的噪声都相当稳健。因而NicheNet并不在推荐范围内。\n不过也提到：\n\nMethods that additionally infer intracellular processes, such as NicheNet19, Cytotalk22, and SoptSC20 are not directly comparable but instead provide complementary analyses.\n\n说明目前看，还是有一些特定的分析有可取之处。"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#数据下载",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#数据下载",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "数据下载",
    "text": "数据下载\n我们将使用SeuratData软件包提供的PBMC数据集。该数据集包含两个PBMC样本：经过IFN Beta处理的刺激样本和对照样本。\n对于细胞交互分析，我们将使用NicheNet，它需要一些精心策划的数据集。虽然这些数据集可以在R环境内下载，但对我来说下载速度太慢了。这些文件可以从Zenodo下（https://zenodo.org/record/3260758）载：gr_network.rds、ligand_target_matrix.rds、lr_network.rds和weighted_networks.rds。\n请将这些数据集放到你的工作目录中。"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#准备",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#准备",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "准备",
    "text": "准备\n本文参考 Seurat 数据整合文档进行分析的输入数据准备，假设你对数据预处理部分已经很熟悉。"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#安装包",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#安装包",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "安装包",
    "text": "安装包\n安装所需要的包。\nBiocManager::install(\"limma\")\nBiocManager::install(\"glmGamPoi\")\n# install.packages(\"devtools\")\ninstall.packages(\"https://seurat.nygenome.org/src/contrib/ifnb.SeuratData_3.0.0.tar.gz\", repos = NULL, type = \"source\") \ndevtools::install_github(\"saeyslab/nichenetr\")\n# 可以下载之后安装，避免安装不上\n# https://github.com/saeyslab/nichenetr\n# unzip(\"~/Downloads/nichenetr-master.zip\")\n# 缺什么装什么\n# BiocManager::install(c(\"fdrtool\", \"DiagrammeR\", \"mlrMBO\", \"parallelMap\", \"emoa\", \"DiceKriging\"))\n# install.packages(\"nichenetr-master\", repos = NULL, type = \"source\")\ninstall.packages(\"tidyverse\")\n加载包：\n\n# load into your session\nlibrary(ifnb.SeuratData)\nlibrary(Seurat)\n\nAttaching SeuratObject\n\n# load dataset\ndata(\"ifnb\")"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#读取数据集并创建一个seurat对象",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#读取数据集并创建一个seurat对象",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "读取数据集并创建一个Seurat对象",
    "text": "读取数据集并创建一个Seurat对象\n第一步是从SeuratData软件包中读取数据。输入数据根据样本进行拆分，并分别应用SCtranform。请注意，这取代了NormalizeData、ScaleData和FindVariableFeatures的步骤。SCTransform是一种专门设计用于单细胞UMI计数数据的统计方法。它克服了之前批量设计的归一化方法中一些过度拟合的限制。有关其优势的更多详细信息可以在SCtranform的论文中找到。请注意，我们没有应用任何质控步骤。这是因为数据已经预先清洁，因此无需重复这些步骤。\n\nifnb.list &lt;- SplitObject(ifnb, split.by = \"stim\")\nifnb.list &lt;- lapply(X = ifnb.list, FUN = SCTransform, method = \"glmGamPoi\")\n\nCalculating cell attributes from input UMI matrix: log_umi\n\n\nVariance stabilizing transformation of count matrix of size 12747 by 6548\n\n\nModel formula is y ~ log_umi\n\n\nGet Negative Binomial regression parameters per gene\n\n\nUsing 2000 genes, 5000 cells\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\nFound 95 outliers - those will be ignored in fitting/regularization step\n\n\nSecond step: Get residuals using fitted parameters for 12747 genes\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\nComputing corrected count matrix for 12747 genes\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCalculating gene attributes\n\n\nWall clock passed: Time difference of 31.92474 secs\n\n\nDetermine variable features\n\n\nPlace corrected count matrix in counts slot\n\n\nCentering data matrix\n\n\nSet default assay to SCT\n\n\nCalculating cell attributes from input UMI matrix: log_umi\n\n\nVariance stabilizing transformation of count matrix of size 12658 by 7451\n\n\nModel formula is y ~ log_umi\n\n\nGet Negative Binomial regression parameters per gene\n\n\nUsing 2000 genes, 5000 cells\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |======================================================================| 100%\n\n\nFound 101 outliers - those will be ignored in fitting/regularization step\n\n\nSecond step: Get residuals using fitted parameters for 12658 genes\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\nComputing corrected count matrix for 12658 genes\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCalculating gene attributes\n\n\nWall clock passed: Time difference of 30.21689 secs\n\n\nDetermine variable features\n\n\nPlace corrected count matrix in counts slot\n\n\nCentering data matrix\n\n\nSet default assay to SCT\n\nfeatures &lt;- SelectIntegrationFeatures(object.list = ifnb.list, nfeatures = 3000)\nifnb.list &lt;- PrepSCTIntegration(object.list = ifnb.list, anchor.features = features)\nifnb.list &lt;- lapply(X = ifnb.list, FUN = RunPCA, features = features)\n\nPC_ 1 \nPositive:  FTL, TIMP1, IL8, CCL2, FTH1, C15orf48, S100A8, SOD2, ACTB, S100A9 \n       LYZ, FCER1G, TYROBP, S100A4, HLA-DRA, CD63, CD14, S100A11, LGALS3, ANXA5 \n       LGALS1, CXCL3, S100A10, IL1B, CTSL, PLAUR, CTSB, TYMP, S100A6, CST3 \nNegative:  RPL3, RPS18, RPS6, LTB, RPL13, CCR7, GIMAP7, RPL13A, PTMA, CXCR4 \n       RPSA, RPS2, RPL21, CD3D, RPS5, LDHB, NPM1, RPS4X, CCL5, RPS3 \n       TMEM66, RPS14, RPS27, PABPC1, RPL10A, RPS15A, RPL5, RPL32, RPS19, RPL34 \nPC_ 2 \nPositive:  CCL2, CCL5, GNLY, S100A8, NKG7, FTL, S100A9, GZMB, GIMAP7, CD7 \n       CD3D, CD14, FTH1, IL8, CXCL3, CD2, FGFBP2, CCL7, RGCC, RARRES3 \n       GZMH, APOBEC3G, TMEM66, CLIC3, ITM2A, CTSL, LTB, IL32, LCK, GIMAP5 \nNegative:  HLA-DRA, CD74, HLA-DQA1, HLA-DPA1, HLA-DRB1, HLA-DPB1, HLA-DQB1, CD83, TXN, CCR7 \n       CCL22, HLA-DMA, IDO1, ACTB, HERPUD1, LYZ, SYNGR2, CD79A, MIR155HG, BIRC3 \n       PKIB, RAB9A, SERPINB1, RAMP1, GPR183, CST3, FABP5, HLA-DRB5, TVP23A, ID2 \nPC_ 3 \nPositive:  RPS18, RPL13, LTB, RPS6, FTL, RPL3, RPL32, RPL10, RPS2, PABPC1 \n       RPL34, RPL13A, RPS14, RPS4X, RPL21, S100A8, CCL2, RPS12, RPL11, RPL7 \n       RPS3A, RPL18A, LDHB, SELL, RPS15A, RPS3, RPS8, S100A9, RPS5, RPS27A \nNegative:  GNLY, NKG7, CCL5, GZMB, CST7, APOBEC3G, FGFBP2, CLIC3, GZMH, KLRD1 \n       GZMA, CTSW, PRF1, HOPX, KLRC1, CHST12, TNFRSF18, CCL4, VMO1, FCGR3A \n       DUSP2, LDHA, CXCR3, C1orf21, ID2, MATK, SPON2, SH2D1B, RARRES3, APMAP \nPC_ 4 \nPositive:  HSPB1, HSP90AB1, HSPE1, CACYBP, CCL2, HSPA8, CREM, HSPD1, SRSF7, HSPH1 \n       HSPA1A, FTL, CD69, UBB, GADD45B, UBC, SRSF2, ZFAND2A, HSP90AA1, IL8 \n       NOP58, DDIT4, NR4A2, CHORDC1, H3F3B, BTG1, RSRC2, DNAJB1, SNHG15, YPEL5 \nNegative:  VMO1, FCGR3A, TIMP1, MS4A7, RPL3, FAM26F, TNFSF10, RPS4X, RPS2, RPS19 \n       RPL10, RPS18, PLAC8, CXCL16, CXCL10, HLA-DPA1, LST1, HLA-DPB1, RPL13, TNFSF13B \n       MS4A4A, RPS14, IFITM3, HN1, AIF1, GBP5, RPS6, CCL5, CD74, GBP1 \nPC_ 5 \nPositive:  CCL2, FTL, GNLY, CCL5, NKG7, IL8, GZMB, S100A8, S100A9, LYZ \n       HLA-DRA, CST7, CD74, HLA-DQA1, CXCL3, TXN, FGFBP2, CD63, GAPDH, CLIC3 \n       IL1B, HLA-DPB1, HLA-DRB1, VIM, RPL3, GZMH, CCL7, APOBEC3G, C15orf48, HLA-DQB1 \nNegative:  VMO1, FCGR3A, MS4A7, TIMP1, FAM26F, TNFSF10, CXCL10, HSPB1, IFITM3, GBP1 \n       AIF1, PLAC8, MS4A4A, LST1, GBP5, CXCL16, HN1, SAT1, ATP1B3, OAS1 \n       ISG15, C3AR1, WARS, CTSC, PPM1N, RGS2, TNFSF13B, SOD2, IFIT3, FGL2 \n\n\nPC_ 1 \nPositive:  FTL, CXCL10, CCL2, CCL8, APOBEC3A, SOD2, TIMP1, C15orf48, CCL7, APOBEC3B \n       ISG15, IDO1, CCL3, IFITM3, CXCL11, LYZ, TYROBP, CCL4, IFI27, FCER1G \n       FTH1, CTSL, SAT1, TYMP, IL1RN, LGALS3, S100A11, CTSC, CD63, LGALS1 \nNegative:  RPL3, RPS6, RPS18, RPL13, RPL21, RPL7, RPL13A, CCR7, RPL10A, RPS4X \n       RPS2, GIMAP7, RPS3, RPS3A, RPS14, RPS27A, PTMA, RPL10, RPS15A, RPL34 \n       RPS5, RPL32, PABPC1, RPL5, CXCR4, RPS27, RPL31, LTB, MALAT1, RPL14 \nPC_ 2 \nPositive:  CCL2, CCL8, CCL7, RPS18, RPL13, RPS6, FTL, RPL3, RPL32, SELL \n       RPS14, RPL34, LTB, RPL7, RPL13A, RPL21, RPL10, GIMAP7, PABPC1, RPS2 \n       RPS4X, APOBEC3B, RPL10A, RPS15A, RPS3A, LDHB, S100A9, RPS12, RPL35A, RPL31 \nNegative:  GNLY, GZMB, NKG7, CCL5, PRF1, CD74, CLIC3, CST7, GZMA, HLA-DRA \n       APOBEC3G, GZMH, HLA-DPB1, HLA-DPA1, KLRD1, CTSW, TXN, FCGR3A, HLA-DQA1, VMO1 \n       HLA-DRB1, FASLG, ID2, FGFBP2, CD83, KLRC1, MS4A7, HLA-DQB1, RARRES3, TIMP1 \nPC_ 3 \nPositive:  CD74, HLA-DRA, HLA-DPB1, CXCL10, HLA-DPA1, HLA-DQA1, HLA-DRB1, TXN, TIMP1, CD83 \n       VMO1, FCGR3A, CCR7, HLA-DQB1, MS4A7, MARCKSL1, IRF8, CST3, BIRC3, SYNGR2 \n       MS4A4A, SERPINB1, CD79A, CXCL16, CD86, MIR155HG, HLA-DMA, CCL22, ACTB, FAM26F \nNegative:  GNLY, CCL2, CCL8, NKG7, CCL5, GZMB, CCL7, PRF1, GZMA, GZMH \n       CLIC3, KLRD1, CTSW, RARRES3, S100A9, ALOX5AP, FASLG, APOBEC3G, FGFBP2, CTSL \n       CD7, CCL4, KLRC1, CST7, S100A8, ANXA1, CCL3, CD247, IL32, LGALS3 \nPC_ 4 \nPositive:  FCGR3A, VMO1, MS4A7, TIMP1, MS4A4A, CXCL16, FAM26F, PLAC8, TNFSF10, TNFAIP6 \n       C3AR1, LST1, FTH1, CXCL10, TYROBP, CTSC, AIF1, GBP5, PPM1N, CFD \n       MT2A, CDKN1C, SERPINA1, IFITM3, SLC31A2, HN1, PLAUR, FCER1G, FCGR3B, EDN1 \nNegative:  CCL2, CCL8, CD74, HLA-DRA, TXN, HLA-DQA1, CCL7, HLA-DPB1, FTL, HLA-DRB1 \n       LYZ, MIR155HG, HLA-DQB1, CD83, IRF8, FABP5, CCR7, MARCKSL1, BIRC3, APOBEC3B \n       TSPAN13, HERPUD1, HSPE1, HSPD1, HLA-DPA1, ID3, PMAIP1, HSP90AB1, CD79A, NME1 \nPC_ 5 \nPositive:  CD74, HLA-DPB1, HLA-DRA, RPL3, TXN, RPS18, RPS4X, MARCKSL1, HLA-DPA1, RPL10 \n       RPL13, LYZ, HLA-DRB1, FTL, RPS2, GNLY, RPS14, GZMB, CCL5, RPS3 \n       RPL34, NKG7, RPL11, RPL13A, CST3, RPL32, RPS6, RPL21, HLA-DQA1, LSP1 \nNegative:  CCL4, CCL3, CD69, SRSF7, HSPE1, HSP90AB1, CACYBP, HSPA8, HSPD1, SRSF2 \n       DDIT4, CREM, GADD45B, UBC, NOP58, UBB, HSPH1, TCP1, JUNB, SOD1 \n       SAT1, CLK1, RSRC2, BTG1, DNAJB6, NR4A2, MYC, IRF1, DNAJB1, BATF \n\n\n接下来，我们将两个数据集进行整合。整合依赖于在两个数据集中都存在的高变量特征，并且分为两个步骤。第一步FindIntegrationAnchors定义锚点或细胞对（每个样本中各选择一个细胞），这些细胞非常相似，因此可以有信心地将它们分配到相同的簇（细胞类型和状态）。第二步IntegrateData使用所定义的锚点对齐完整的数据集。有关整合方法的更多详细信息和自原始Seurat论文以来的改进可以在他们最近的出版物中找到。\n\nimmune.anchors &lt;- FindIntegrationAnchors(object.list = ifnb.list, normalization.method = \"SCT\", anchor.features = features, dims = 1:30, reduction = \"rpca\", k.anchor = 20)\n\nComputing within dataset neighborhoods\n\n\nFinding all pairwise anchors\n\n\nProjecting new data onto SVD\nProjecting new data onto SVD\n\n\nFinding neighborhoods\n\n\nFinding anchors\n\n\n    Found 32565 anchors\n\nimmune.combined.sct &lt;- IntegrateData(anchorset = immune.anchors, normalization.method = \"SCT\", dims = 1:30)\n\nMerging dataset 1 into 2\n\n\nExtracting anchors for merged samples\n\n\nFinding integration vectors\n\n\nFinding integration vector weights\n\n\nIntegrating data\n\n\n整合后的对象包含经过批次效应校正的值，以及原始计数值作为单独的测定。\n\nimmune.combined.sct &lt;- RunPCA(immune.combined.sct, verbose = FALSE)\nimmune.combined.sct &lt;- RunUMAP(immune.combined.sct, reduction = \"pca\", dims = 1:30)\n\nWarning: The default method for RunUMAP has changed from calling Python UMAP via reticulate to the R-native UWOT using the cosine metric\nTo use Python UMAP via reticulate, set umap.method to 'umap-learn' and metric to 'correlation'\nThis message will be shown once per session\n\n\n15:39:33 UMAP embedding parameters a = 0.9922 b = 1.112\n\n\n15:39:33 Read 13999 rows and found 30 numeric columns\n\n\n15:39:33 Using Annoy for neighbor search, n_neighbors = 30\n\n\n15:39:33 Building Annoy index with metric = cosine, n_trees = 50\n\n\n0%   10   20   30   40   50   60   70   80   90   100%\n\n\n[----|----|----|----|----|----|----|----|----|----|\n\n\n**************************************************|\n15:39:35 Writing NN index file to temp file /var/folders/bj/nw1w4g1j37ddpgb6zmh3sfh80000gn/T//RtmpF254ot/file1575b69d2520a\n15:39:35 Searching Annoy index using 1 thread, search_k = 3000\n15:39:38 Annoy recall = 100%\n15:39:39 Commencing smooth kNN distance calibration using 1 thread with target n_neighbors = 30\n15:39:40 Initializing from normalized Laplacian + noise (using irlba)\n15:39:41 Commencing optimization for 200 epochs, with 600044 positive edges\n15:39:49 Optimization finished\n\n\n可视化：\n\np1 &lt;- DimPlot(immune.combined.sct, reduction = \"umap\", group.by = \"stim\")\np2 &lt;- DimPlot(immune.combined.sct, reduction = \"umap\", group.by = \"seurat_annotations\", label = TRUE,\n    repel = TRUE)\np1 + p2"
  },
  {
    "objectID": "posts/2023-07-31-cell-interaction-analysis/index.html#细胞相互作用分析",
    "href": "posts/2023-07-31-cell-interaction-analysis/index.html#细胞相互作用分析",
    "title": "Single-cell Workshop 2021 - 03 - 细胞相互作用分析",
    "section": "细胞相互作用分析",
    "text": "细胞相互作用分析\nNicheNet旨在通过将特定簇的单细胞表达数据与关于配体-受体对和目标下游的基因调控网络的先验知识相结合，预测配体和目标细胞之间的相互作用连接。特别地，它旨在定义能够最好解释目标簇中观察到的差异表达的配体。\n我们首先加载必要的R软件包。⚠️在我加载了NicheNet软件包后，部分Seurat整合命令未能成功执行。两个软件包之间可能存在一些不兼容性。\n\nlibrary(nichenetr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n加载数据集：\n\nligand_target_matrix = readRDS(\"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/ligand_target_matrix.rds\")\nligand_target_matrix[1:5,1:5]\n\n                CXCL1        CXCL2        CXCL3        CXCL5         PPBP\nA1BG     3.534343e-04 4.041324e-04 3.729920e-04 3.080640e-04 2.628388e-04\nA1BG-AS1 1.650894e-04 1.509213e-04 1.583594e-04 1.317253e-04 1.231819e-04\nA1CF     5.787175e-04 4.596295e-04 3.895907e-04 3.293275e-04 3.211944e-04\nA2M      6.027058e-04 5.996617e-04 5.164365e-04 4.517236e-04 4.590521e-04\nA2M-AS1  8.898724e-05 8.243341e-05 7.484018e-05 4.912514e-05 5.120439e-05\n\n\n\nlr_network = readRDS(\"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/lr_network.rds\")\nhead(lr_network)\n\n# A tibble: 6 × 4\n  from  to    source         database\n  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;   \n1 CXCL1 CXCR2 kegg_cytokines kegg    \n2 CXCL2 CXCR2 kegg_cytokines kegg    \n3 CXCL3 CXCR2 kegg_cytokines kegg    \n4 CXCL5 CXCR2 kegg_cytokines kegg    \n5 PPBP  CXCR2 kegg_cytokines kegg    \n6 CXCL6 CXCR2 kegg_cytokines kegg    \n\n\n\nweighted_networks = readRDS(\"/Users/wsx/Library/CloudStorage/OneDrive-shanghaitech.edu.cn/Public/data/weighted_networks.rds\")\nweighted_networks_lr = weighted_networks$lr_sig %&gt;% inner_join(lr_network %&gt;% distinct(from,to), by = c(\"from\",\"to\"))\nhead(weighted_networks$lr_sig) # interactions and their weights in the ligand-receptor + signaling network\n\n# A tibble: 6 × 3\n  from  to     weight\n  &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n1 A1BG  ABCC6  0.422 \n2 A1BG  ACE2   0.101 \n3 A1BG  ADAM10 0.0970\n4 A1BG  AGO1   0.0525\n5 A1BG  AKT1   0.0855\n6 A1BG  ANXA7  0.457 \n\n\n\nIdents(immune.combined.sct) &lt;-immune.combined.sct$seurat_annotations\nDefaultAssay(immune.combined.sct) &lt;- \"integrated\"\n\n在本文中，我们依赖于数据提供的筛选细胞注释。 我们将重点放在CD8 T细胞作为接收细胞，并寻找pDC、CD14 Mono、CD16 Mono、NK、B、DC、B Activated群集中的发送器配体。\n我们定义接收细胞和发送器细胞群体。此外，我们还识别在接收细胞中表达的基因，这些基因也是我们知识库的一部分。\n💡如果某个基因在数据库中没有被知道由任何配体调控，该方法将无法在其预测中使用它。\n\nreceiver = \"CD8 T\"\nexpressed_genes_receiver = get_expressed_genes(receiver, immune.combined.sct, pct = 0.10, assay_oi=\"RNA\")\nbackground_expressed_genes = expressed_genes_receiver %&gt;% .[. %in% rownames(ligand_target_matrix)]\n\nsender_celltypes = c(\"pDC\", \"CD14 Mono\" ,  \"CD16 Mono\",  \"NK\", \"B\", \"DC\", \"B Activated\")\n\nlist_expressed_genes_sender = sender_celltypes %&gt;% unique() %&gt;% lapply(get_expressed_genes, immune.combined.sct, 0.10, assay_oi=\"RNA\") \nexpressed_genes_sender = list_expressed_genes_sender %&gt;% unlist() %&gt;% unique()\n\n定义接收细胞中相对于样本处理的差异基因。\n\nseurat_obj_receiver= subset(immune.combined.sct, idents = receiver)\nseurat_obj_receiver = SetIdent(immune.combined.sct, value = seurat_obj_receiver[[\"orig.ident\"]])\n\ncondition_oi = \"IMMUNE_STIM\"\ncondition_reference = \"IMMUNE_CTRL\" \n\nDE_table_receiver = FindMarkers(object = seurat_obj_receiver, ident.1 = condition_oi, ident.2 = condition_reference, min.pct = 0.10, assay=\"RNA\") %&gt;% rownames_to_column(\"gene\")\n\ngeneset_oi = DE_table_receiver %&gt;% filter(p_val &lt;= 0.05 & abs(avg_log2FC) &gt;= 0.25) %&gt;% pull(gene)\ngeneset_oi = geneset_oi %&gt;% .[. %in% rownames(ligand_target_matrix)]\n\n定义在各自群集中表达的配体和受体基因。潜在的配体集被缩小为在接收细胞群集中表达的具有靶标的配体。\n\nligands = lr_network %&gt;% pull(from) %&gt;% unique()\nreceptors = lr_network %&gt;% pull(to) %&gt;% unique()\n\nexpressed_ligands = intersect(ligands,expressed_genes_sender)\nexpressed_receptors = intersect(receptors,expressed_genes_receiver)\n\npotential_ligands = lr_network %&gt;% filter(from %in% expressed_ligands & to %in% expressed_receptors) %&gt;% pull(from) %&gt;% unique()\n\nNicheNet的配体活性分析根据接收细胞中存在的表达靶标基因的情况对配体进行排名。\n\n# ligand_activities = predict_ligand_activities(geneset = geneset_oi, background_expressed_genes = background_expressed_genes, ligand_target_matrix = ligand_target_matrix, potential_ligands = potential_ligands)\n\n# ligand_activities = ligand_activities %&gt;% arrange(-pearson) %&gt;% mutate(rank = rank(desc(pearson)))\n# ligand_activities"
  },
  {
    "objectID": "posts/2023-08-01-duplication-rate/index.html",
    "href": "posts/2023-08-01-duplication-rate/index.html",
    "title": "Duplication Rate 经验值",
    "section": "",
    "text": "最近处理 RNA-Seq 的原始数据，用 fastp 发现 duplication rate 很高，有不少 60% 的，比较奇怪。\n搜索了下常见的值，记录一下。"
  },
  {
    "objectID": "posts/2023-08-01-duplication-rate/index.html#duplication-rate经验值",
    "href": "posts/2023-08-01-duplication-rate/index.html#duplication-rate经验值",
    "title": "Duplication Rate 经验值",
    "section": "Duplication Rate经验值",
    "text": "Duplication Rate经验值\nWES：数据量约10G时，Duplication rate在10%左右;\nWGS：数据量约90G时，Duplication rate在10%左右;\nRNA-seq：Duplication rate在40% ~ 50%左右;\nWGBS：&gt;10G, Duplication rate &gt; 10%；\n多重PCR试剂盒和靶向捕获Panel，差异很大，跟需要测序的区域以及测序量有关，通常情况下只要中靶部分数据质量足够好，Duplication rate不是一个重要的考虑指标。\n\n参考文章：https://zhuanlan.zhihu.com/p/635160066"
  },
  {
    "objectID": "posts/2023-08-11-caddyfile/index.html",
    "href": "posts/2023-08-11-caddyfile/index.html",
    "title": "caddyfile",
    "section": "",
    "text": "Caddy 是一个反向代理工具，自动支持 https。 这里我记录下之前弄的代理配置，免得服务器到期了这段经验被遗忘掉了。\n$ cat /etc/caddy/Caddyfile \n# The Caddyfile is an easy way to configure your Caddy web server.\n#\n# Unless the file starts with a global options block, the first\n# uncommented line is always the address of your site.\n#\n# To use your own domain name (with automatic HTTPS), first make\n# sure your domain's A/AAAA DNS records are properly pointed to\n# this machine's public IP, then replace \":80\" below with your\n# domain name.\n\n#:80, 139.9.81.126:80 {\n#   # Set this path to your site's directory.\n#   #root * /usr/share/caddy\n#\n#   # Enable the static file server.\n#   #file_server\n#\n#        #tls self_signed\n#\n#   # Another common task is to set up a reverse proxy:\n#   reverse_proxy localhost:8090\n#\n#   # Or serve a PHP site through php-fpm:\n#   # php_fastcgi localhost:9000\n#}\n\nshixiangwang.work {\n\n  encode gzip\n  reverse_proxy localhost:8090\n\n  #handle_path /blog/zd* {\n  #  reverse_proxy localhost:8091\n  #}\n}\n\nmia.shixiangwang.work {\n  encode gzip\n  reverse_proxy localhost:8091\n}\n\nlove.shixiangwang.work {\n  root * /var/www/love\n  file_server\n}\n\npybook.shixiangwang.work {\n  root * /var/www/pybook\n  file_server\n}\n\ncookbook4r.shixiangwang.work {\n  root * /var/www/Cookbook-for-R-Chinese/docs\n  file_server\n}\n\n# Refer to the Caddy docs for more information:\n# https://caddyserver.com/docs/caddyfile"
  },
  {
    "objectID": "posts/2023-08-17-normalize-and-scale-data/index.html",
    "href": "posts/2023-08-17-normalize-and-scale-data/index.html",
    "title": "normalize data 和 scale data 的差异",
    "section": "",
    "text": "“Normalize data” 和 “scale data” 都是数据预处理的步骤，但它们在具体操作和效果上有所不同。\nNormalize Data (数据归一化): 数据归一化是一种常见的预处理方法，用于将不同特征的数据调整到相同的尺度范围内。这有助于避免在模型训练中因为某些特征值范围较大而导致的模型收敛问题。通常，数据归一化意味着将数据缩放到0到1之间的范围，使其具有相似的尺度。常见的归一化方法包括最小-最大缩放、Z-得分标准化等。数据归一化通常不会改变数据的分布形状，只是将其映射到一个更统一的尺度。\nScale Data (数据标准化): 数据标准化也是数据预处理的一部分，但它更注重将数据转换成特定的分布，通常是均值为0，标准差为1的正态分布。标准化后，数据的均值会接近0，标准差接近1，从而使得数据更适合一些要求数据服从正态分布的模型。标准化通常用于在特征之间的差异较大且可能对模型性能产生影响时。\n因此，“normalize data” 侧重于将数据映射到相同的尺度范围内，而 “scale data” 则强调将数据调整为特定分布的形式，通常是标准正态分布。选择使用哪种方法取决于具体的问题和模型需求。\n\n问题是我问的，回答是 ChatGPT。我觉得很好。\n\n我继续问：“z score 变换不就是数据转换成特定的分布，通常是均值为0，标准差为1的正态分布？”\n回答来了：\n是的，您说得对。Z-score标准化（Z-score transformation）是一种常见的数据标准化方法，其目的正是将数据转换为均值为0，标准差为1的正态分布。这种标准化方法通过减去均值，然后除以标准差，使数据的分布接近标准正态分布。这有助于使数据适用于那些要求数据服从正态分布的模型，同时也有助于处理不同尺度和范围的特征，以提高模型的稳定性和效果。\n在前面的回答中，我没有清晰地表达出 “scale data” 的概念，我混淆了 “normalize data” 和 Z-score标准化。非常抱歉对此造成了困扰。事实上，Z-score标准化就是 “scale data” 的一个具体示例，用于将数据进行标准化，使其符合均值为0，标准差为1的正态分布。\n\n看来机器人也有点懵。\n\n我们看看别人的回答，下面的比较对味：\nOne of the reasons that it’s easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and, to make it even more confusing, they are very similar! In both cases, you’re transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that:\nin scaling, you’re changing the range of your data, while in normalization, you’re changing the shape of the distribution of your data.\n为了尽量避免误读，我这里放了英文原文。感兴趣的可以看下 https://www.kaggle.com/code/alexisbcook/scaling-and-normalization/tutorial 具体说明。\n简而言之：\n\nscaling 改变数据取值范围（比如 min-max 变换，z-score 变换）。\nnormalization 改变数据的分布（如 log 变换）。"
  },
  {
    "objectID": "posts/2023-08-31-pnas/index.html",
    "href": "posts/2023-08-31-pnas/index.html",
    "title": "pnas",
    "section": "",
    "text": "想发一篇 PNAS，希望 3 年内能达成。"
  },
  {
    "objectID": "posts/2023-09-03-use-circrna-pipeline/index.html",
    "href": "posts/2023-09-03-use-circrna-pipeline/index.html",
    "title": "circRNA鉴定流程-使用手册",
    "section": "",
    "text": "本系统提供了一个使用四种方法（CIRIquant、Circexplorer2、find_circ和circRNA_finder）从转录组双端测序的FASTQ文件中轻松且可重复地检测环状RNA（circRNA）的计算流程。"
  },
  {
    "objectID": "posts/2023-09-03-use-circrna-pipeline/index.html#功能特性",
    "href": "posts/2023-09-03-use-circrna-pipeline/index.html#功能特性",
    "title": "circRNA鉴定流程-使用手册",
    "section": "功能特性",
    "text": "功能特性\n\n多方法鉴定和整合\n并行计算支持\n断点续跑\n结果整合和规整化输出\n日志记录"
  },
  {
    "objectID": "posts/2023-09-03-use-circrna-pipeline/index.html#准备工作",
    "href": "posts/2023-09-03-use-circrna-pipeline/index.html#准备工作",
    "title": "circRNA鉴定流程-使用手册",
    "section": "准备工作",
    "text": "准备工作\n\n操作系统环境\n本系统需要运行在 Linux 系统环境中（不限发行版本，测试系统包括 Ubuntu、CentOS）。 需要用户知道如何操作 Linux 系统。\n\n\n步骤1. 安装所需的conda环境\n\n（可选）创建一个名为’circrna’的独立Linux帐户，用于部署和运行循环RNA识别流程。\n安装miniconda3到默认路径，即~/miniconda3。 如果按照上述推荐设置，conda应该可在/home/circrna/miniconda3处使用。\n使用conda install -n base --override-channels -c conda-forge mamba 'python_abi=*=*cp*'将mamba安装到base环境中。\n使用以下命令安装just：curl --proto '=https' --tlsv1.2 -sSf https://just.systems/install.sh | bash -s -- --to ~/bin。请将~/bin添加到您的$PATH中。 您可以将~/bin更改为任何位置，但在进入终端时需要使just可用。\n安装rush并将其路径添加到$PATH，类似于just。\n（可选）如果需要，设置conda和pypi（pip）的注册表。例如，如果您在中国，我建议使用以下链接进行设置：https://mirrors.tuna.tsinghua.edu.cn/help/anaconda/ 和 https://mirrors.tuna.tsinghua.edu.cn/help/pypi/。\n使用本软著提供的代码，并切换到代码目录。\n按顺序安装conda环境。\ncd circrna-pipeline\ncd CIRIquant\njust install\ncd ../FindCirc\njust install\ncd ../Circexplorer2\njust install\ncd ../circRNA_finder\njust install\n\n请确保已创建所有conda环境并安装了所需的软件。\n\n\n\nimage-20230903160124763\n\n\n\n\n步骤2. 准备比对索引、参考文件和配置文件\n要运行流程，需要准备好一些参考数据文件。\n\n准备基因组fasta文件和gtf文件。我们使用GRCh38.primary_assembly.genome.fa和gencode.v34.annotation.gtf。\n对于Circexplorer2，您需要使用Circexplorer2环境中的fetch_ucsc.py脚本下载参考文件hg38_ref_all.txt（应与您的参考基因组相对应）。\n准备比对索引，config_zhou.sh中记录了命令和配置，截图如下。\n\n请注意，在运行索引命令之前，您需要激活相应的环境。\n例如，为CIRIquant准备索引。 sh  source activate CIRIquant  bwa index -a bwtsw -p /path/to/GRCh38.primary_assembly.genome.fa /path/to/GRCh38.primary_assembly.genome.fa  hisat2-build -p 40 /path/to/GRCh38.primary_assembly.genome.fa /path/to/GRCh38.primary_assembly.genome.fa\n对于CIRIquant，需要一个yml文件来设置软件和文件的路径，例如hg38.yml (CIRIquant/hg38.yml，截图如下)。您需要修改内容以适应您的设置（也可以创建另一个yml文件）。\n\n设置一个config.sh文件，它使用SHELL变量设置了所有必需的设置，config_zhou.sh (config_zhou.sh) 是一个很好的参考（当然，您可以根据需要修改其内容）。"
  },
  {
    "objectID": "posts/2023-09-03-use-circrna-pipeline/index.html#使用流程",
    "href": "posts/2023-09-03-use-circrna-pipeline/index.html#使用流程",
    "title": "circRNA鉴定流程-使用手册",
    "section": "使用流程",
    "text": "使用流程\n\n预处理转录组双端测序的 fastq 文件\n这一部分的代码存储在 qc 和common目录下面，我们提供了 md5check.sh 方便用户对自己的数据进行 md5 完整性检查，确保数据完整后可以开始进行数据的预处理步骤，包括质控、去除双端的人为标记序列（adapters）。用户输入的文件后缀需要确保为_1.fastq.gz和_2.fastq.gz。示例文件列表截图如下，红色框选了文件后缀。\n\n\n\nimage-20230903160856615\n\n\n完成后可以进行预处理操作，下面提供了示例的 Shell 脚本。\n#!/usr/bin/env bash\n\n# 指定软件路径\npy=/home/circrna/miniconda3/bin/python3\nfp=/home/circrna/miniconda3/bin/fastp\n# 指定代码目录\nPIPELINE=/home/circrna/circrna-pipeline\n\n# 指定输入、输出路径\nfqfile=./sample_list.txt\nindir=/path/to/input\noudir=/path/to/output\n\n# 配置 CPU 数目\nnthreads=20\n\n# 调用脚本生成输入目录下的样本 ID 列表\n${py} ${PIPELINE}/common/ll_fq.py ${indir} --output ${fqfile}\n\n# 调用预处理命令到后台运行，并将运行的日志导出到 test_qc.log\nnohup bash ${PIPELINE}/qc/fp.sh ${fqfile} ${indir} ${oudir} ${nthreads} ${fp} &&gt; test_qc.log &\n下面截图展示了预处理的日志信息，最终会生成样本的质量检测报告（html文件）。\n\n\n\nimage-20230903161015118\n\n\n报告文件截图：\n\n\n\nimage-20230903161412806\n\n\n\n\n运行检测流程\n创建一个包含以下设置和命令的shell脚本。\n# 指定输入、输出路径\nfqfile=./sample_list.txt\nindir=/path/include/paired/fastq/files\noudir=/path/to/output\nnthreads=20\nconfig=/path/to/your/config.sh\n\ncommon/ll_fq.py ${indir} --output ${fqfile}\n\nnohup bash caller.sh ${fqfile} ${indir} ${oudir} ${nthreads} ${config} &&gt; run.log &\n\n必须在conda base环境中（或已安装python3的情况下）执行该脚本。 如果您已经自己准备了sample_list.txt文件。 您可以注释掉common/ll_fq.py这一行，然后可以在bash中运行该脚本 而无需任何其他要求（例如，不需要从base环境中安装python3）。\n\n我建议使用2个样本测试流程。如果一切顺利，然后运行您拥有的所有数据文件。流程将跳过已生成结果文件的样本。\n日志截图：\n\n\n\nimage-20230903162126629\n\n\n如果运行出现中断，再次运行脚本/命令即可，流程会自动跳过已检测完成的样本。\n\n\n\nimage-20230903162238611\n\n\n\n\n检查检测输出结果\n输出目录包含根据样本名称和方法组合的结果文件。\n\n\n\nimage-20230903161627392\n\n\n结果文件通常包含循环RNA的位置和计数值。\n\n\n\nimage-20230903161702141\n\n\n\n\n聚合结果\n上述生成了 4 种检测方法的单独结果，我们下一步需要对结果进行聚合，相关代码放在 aggr 目录下。\naggr_beds.R 提供了对方法结果聚合的代码，示例运行命令为：\n./aggr/aggr_dataset.R /path/to/result /path/to/aggr_output\n这样每一个样本都有一个聚合的结果文件。\n\n\n\nimage-20230903161752469\n\n\n进一步，用户还可以利用 aggr_dataset.R 对全部的样本（即整个输入数据集）结果生成一个整合结果。\n调用命令为：\n./aggr/aggr_dataset.R /path/to/aggr_output /path/to/dataset_output\n最后结果会在 /path/to/dataset_output 生成 aggr_output_circRNA.tsv.gz 文件，这是一个大的压缩数据表格，包含以下一些列信息。\nid, symbol, strand, chrom, startUpBSE, endDownBSE, tool, samples...\n\nid: circRNA 的唯一标记信息\nsymbol: 基因名\nstrand: 链信息\nchrom: circRNA 染色体信息\nstartUpBSE: circRNA 染色体起点\nendDownBSE: circRNA 染色体终点\ntool: 方法标记\nsamples…：其他每列都对应处理的一个样本\n\n下面是测试生成的文件：\n\n\n\nimage-20230903162442921\n\n\n查看它的部分内容数据：\n\n\n\nimage-20230903162511289\n\n\n用户可以利用流程不同的结果进行下游的分析和研究，包括：\n\n不同方法的检测结果\n同一个样本不同方法检测的聚合结果\n整个输入样本数据集的检测整合结果"
  },
  {
    "objectID": "posts/2023-09-04-discourse-on-the-method/index.html",
    "href": "posts/2023-09-04-discourse-on-the-method/index.html",
    "title": "discourse on the method",
    "section": "",
    "text": "笛卡尔在《方法论》中揭示了科学研究和发明创造的普适方法，并概括为 4 个步骤：\n\n不盲从，不接受任何自己不清楚的真理。不管什么权威的结论，只要没有经过自己的研究，都可以怀疑。\n化繁为简、化整为零。\n解决小问题时，按照先易后难的次序，逐步解决。\n解决全部小问题后，综合看是否解决了原来的问题。\n\n科学发展的规律为：\n\n提出问题\n进行实验\n从实验中得到结论并解释\n将结论推广并普遍化\n在实践中找出新的问题\n\n\n来源： 吴军 —— 《全球科技通史》"
  },
  {
    "objectID": "posts/2023-09-12-mapq-and-as/index.html",
    "href": "posts/2023-09-12-mapq-and-as/index.html",
    "title": "MAPQ（映射质量）与-AS（比对得分）",
    "section": "",
    "text": "今天看 bwa mem 的 -q 选项，非常困惑：“-q, Don’t reduce the mapping quality of split alignment of lower alignment score”。 与我原来想象中 mapping quality 和 alignment score 两个东西应该相同所不同的是，我查询后发现这的确是两个不同的概念。\nmapping quality (MAPQ) 是告诉你某一个 read 来自某一个基因组区域的可靠程度（可信度），而 alignment score（AS）告诉你的是这个序列与参考序列有多相似（生信图书/教程中一般说的比对算法得分就是这个），匹配的多得分就高。\n如果你的 reads 完美的匹配到了多个基因组区域，那 AS 就会高但是 MAPQ 会低，如果你的 reads 有一些错配之类的，但匹配到某些位点的可信度比其他位点高，那么 AS 会低，但 MAPQ 会高。\n\n参考： https://www.biostars.org/p/179457/\n\n\n更多阅读\n\nhttps://genome.cshlp.org/content/suppl/2008/09/26/gr.078212.108.DC1/maq-supp.pdf\nhttps://bioinformatics.stackexchange.com/questions/2417/meaning-of-bwa-mem-mapq-scores\nhttps://maq.sourceforge.net/qual.shtml\nhttps://support.illumina.com/content/dam/illumina-support/help/Illumina_DRAGEN_Bio_IT_Platform_v3_7_1000000141465/Content/SW/Informatics/Dragen/TPipelineMAPQ_fDG.htm\nhttps://blog.csdn.net/tanzuozhev/article/details/89115080"
  },
  {
    "objectID": "posts/2023-09-13-check-cpu-usage/index.html",
    "href": "posts/2023-09-13-check-cpu-usage/index.html",
    "title": "检查 CPU 的使用量",
    "section": "",
    "text": "在 Linux 服务器上有时候需要检查下 CPU 的使用量，现在一般 server 的 CPU 都比较多，而在计算的时候 CPU 也是在动态切换的，像 htop 这种命令输出可以直观的看大体有多少被占用，但没法得到具体的数值。 而传统的指标 cpu load，一般也不太会去看，我们在运算时想的是尽量占用剩余的 CPU，所以如果能够得到比较直观的数字比较好。下面是一个 bash 函数，可以放到 ~/.bashrc 中去。 也可以直接在 Shell 中运行。\ncpu() {\n  top -bn2 | grep '%Cpu' | tail -1 | grep -P '(....|...) id,'|awk '{print \"CPU Usage: \" 100-$8 \"%\"}'\n}\n具体的解释可以看来源文章：https://www.baeldung.com/linux/get-cpu-usage"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html",
    "title": "Shiny 开发学习路径",
    "section": "",
    "text": "生信领域越来越多的数据库/网页工具采用 Shiny 框架进行搭建，Shiny 的流行度/拓展性在随着时间增加。因此，掌握 Shiny 服务于自己的项目/课题成为生信人的一个新的技能。 在这篇文章中，作为一个有经验的 Shiny 学习者和 Shiny 工具的开发者，我将介绍学习和掌握 Shiny 进行开发的核心路径和相关推荐的（免费）资料。对于每一个关卡（技能点）， 我会给出大致的（全职学习）时间估计（以周为单位）。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡1r-基础2-3-周",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡1r-基础2-3-周",
    "title": "Shiny 开发学习路径",
    "section": "关卡1：R 基础（2-3 周）",
    "text": "关卡1：R 基础（2-3 周）\n学习 R for Data Science。\n\n要求掌握 1-21 章的全部内容，后续章节初学者以了解为主，根据兴趣和时间进行实践。\n核心要点：\n\ntidyverse 的数据处理理念\nreadr 读取操作\ndplyr 数据处理理念和核心操作\nggplot2 可视化理念和核心操作\n\n读者目标自查：可以独自完成从磁盘读取文件、过滤/转换数据、分组处理/聚合数据、生成可视化图表。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡2-网页基础-html5css-0.5-1-周",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡2-网页基础-html5css-0.5-1-周",
    "title": "Shiny 开发学习路径",
    "section": "关卡2: 网页基础 HTML5/CSS （0.5-1 周）",
    "text": "关卡2: 网页基础 HTML5/CSS （0.5-1 周）\n阅读一遍 HTML 教程 和 CSS 教程。\nHTML 教程是重点，建议花多一些时间阅读；CSS 起到辅助和美化的作用，对于初学者，了解其基本语法、如何设定即可。\n核心要点：\n\n网页结构\n网页标签语法以及条目\nCSS 的设定\n\n读者目标自查：可以徒手写一个简单的网页，并通过 CSS 引入一些格式化。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡3shiny-学习2-3-周",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡3shiny-学习2-3-周",
    "title": "Shiny 开发学习路径",
    "section": "关卡3：Shiny 学习（2-3 周）",
    "text": "关卡3：Shiny 学习（2-3 周）\n学习 Mastering Shiny。\n\n要求认真阅读全部内容，并根据读物的代码进行实践，边学边练。\n核心要点：\n\nShiny app 结构\nShiny 网页布局\nShiny reactivity\nShiny module\n\n读者目标自查：可以在关卡1的基础上，将可视化展示通过 Shiny app 实现，并提供多个选项用于控制图形的输出展示。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡4-学习如何组织-shiny-app-代码1-周以上",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡4-学习如何组织-shiny-app-代码1-周以上",
    "title": "Shiny 开发学习路径",
    "section": "关卡4: 学习如何组织 Shiny app 代码（1 周以上）",
    "text": "关卡4: 学习如何组织 Shiny app 代码（1 周以上）\n好的代码结构，会极大地方便运维和拓展你写的 Shiny app。 下面有几种推荐的代码组织形式，读者可以根据自己的需求参考设定，也可以根据罗列的资料逐步学习。\n\nShiny 默认的代码部署方式\n\n使用一个 app.R 文件存储全部的前后端代码，适合非常小的应用。\n使用单独的 ui.R 和 server.R 文件放前后端代码，适合稍微复杂一点的小应用。\n在以上两种设定的基础上，将复杂的操作逻辑统统整理为函数，放到单独的 R/ 目录下或者一个 functions.R 文件中方便调用和维护。如果更复杂一点，可以整理为一个 R 包，需要通过 R Packages 学习如何开发 R 包。\n\n使用 golem 框架，学习 Engineering Production-Grade Shiny Apps\n使用 rhino 框架，通过 官方各种文档 学习。\n\n读者目标自查：理解和熟练掌握 Shiny 默认的代码部署方式。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡5-学习如何部署-shiny-app1-2-周",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡5-学习如何部署-shiny-app1-2-周",
    "title": "Shiny 开发学习路径",
    "section": "关卡5: 学习如何部署 Shiny app（1-2 周）",
    "text": "关卡5: 学习如何部署 Shiny app（1-2 周）\n当你开发的 app 可用时，接下来就是进行部署，方便自己测试或者他人使用。有以下几种部署的方式：\n\n通过 Posit 公司提供的 shinyapps 服务。推荐初学者尝试和摸索。\n自己在本地的/云服务商的 Linux 服务器上进行部署，需要了解 Linux 系统并掌握 Linux 的基本操作（阅读 The Unix Workbench）。具体的部署可以参考以下几篇文章：\n\n在Ubuntu服务器上配置Shiny应用\nHow to Deploy Interactive R Apps with Shiny Server\nHow To Set Up Shiny Server on Ubuntu 20.04\n\n\n读者目标自查：掌握第 1 种应用部署方式；探索第 2 种部署方式并能解决遇到的问题。"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡6-学习和重现完整的-shiny-app2周以上",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#关卡6-学习和重现完整的-shiny-app2周以上",
    "title": "Shiny 开发学习路径",
    "section": "关卡6: 学习和重现完整的 Shiny app（2周以上）",
    "text": "关卡6: 学习和重现完整的 Shiny app（2周以上）\n这是最后一道关口，闯关意味着你具备了独立完成 app 开发的能力。大家（特别是初学者）需要谨记，一个好的 app 在于它的代码结构清晰、能够完成任务，不在于漂亮的 UI。 所有的功能实现应当把完成分析/可视化目标为第一位，然后再花时间探索具体的 UI 美化。\n由于各类读者背景和需求不一致，我这里只做一些通用的学习资料推荐，针对读者所在领域的 app 开发，需要读者自行了解和掌握相关知识背景、分析技能，在此基础上寻找一些已经通过 Shiny 实现的数据库/工具的源代码仓库进行学习。\n\nShiny examples: https://github.com/rstudio/shiny-examples\nShiny gallery：https://shiny.posit.co/r/gallery/。官方提供了很多功能特性的 app 实现。\nShiny Data Portal: https://github.com/SebKrantz/shiny-data-portal。A free Shiny Application to host, filter, aggregate and and download data in various formats.\nmachLearn, Machine learning dashboard created with R/shiny: https://github.com/davesteps/machLearn\nShiny app for customizing Raincloud plots: https://github.com/gabrifc/raincloud-shiny"
  },
  {
    "objectID": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#推荐资料",
    "href": "posts/2023-09-13-shiny-learning-path-for-bioinformatics/index.html#推荐资料",
    "title": "Shiny 开发学习路径",
    "section": "推荐资料",
    "text": "推荐资料\n在本文中，我尽量避免推荐比较多的资料，因为对于初学者过多的资料带来的是混淆与压力。我希望你能够通过上述的关卡真正有效地学习和掌握 Shiny 开发的知识要点，并应用于你感兴趣的项目中去。 作为本文的最后一部分，我将提供一些非常重要的资源地址，希望在你困惑或者想要学习更多内容时找到灵感！\n\nhttps://github.com/veeps/shiny_workshop\nhttps://github.com/grabear/awesome-rshiny\nhttps://github.com/nanxstats/awesome-shiny-extensions\nhttps://github.com/davidruvolo51/shinyAppTutorials\nhttps://github.com/daattali/advanced-shiny\n\nEnjoy playing with Shiny!"
  },
  {
    "objectID": "posts/2023-10-07-show-me-ai-cheatsheet/index.html",
    "href": "posts/2023-10-07-show-me-ai-cheatsheet/index.html",
    "title": "ShowMeAI 速查表",
    "section": "",
    "text": "#分享# #AI# 今天看到 showmeAI（https://github.com/ShowMeAI-Hub），有一些速查表非常不错，这里整理分享给大家。 https://www.aliyundrive.com/s/mogzKzDSinf 点击链接保存，或者复制本段内容，打开「阿里云盘」APP ，无需下载极速在线查看，视频原画倍速播放。"
  },
  {
    "objectID": "posts/2023-10-09-kk-nobel/index.html",
    "href": "posts/2023-10-09-kk-nobel/index.html",
    "title": "2023 年诺贝尔生理或医学奖授予 KK",
    "section": "",
    "text": "匈牙利科学家Katalin Karikó和美国科学家Drew Weissman获得了2023年诺贝尔生理学奖或医学奖，因为他们在核苷酸碱基修饰方面的发现，使得开发出针对COVID-19的有效mRNA疫苗成为可能。\n虽然处于生物医学领域，但对于 mRNA 疫苗也只是耳闻，而没有细致去了解过。真正让我着力写下这篇记录的，是Katalin Karikó几十年跌宕起伏的科研生涯，这给予我莫大的震撼和感动。 在流量的推荐下我也刷到了两位获奖者的采访视频，相比于Drew Weissman的沉稳冷静，KK溢于言表的热情和笑容让我再次深刻感受到科学研究的魅力。\n我似乎能够通过附着在KK身上的岁月痕迹去窥探未来我可能的命运，当然，我并不是说我能够产生巨大的科学贡献并幸运地荣获这类科学大奖。 我在思考的是，我该用如何的态度去面对未来不确定的职业道路，如何去探寻和坚持未来想做的工作、去感受生活的起落、去陪伴爱人家人、去教育子女。 人是一种复杂的多细胞生物，在岁月的成长中，我们引入了太多物质与精神的变量，但所谓“不忘初心”的，不是回到多少年前某个幼稚的思考、充满天真梦想的时刻， 而是坚持一种贯穿身心与时空的信念和力量，让自我的“神迹”绽放在心灵的沃野，完成一场超脱自我的“成佛”之旅。\n最后用 KK 讲述的一段话作为你我的共勉：\n\n涉足科学领域，你会经常在毫无把握的情况下工作。我经常把做科学比作我女儿的划船：你在一艘向后行驶的船上，看不清鱼鳍线有多远（卡里科的女儿苏珊·弗朗西亚是两届奥运会赛艇冠军和金牌得主）。所以我认为，激情、专注和使命感是在研究生涯中取得成就的关键因素。"
  },
  {
    "objectID": "posts/2023-10-09-webr-in-quarto/index.html",
    "href": "posts/2023-10-09-webr-in-quarto/index.html",
    "title": "结合 quarto 出版系统和 webr 技术",
    "section": "",
    "text": "这是一个简单的例子，直接通过浏览器运行 webr 技术来实时运行 R 代码。你还可以实时修改代码进行交互式探索！\n🟡 Loading\n  webR...\n\n\n  \n\n🟡 Loading\n  webR...\n\n\n  \n\n🟡 Loading\n  webR...\n\n\n  \n\n技术详情可以参阅 quarto-webr 拓展文档。 目前该插件（包括 webR）还在积极的开发中，期待未来更加成熟稳定，跟本地执行代码一样方便和容易， 我相信这会极大地改变开发者和使用者的交互方式。"
  },
  {
    "objectID": "posts/2023-10-19-can-i-use-open-source-tools-for-free/index.html",
    "href": "posts/2023-10-19-can-i-use-open-source-tools-for-free/index.html",
    "title": "聊聊开源协议：我们可以免费使用开源软件吗？",
    "section": "",
    "text": "答案是 YES!!!\n逐步了解开源的朋友，可能会对开源协议产生困惑。既然协议中有那么多的限定，我们是不是在一些情况下无法使用他们呢？\n如果是商业行为的使用（用别人的东西赚钱不应该遵守一些条条框框吗？），咱们暂且不论。作为个人使用者，我们首先需要区分“使用软件”和“改写软件”是两个完全不同的行为，而开源协议中各种条目都是写给那些改写它的人看的！\n\n\n\n\n来源：https://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html\n\n\n\n\n注意这里的第一出发点就是是否发生了“改写”行为。\n\n简单说就是不改源码，所有开源协议都支持写闭源软件。\n我之前也很困惑，所以看过很多介绍。主要是我们混淆了修改别人的代码和使用别人代码，开源协议限制的是对别人代码的修改（包括在你自己的软件包里拷贝了他人的源码），不限制后者。\n一般我们做数据分析，喜欢拷贝和利用已有的代码，这就是“改写”行为。假设我们不遵守相关的协议（比如有些博客要求你放上博客地址和作者等等），那么就违规啦。但是，如果你是看着别人的思路或代码自己完全重新写的， 就不是“改写”行为了！所以，不难发现，如果存在违规，这种除了本人其他人是没法分辨的。当然了，一般网上公开的代码也不存在任何的协议，以及常见的代码很难溯源和追责，所以我们不需要考虑这些问题，根据自己情况使用就行。"
  },
  {
    "objectID": "posts/2023-10-27-revist-nanjing/index.html",
    "href": "posts/2023-10-27-revist-nanjing/index.html",
    "title": "南京行记 两首",
    "section": "",
    "text": "其一 重访南京有感\n\n\n\n烟水近，城镇明，高楼须弥作蝼蚁。\n夜半独飞落江宁，驰霄横度三千里。\n旧都印，故金陵，六朝风流休说起。\n夫子庙头看古今，忆往年书生意气。\n\n\n\n\n\n\n\n其二 足踏秦淮有记\n\n\n\n行路迟迟眼欲呆，金陵小巷赋文才。\n流水车龙听钟鼓，穿街梧桐掩楼牌。\n集庆门接迎凤道，长干桥通雨花台。\n秦淮酒家今安在，旧里新村说不来。"
  },
  {
    "objectID": "posts/2023-10-29-dont-forget/index.html",
    "href": "posts/2023-10-29-dont-forget/index.html",
    "title": "dont forget",
    "section": "",
    "text": "不要忘记，有一个诺贝尔奖的梦。\n不要忘记，最多也就剩下四十余年。"
  },
  {
    "objectID": "posts/2023-11-13-download-and-prepare-sra-data/index.html",
    "href": "posts/2023-11-13-download-and-prepare-sra-data/index.html",
    "title": "SRA 测序数据下载和处理三部曲",
    "section": "",
    "text": "这里简单记录和备忘下下载和预处理 SRA 数据我常用的三个基本步骤。"
  },
  {
    "objectID": "posts/2023-11-13-download-and-prepare-sra-data/index.html#依赖准备",
    "href": "posts/2023-11-13-download-and-prepare-sra-data/index.html#依赖准备",
    "title": "SRA 测序数据下载和处理三部曲",
    "section": "依赖准备",
    "text": "依赖准备\nconda install -c bioconda -c conda-forge parallel-fastq-dump 'sra-tools&gt;=3.0.0' fastp"
  },
  {
    "objectID": "posts/2023-11-13-download-and-prepare-sra-data/index.html#步骤",
    "href": "posts/2023-11-13-download-and-prepare-sra-data/index.html#步骤",
    "title": "SRA 测序数据下载和处理三部曲",
    "section": "步骤",
    "text": "步骤\n\n下载\n\nprefetch -c --max-size 100GB -C yes -r yes -O . -p SRR8670672\n\n转换数据格式为 FASTQ\n\nparallel-fastq-dump -t 20 -O fq/ --split-3 --gzip -s SRR8670672\n\n预处理 FASTQ 文件\n\nfastp -i fq/SRR8670672_1.fastq.gz -I fq/SRR8670672_2.fastq.gz -o output/SRR8670672_1.fastq.gz -O output/SRR8670672_2.fastq.gz -h output/SRR8670672.html -j output/SRR8670672.json --thread 16 --dont_overwrite\n如果多个文件还可以结合 multiqc。"
  },
  {
    "objectID": "posts/2023-11-14-random-thoughts/index.html",
    "href": "posts/2023-11-14-random-thoughts/index.html",
    "title": "随想",
    "section": "",
    "text": "没有问题只有目标的数据分析，就像人有大脑却缺乏灵魂。"
  },
  {
    "objectID": "posts/2023-11-15-talk-about-ides/index.html",
    "href": "posts/2023-11-15-talk-about-ides/index.html",
    "title": "谈谈使用过的代码编辑器",
    "section": "",
    "text": "最近在使用 JupyterLab 时感觉受挫，想简单总结下自己这些年使用过的代码编辑器（有较长的使用时间）。\n最开始接触的应该是 Sublime text3，简单、轻量、速度响应快，缺点是要自己搞定盗版问题、要自己配置、装插件等。\n后面大量使用 R 和利用 R 开发，所以 RStudio 和 RStudio Server 使用非常多，它可以系统解决利用 R 进行的一系列数据处理和开发工作。缺点是单线程、有时候出问题会卡得很、与其他编程语言几乎很难协同，包括我常用的 Bash 和偶尔用的 Python，这类体验感很不好。\n再后面有用过 Jupyter 笔记本，优点是简约轻便，它对 Python 的支持比较好，其他比较烂，界面和导航不太行，我近期尝试部署 JupyterLab 后发现对插件的支持体验不是很理想，然后插件对 R 的支持比较烂。\n还用过 JetBrains 的一系列编辑器，包括支持 Python、Go等等的不同软件名称，但基本框架是一样的，后面在 IDEA 中都可以进行支持，优点是交互感好、界面专业、插件稳定好用、非常适合开发，缺点是不太适合数据分析探索类工作，也比较笨重。如果是学生/老师用户可以免费使用。\n全部使用过的所有编辑器中综合体验感、性能、特性等比较完美的是 VS Code，不愧是大厂出产的产品，这些年陆续间歇在用，功能演进频繁但不失稳定，目前包括 R、Python、Jupyter的支持都比较好，更不说其他常用编程了。缺点是 VS Code 在 R 方面的专业性没有 RStudio 强（特别是图像预览的方便程度），另外界面有时候个人使用起来不是很喜欢（相比于 JetBrains 产品）。\n当前还有一些新的产品在推动编辑器的发展，包括JB 的 Fleet，我觉得非常有前景。\n当然了，终端上用过 Vim，它基本是命令行的无冕之王，虽然我极少在终端写代码，但用它基本已经满足了。之前还学习过 Vim 和相关配置、包括 Emacs，但实际上个人的使用偏好和职业领域导致这一块的使用极少有所进展，只是满足偶尔的编辑需求。而这部分需求，现状VS Code 都是可以替代的。\n综上看，如果是从事与我类似的数据科学工作和开发的读者朋友，优先选择 VS Code 和 RStudio 是比较好的选择。"
  },
  {
    "objectID": "posts/2023-11-21-talking-from-faker/index.html",
    "href": "posts/2023-11-21-talking-from-faker/index.html",
    "title": "从 Faker 夺冠讨论竞争与热爱",
    "section": "",
    "text": "好的想法需要记录，在这里动笔前，昨天睡前的好想法差点从指缝中溜走。\n\nFaker，闪耀英雄联盟十年的职业选手，带领 T1 再次登上王座。 这是他的第 4 座世界赛冠军奖杯，但他离上一次登顶已经过去很多年了。 想要回忆，都已经得翻过我人生几千天的篇章。在那个懵懂和充满斗气的年代， 他也是青涩，却以高超的技巧以魔王的姿态统治了游戏中的王朝。\n但他又再次登顶了，当众多英雄折戟沉沙、黯然退出，为什么他还在坚持？ 为什么在沦入尘沙时坚持，为什么在近年当千年老二坚持？奶奶对他说， 要做便做到最好。他早已做到最好，为什么还在坚持？\n我思考了很久。只有热爱，才能解释。\n在越加竞争的世界里，我曾以为科研工作是一个宁静的港湾，但很显然，飓风已成，一切都将被席卷。 当资源的博奕无法避免，个人而言，躺平是最下层的解法。 为什么今年的比赛 Faker 带领的 T1 能够把把将 LPL 战队击穿，如人机一般？ 我在一个知乎的帖子里看到了最认同的解析：“不惧强度对抗”。 「不怕摩擦，不怕双方接触，寻求双方对抗。」在对抗中寻找战机和赛果。\n未战先怯，败亡之道。\n这狠狠地击中了我近年的“科研心态”。个人独立独行时，我们天然地寻求庇护之所，以求遮风挡雨、得以存活。 我们把对思想的追求降低到对生活的满足，就此沉沦。工作/事业，绝不会因为你把它当作一份工作而出色，它只会 因为你的热爱，你灌注的思想情绪而出彩。\n固然，世界，特别是周遭的环境变化太大、压力太巨，但追求之心、努力之行不该有所松懈。不要让任何的标签定义，不要让任何的时间限制，不要让任何的思想束缚，一步一步、锤炼再锤炼、博弈再博弈。\n与天斗，其乐无穷；与地斗，其乐无穷；与人斗，其乐无穷。"
  },
  {
    "objectID": "posts/2023-12-07-e-pluribus-unum/index.html",
    "href": "posts/2023-12-07-e-pluribus-unum/index.html",
    "title": "E pluribus unum: From Complexity, Universality",
    "section": "",
    "text": "标题是陶哲轩于 2012 年发表的一篇论文题目，表达了从多样性、复杂性中产生统一、普遍性的意念。这可以涉及到社会、科学、文化或其他领域，强调在多样性和复杂性中寻找普遍性原则或价值观。\n\n探究宇宙中的秩序和混沌一直是科学研究的核心命题。陶哲轩教授的这篇洞察性文章引领我们进入普适的世界——一个展现了如何从错综复杂的微观动态系统中抽象出简明的宏观规律的奇妙领域。从统计学的经典规律到物理学中的相变现象，从自然数序列的神秘规律到量子力学的精确预测，普适性作为一种被广泛记录和验证的模式在各种不同尺度和领域中展现了其惊人的一致性。然而，尽管这些规律已被实证学科反复确认，但对于普适性的严格数学基础仍存在着诸多未知之谜。\n– 译文\n\n感觉这就是我一直想要追求的科研道路，但如何从细节入手，进入这一方世界，当前的我还一无所知。\n自然是一朵变幻莫测的云，永远如一，又从不相同。 —— Ralph Waldo Emerson，《历史》(1841年)\n译文一些摘要：\n\n在一个包含许多相互作用组件的系统上无法执行切实可行的计算，这一现象被称为“维数诅咒”。尽管存在这个诅咒，但是当组件数量足够大时，通常会出现一个令人惊奇的现象：复杂系统的聚合属性（注：就是统计量，例如均值）可以不可思议地再次变得可预测，并由简单的自然法则支配。\n更令人惊讶的是，整个系统的宏观规律常常在很大程度上与控制系统内部单个组件的微观规律无关。换句话说，人们可以用完全不同类型的对象替换微观组件，并在宏观层面上获得相同的控制规律。这意味着，无论系统的微观组成如何，只要系统的组件数量足够多，宏观规律就会出现，并且这些规律可以独立于微观组件的特性而存在。我们就说宏观规律是普适的。\n大数定律（Law of large numbers）：在数学与统计学中，大数定律又称为大数法则、大数律，是描述相当多次数重复实验后的结果的定律。根据这个定律，随着样本数量的增加，其算术平均值越来越有高的几率接近期望值。大数定律非常重要，因为它“说明”了一些随机事件均值的长期稳定性。人们发现，在重复试验中，随着试验次数的增加，事件发生的频率趋向于一个稳定值；人们同时也发现，在对物理量的测量实践中，测定值的算术平均也具备稳定性。比如，我们向上抛掷一枚硬币，硬币落下后哪一面朝上是偶然的，但当我们抛硬币的次数足够多后，达到上万次甚至几十万、几百万次之后，我们会发现硬币每一面向上的次数约占总次数的二分之一，亦即偶然中包含着必然。大数定律是数学和自然界中最简单、最容易理解的普适性规律之一，但它绝不是唯一的一个。\n另一个最基本的普适性规律的例子是中心极限定理（Central Limit Theorem）。粗略地说，这个定理断言，如果一个统计量是由许多独立且随机波动的组成部分组合而成，且没有一个组成部分对整体有决定性影响，那么这个统计量将近似地按照一种称为正态分布（或高斯分布）的规律分布，或者更通俗地称为钟形曲线。这个规律是普遍的，因为无论个别成分如何波动，或者成分有多少个，它都成立（尽管成分数量越多，规律的准确度越高）。\n中心极限定理存在着一些扩展，它们是针对某些略有不同的统计数据类型的普适性规律。本福德定律（Benford’s Law）就是一个例子，它是关于大量级统计数据的前几位数字的一个普适性规律，如一个国家的人口数量或账户的金额大小；这个法则给出了一些反直觉的预测，比如，在自然界中出现的任何给定统计数据都更可能以数字1开头，而不是数字9开头——实际上以1开头的概率是以9开头的六倍多。\n类似地，Zipf 定律是描述特定类别中最大统计量的普适性规律，例如世界上人口最多的国家或英文中使用频率最高的单词。它指出，统计数据的大小通常与其排名成反比；因此，例如，第十大的数据大约是第五大数据的一半大小。这个法则对于最顶端的两三个数据可能不太精确，但在之后的数据中准确性提高。与数学上较容易理解的中心极限定理和本福德定律不同，Zipf 定律是基于经验的。它是通过实践观察而确定的，但数学家们尚未给出一个完全满意和有说服力的解释，来说明这个定律怎样产生以及为什么具有普适性。\n在20世纪50年代，著名物理学家和诺贝尔奖得主尤金·维格纳（Eugene Wigner）研究了这些共振统计数据，并提出了一个非凡的数学模型来解释它们，就是我们现在所称的随机矩阵模型（random matrix model）。这些模型精确的数学细节太过技术性，在此不便描述。但总的来说，可以将这样的模型看作是一个大量质量的集合，所有质量都通过各种随机选择的、不同强度的弹簧连接在一起。这样一个机械系统将在一定的频率下振荡（或共振）；维格纳假设认为，大型原子核的共振应该类似于随机矩阵模型的共振。特别地，它们应呈现相同的排斥效应。因为可以严格证明随机矩阵模型的频率排斥，所以可以对核共振实验观察到的相同现象提供启发式的解释。高斯酉集（Gaussian Unitary Ensemble，GUE）假设\n然而，普适性定律确实存在一定的局限。以中心极限定理为例，它预测了任何由众多小的、独立因素组合而成的数量都将趋向钟形曲线分布。但如果不满足该定理所需的前提假设，这个定律就可能不适用。比如说，所有成年人（男性和女性）的身高分布并不符合标准的钟形曲线，因为性别这一单一因素对身高的影响巨大，无法被其他所有环境和遗传因素平衡掉。\n中心极限定理还有一个重要的失效场景，那就是构成某个量的独立因素并非真正独立，而是存在相关性，导致它们倾向于同涨同跌。在这样的情况下，可能形成“胖尾”现象（俗称“黑天鹅”），其数值会比中心极限定理所预测的平均值波动范围大得多。这一现象在金融模型中特别重要，在处理复杂金融工具时尤其明显，比如通过汇集抵押贷款所形成的抵押债务担保证券（CDOs）。只有在假设各个抵押贷款相互独立时，中心极限定理才能用来评估这些工具的风险；但在近期金融危机中（典型的“黑天鹅”事件），这种独立性假设戏剧性地崩溃了，导致许多持有这些证券的投资者和它们的保险公司遭受巨大财务损失。数学模型的强大仅取决于其背后的假设是否坚实。\n普适性规律瓦解的第三种情形是系统没有足够的自由度来使得这些定律生效。举例来说，宇宙学家可以依据流体力学的普适性规律来描述整个星系的运动模式，然而，在仅受到三个天体引力影响的环境下，单个卫星的运动可能要复杂得多——这实际上就是所谓的火箭科学。\n流体力学的普适性规律还会在介观尺度上失效：这个尺度比单分子的微观尺度要大，但又小于宏观尺度，后者是普适性规律适用的范围。一个典型的介观流体例子是流经血管的血液；构成这种液体的血细胞非常大，以至于它们不能简单地被视作微观分子的集合体，而应该被认为是具有复杂行为的介观实体。其他一些具有有趣介观特性的材料包括胶体流体（比如泥浆）、某些类型的纳米材料和量子点等；为这类材料建立准确的数学模型仍然是一个持续存在的挑战。\n古德哈特定律（Goodhart’s law）表明，当一个度量指标成为目标时，它就不再是一个好的度量指标。"
  },
  {
    "objectID": "posts/2023-12-08-the-world-is-not-all-sunshine-and-rainbows/index.html",
    "href": "posts/2023-12-08-the-world-is-not-all-sunshine-and-rainbows/index.html",
    "title": "The world is not all sunshine and rainbows",
    "section": "",
    "text": "刷短视频的时候看到小孩跟史泰龙激情地对一段电影（洛奇）台词，把它翻找了出来，记录一下。\n　　You let people stick a finger in your face and tell you you’re not good.\n　　你让别人指着脸说你不好。\n　　And when things got hard, you started to looking for something to blame, like a big shadow.\n　　当遇到不顺心的事，你开始怪罪其他事，比如说巨大的阴影。\n　　Let me tell you something you already know.\n　　让我告诉你一些你已经知道的事情。\n　　The world is not all sunshine and rainbows.\n　　这世界不完全都是阳光和彩虹。\n　　It’s a very mean and nasty place, and I don’t care how tough you are.\n　　这是一个非常卑鄙险恶的世界，我不在乎你是多么坚强。\n　　It will beat you to your knees and keep you there if you let it.\n　　如果你屈服，他会把你打得永远趴在地上。\n　　You, me or nobody is gonna hit as hard as life.\n　　你，我，没有人能比生活更会击打人了。\n　　But it’s not about how hard you hit. It’s about how hard you get hit and keep moving forward, how much you can take and keep moving forward.\n　　但这不在于你击打的多重，而在于你能被击打的多重，仍保持前进，在于你能怎么承受，然后继续前进。\n　　That’s how winning is done.\n　　这才是真正的胜利。\n　　Now if you know what you’re worth, go and get what you’re worth.\n　　现在如果你知道你的价值，你就前进实现你的价值。\n　　But you gotta be willing to take the hits, and not pointing fingers saying you aren’t where you wanna be because of him or her or anybody!\n　　但你必须乐意承受挨打，而不是没能实现目标时却指着别人说是因为他或她的原因!\n　　Cowards do that, and that ain’t you! You’re better than that!\n　　懦夫才会那样做!你不是!你远比那个出色!"
  },
  {
    "objectID": "posts/2023-12-12-the-poem-of-youth/index.html",
    "href": "posts/2023-12-12-the-poem-of-youth/index.html",
    "title": "少年时",
    "section": "",
    "text": "这里主要存储下大学期间随笔写作经整理的诗篇。\n网页放一首。\n\n镜照 \n听夜风沉沉\n弹奏在其中的吉他一声声\n寂静的黑里\n见不到家乡的月与星辰\n远方，谁在等 \n看秋叶层层\n潜藏于体表的圆圈一轮轮\n微冷的雨中\n何处疗梦里的痴与伤痕\n近处，我轻问 \n山一程，水一程\n一路千里的风尘\n生命的刻度勾勒距离的尺度\n我越走越浑\n瞧，镜子里的身影\n白皙的脸上斑点纵横\n总被迷惑在模糊的清晰\n而一走近\n现实的自己\n呈现得一滴不剩\n这岁月的梦魇\n该死的真诚 \n逃不掉，如何跑\n问问问，一声一声接一声\n我对着我自己\n里头是静止的灵魂\n外面是活着的人生 \n\n全文见《少年时》\n\n感谢梦婷几年前的整理。"
  },
  {
    "objectID": "posts/2023-12-12-the-archive-of-poem/index.html",
    "href": "posts/2023-12-12-the-archive-of-poem/index.html",
    "title": "诗歌存档",
    "section": "",
    "text": "存档下研究生期间写的一些诗，主要从简书搬运过来。大抵作于16年底到19年初。\n\n\n电梯\n楼上的人想下来\n楼下的人想上去\n在电梯门口相遇\n奔往不同的去处\n夜深了\n众人归寝\n他似从未闪过你的眼窗\n你也未曾入了他的梦\n\n\n脑中住着一个恶魔\n我终于发现——\n我的脑中，住着一个\n喋喋不休的恶魔\n它巧妙地隐藏在\n我思绪的背后，如此小心地\n偷偷窥视着我的一切\n以至于，二十二年的时光里\n我毫无所知\n它第一次出现踪迹时\n我还小，还年轻\n现在我知道，它一直\n躲在我思绪的影子下\n它的动作太快，我的上一个念头\n才产生反应，它已经\n没有了踪迹\n我不知道我现在怎么发现了它\n正如我知道它曾经有被我发现\n啊，曼妙的思潮\n涌过了时空的海洋\n降临于我之身\n贫瘠的文字和韵律般的诗词怎能描述\n那与我共生的恶魔啊\n闪烁的火花被语言粘附\n怎能寻得来时的去处——\n便是我同恶魔一起从天国降落之地\n\n\n我们在追寻着什么\n我们在追寻着什么\n像夜空追寻黎明\n像喧嚣追寻安静\n厌倦着黑夜，又寻觅着孤寂\n我们在追寻着什么\n像群星围绕着明月\n像鹤立在鸡群\n突显着自立，又遮掩着卑微\n我们在追寻着什么\n像头仰望着星空\n像脚觅着实地\n怀揣着梦想，又难以飞起\n我们在追寻着什么\n像夸父追逐着金乌\n像愚公搬弄着山石\n愚昧地坚持，又诚恳地守望\n我们在追寻着什么\n像不甘坠在平凡\n像生活归于平淡\n追寻着什么，丢失了什么\n\n\n蝶恋花（一）\n枫叶落红蝉韵歇。一觉秋深，窗外风声烈。\n柳色犹新珠露结，厌教人世轻离别。\n故友但去音笑绝。有限心身，仍共年年月。\n何必年光留且住，随风吹取三山去。\n\n三山：古代神话，东方大海里有三座仙山，叫做蓬莱、方丈、瀛州\n\n\n\n蝶恋花（二）\n阅尽友朋难久聚。旧梦残痕，空落思如许。\n相忆常叹别离苦，秋风日夜无头绪。\n拟送相思明月处。时雨轻寒，无计留它住。\n一树桂花香暗渡，楼高不见来时路。\n\n\n夜行者\n我是夜中行者，\n在梦眠时入睡，\n在梦醒时工作。\n睡时我总爱睁着眼睛，\n摆动身躯，频频顿首，\n容易，不废气力。\n讨厌的是有时没法，精神紧张就\n只能收紧身形，端坐，\n好一副可爱模样。\n我爱工作，\n我是夜行者，\n我爱工作。\n随意地逡巡，行走\n走过四季，边走边\n望星空，望远山，望\n春的夜风抚柳，听\n夏的蛙蝉相和成歌，闻\n秋的桂树落香，触\n冬的雪花融化，\n——成了水\n喔！这一年的四季。\n喔！这夜的行者。\n伟大的夜的行者，\n我要唱歌，喔——\n该死的长喉，却是个咽炎患者！\n那就继续走吧，\n等那梦眠时我便要睡去，如何\n再赏这多情的四季，\n再望情坚不移的夜空星辰。\n啊，上天，不要\n不要让我躺下——\n让我继续走，前行！\n我是夜的行者。\n啊，不要——\n一躺下，\n满是空虚，脑中会传来\n心里的声音。\n\n\n疯子对傻子\n你问，\n你别问——\n我从哪里来，\n说得清\n说的不清。\n你念，\n你别念——\n我往何处去，\n不改变\n一直改变。\n你笑，\n你别笑——\n我为何在这里，\n人只道\n初衷不复。\n你哭，\n你别哭——\n昨梦明朝尽空，\n恭喜我\n已修成疯。\n\n\n点绛唇 望潇湘\n一夜春风，\n河边吹绿柳多少？\n数声啼鸟，\n频传春芳晓。\n才过春初，\n却也春将老。\n远山道，\n清明芳草，\n看取西天渺。\n\n\n暮春早行\n垂枝映絮柳，情多添为愁。\n人生几回乐，微风绿春波。\n\n\n清明·崇明\n草木方始绿，时节入春深。\n遥叹连天水，近寻众芳痕。\n少小囿世面，老大拭喧尘。\n泛舟江海上，愿为钓鱼翁。\n\n\n春晨惊起\n恍然大梦春秋去，\n不知鸿雁南归来。\n惊起一瞥门窗外，\n楼下樱花倚春开。\n\n\n湿落的花，你莫留恋\n如果你不曾料想，\n又岂会无端地渴望。\n机械钟表上起了发条，\n是它在循环——\n还是你在旋转。\n听四月的风声暗哑，\n低摆抽绿的絮柳，\n园间一夜绽满的樱花，\n共着桃花，\n被连雨摧残。\n湿落一地的花啊，\n你是樱花，还是桃花。\n你原是渴望春雨滋润，\n还是期待飞落前的旋转？\n你也莫想，\n落红化入春泥，\n也莫贪恋，\n这人间的风景。\n四周有的是水泥，\n不缺的是轮回的春季。\n我在烟雨朦胧中写下了你，\n也只是因为，\n手里的机器，\n比我拥有更多的感情。\n\n\n夜中的诗人\n光消匿醒目\n夜于风中放歌\n委身黑暗，或鲜明\n你无须选择\n选择打造的包袱\n不必拿起，你\n孤注一掷，不留退路\n月是你的魂侣\n天晴，或多雨\n是一种姿态，只为\n靠近她\n再无忧伤，反复\n你是？\n躯壳里的空空\n月亮之上的白白\n空白，哦——\n是你，夜中的诗人\n心在跳，思想\n在发烫\n\n\n五月\n一场大雨，或天晴\n将四月的尾身摆弄\n水与空气升起嘈杂的共舞\n冲刷犹存春痕\n垂降夏季的五月\n温度骤击突来的灼感\n掀起层层陈年的躯皮\n燥热，或寒酷\n一时体内沉浸\n年年的感觉回归\n在这个初来的夏日\n记忆恍若齿轮\n回拨过往的印记\n真也是齿轮——\n盘过一圈，忘记\n是在拨后，或往前\n也忘记，正站在\n旧时开端的终点\n\n\n如果风不再刮\n如果风不再刮\n雨将笔直地下\n多少雨外的等待\n一场雨中的花开\n如果风不再刮\n小草无声地艳羡\n众木躯干的挺直\n多少历史的沧桑\n验证谁内心执着挺拔\n谁人倒下\n如果风不再刮\n古道西风瘦马\n笑成满纸黄沙\n铮铮心怀\n谁人来写\n谁人来听\n谁人来看\n\n\n行走在五月的风雨中\n五月的风雨初骤\n敲打你灯下的身影清瘦\n于飘零之中，绿叶登台\n恍然一场离家的远游\n乘风而上\n随雨而下\n风雨中的叶\n风雨中的你\n逢过春的花已鲜明\n润着水的绿正斑驳\n该骂这天的捉弄\n该笑这云的情多\n看天空放阴\n你兀自犹豫着\n行走在初夏的雨后\n\n\n当思绪反复成河\n我在寻找一个词语\n砥砺思绪，寻找\n一个开始，一句借口\n寻找，不停地\n要文艺，像那\n像那悲伤逆流成河\n唯美诗意\n要洒脱，将会\n悲喜均不沾\n无言升落\n可惜我最终也没找到\n或许我也说不出口\n一日一夜的二十四小时\n八万六千四百秒，有些精准\n当我的心跳敲过这时间\n地球又绕过了八万里的路程\n反复的起点，也是\n反复的终点\n无言的岁月，反复\n我的思绪，反复\n反反复复在过去的两万多个日子里\n不觉成河\n而我又该流向哪里呢\n我的大海\n我的思潮奔涌\n却还没有走到大陆尽头\n\n\n道化之花\n他躺在床上\n看海波上淫淫的细雨\n窗外，有海岸\n也有天空\n怎样去塑造一幅画镜\n我心想\n在作者的故事里\n在太宰的内心处\n退潮的海水\n释放对世界侵蚀的火焰\n道化之花\n在那个女人的身体上绽放，枯萎\n是的，那个一同殉情的女人\n谁知道是我将她杀死？\n我的朋友，你们可知道\n叶藏望向窗外\n可别离我而去，我的朋友\n\n\n那年七夕\n梦里一度贪欢，推杯到天明。\n风雨旧时已谙，何处寻真情？\n倚马待，望流云，天蓝人痴云不停。\n何似幼时观月，月在人无心！\n空中百鸟低巡，小镇响雷鸣。\n日照离人一群，有晴也无情。\n枝折花残谁在意，最是愁煞少年心：\n肯为新词把酿得的才学倾。\n品得书中颜玉，词新意不新。\n看罢太白诗行，千愁同古今。\n读新词，写旧诗，一生伴我度悲兴。\n万物莫不月似，当自负亏盈！\n七夕一岁又至，泪多软语频。\n长情更是短情，此山万年青！\n自古世间少味趣，诗与佳人共追寻。\n恰如我意不负长词不负卿。\n\n又翻出了14年七夕写的诗，心中莫名伤感。不，不应称为莫名。相同的情绪总是会在心底的某个时间点发酵，撒播出愁绪的气味。\n曾把青春看做一个年龄阶段的必然经历。但我已不清楚我现在是入了青春，还是已在青春之外。时间总是耦合经历，把成长的气息悄然掩盖，而其中横跨的岁月光景，似乎仅在海马区留下关联的模式，在偶然的联想中掀起朵朵的记忆之花，提示过去真的不复存在。说到底，我们都是时间的扫墓人。\n\n\n\n昏梦游思\n风似乎从海边刮来\n轻轻地，绕过森林与高楼\n柔柔地，轻抚垂柳与衣袖\n我站在高楼，感受着\n风正从海边拂来\n暑热退散\n天上的火球降为远景\n由黄转红的光色\n洗澄金日的余晖\n把留有残温的美景\n送将夜的君国\n一只鸟儿不耐寂寞，啁啾\n我猜想它不是在唱歌——\n文字里早已写烂\n求偶吧？身体往往\n比灵魂更诚实\n人儿又何尝耐得住啊，繁华的\n闪耀着白光，黄光，霓虹光的千盏万盏\n在夜兽的胃腔中游弋，呼吸\n争夺尚未分解的残食\n直到杯盘狼藉，独自蜷缩入梦\n盼着，梦里也是这般美好的世界\n一个只关于兽的世界\n而我，早已化身成兽\n\n\n踏莎行 端午至\n夜色将阑，\n黄鹂报晓，\n树头绿叶如新袄。\n浮光透尽室窗明，\n群楼难掩晨辉袅。\n芳草勃勃，\n离乡渺渺，\n手拨箬叶眉心挑。\n佳节又至欲销魂，\n频传故梦托家老。\n\n\n听歌\n旋律升起\n音调深浅高低\n交纵组合开始，美妙啊\n我渴望下一段\n心底有个相同的节奏\n恍若红日升起\n普照世界啊，音乐\n要什么神佛\n好像什么人要来了\n或是正在走过\n生活啊，人类\n我呆立良久\n心有所思\n物皆物语\n\n\n独自莫凭楼\n夜塔拨紫月拨银，\n远灯照，近蝉鸣。\n斜窥小径少人行，\n空余暗柳，\n倚光落疏影。\n不闻浦江远水流，\n近悠悠，远断愁。\n难悟人间有情眸，\n唯有短叹，\n独自莫凭楼。\n\n\n失眠\n躺着\n凌晨三点\n一点亮点\n又一个三点\n无眠……\n我怕是病了\n入骨的热潮裹挟\n夜幕的深渊\n靠近，一点\n一点，又一点\n在夏季的凌晨三点\n\n\n通宵\n我隐匿于黑暗\n沉醉于思考的逻辑\n寂静夜色里\n指针划过零点的界限\n在屏幕微芒中显露尖锐\n我浑然不觉\n思维在图影的脉搏间跳跃\n奔跑、迷失\n“黑夜给了我黑色的眼睛”\n我用它影射光明\n信息汹涌流动着\n在闪耀RGB编码的颜色光影里\n我无心睡眠\n也无心失眠\n\n\n寂寞诗\n寂寞的人逢着寂寞的夜\n等着同样寂寞的人\n该是何等的寂寞？\n寂寞上的寂寞\n寂寞后的寂寞\n寂寞里的寂寞\n最终能够不寂寞？\n有点寂寞\n喜欢说挺寂寞\n恐怕说好寂寞\n说不出不寂寞\n哦，寂寞\n\n\n尘光\n白天的时光里身处喧嚣\n夜晚至深时终归安宁\n这城市静卧后\n掸去了多少尘光\n我就要在这物心两安的时刻睡去时\n远桥的车光恍惚闪过\n尘光还在——\n我侧脸望向门窗外的夜空\n最后的一眼里——星光遮掩\n也许它一往如常\n\n\n夏末\n阳光是无声的沉默\n轻风在无言地相守\n这温暖而清爽的夏末\n几月的酷热退散\n季节转向初秋时候\n我在小河旁漫走\n眼中的绿色\n依垂在杨柳枝头\n粼粼泛起的波光中\n我又想起了你\n我又想起了你\n眼波里正泛起的湖光粼粼\n依垂在柳梢的绿色\n映着我在河边走过\n\n\n我在风中听雨\n我在风中听雨\n在雨中闻风\n在风雨中想你\n回到室内时\n风雨隔窗远去\n恍如背景\n灰白天际里\n我轻轻将你放入心底\n抬眼向西\n所有方向\n在一片迷离\n\n\n我是真的爱你\n我是真的爱你\n不仅爱你的身体\n也爱你的心\n爱阳光照你时的明媚\n也爱微风轻抚你时的温柔\n我是真的爱你\n愿解开所有物质的锁链\n放下精神上的所有\n远赴千里\n只为将你拥入怀中\n我是真的爱你\n衷心地许下每一个承诺\n又默默将它们守候，见证我们\n走过以后所有的一年四季\n春夏秋冬\n\n\n爱你的心\n时间抹不去\n疾病带不走\n别人拿不了\n爱你的心\n我爱的人呵\n只有你能将它呵护\n又能把它摧毁\n好好对待它吧\n它因你而坚强存在\n也因你而软弱无助\n\n\n立冬后\n立冬成昨日，好风下眉秋。\n懒起梳妆晚，对镜已白头。\n\n\n如一\n一边是想你的绵愁\n一边是工作的烦忧\n我望着窗外阴沉的上海\n这已是第二个年头\n慢道忧虑的日子终将过去\n去了归来，频频回首\n一边是遇你的欢喜\n一边是成长的苦痛\n我在冬日写下此诗时\n岁月正努力奔往春季\n诗中也努力与四季作一场别离\n末尾刻下爱你的如一\n\n\n认岁月\n若寻岁月身，音容辨旧痕。\n凭吊芳华易，健忘父母恩。\n\n\n晨起怅然有感\n一盏黎明一盏灯，一宿思绪梦难成。\n只恨神州地高远，别后山河是故人。\n\n\n我昨日逢着一条狗\n我昨日逢着一条狗\n我散步路过的时候\n看到它透过窗户\n正对着我咧嘴笑\n狗什么时候学会笑的？\n我心里惊讶得很——\n居然跟人一样\n皮肉拉扯，琢磨不透\n可能是在问好吧\n我点点头\n倒是一条好狗\n眼睛也闪着亮泽\n今天我又想遇着它\n却看到门口小孩杵着狗皮在耍\n一身贼亮的毛\n还有塌陷下去的笑\n\n\n一场九百年的梦\n我做了一场九百年的梦\n旁人问我为何\n我告诉他——\n我自己感觉如此呀\n我的感觉一向准确\n九百年，不多不少\n在那之前，老师叫我们——\n用“一场”造个句子\n正好不多不少，九百年\n我睡了九百年\n然后写了下来\n他骂我不老实——\n“你，你怎可能睡得这么久？”\n我微笑摇摇头\n指了指我的席位\n牌子上写着——\n“一场九百年的梦”\n\n\n吃掉你\n我已经饱了\n胃口撑的满满\n可是……\n为什么心里还有欲望\n想要吃掉你\n春风有些安静\n请感受我的呼吸——\n亲爱的\n在床上，在梦里\n来吧，让我们\n依靠彼此\n不要沉醉这夜\n亲爱的，\n也不要悲恼这黑\n我将你挽进心底\n不管它\n明月圆缺\n窗外迷离\n\n\n做梦\n似假非假，似空非空。\n如不醒来，未觉梦中。\n\n\n去做梦\n夜沸腾得厉害\n在黑暗里熬煮着\n摇晃的城市里\n大地满是水泡\n那惨白的\n早已退避的月光\n是往别处去了\n还是正蜷缩养伤？\n还是睡去吧——\n去梦里吧\n至少梦里，可能\n有一个想象的太阳\n\n\n24小时\n工作、学习、吃饭与睡眠\n这日常的四件事情\n将一天的24小时分割\n一大段、一大段、一小段、一大段\n还有究竟有多少间隙\n我说不清\n我好像做了很多\n在这一天天的24小时\n在这一段段分割的时空里\n但却好像乱了\n心里血液喷薄的红线\n将身躯一圈圈紧绕\n24小时，又24小时\n我看不见它的线头\n也瞧不见它的去处\n只感觉越来越紧\n冒着火色的双眼\n越来难以看清\n24小时够了吗？\n不够再上24小时。\n我在畅想，也在回望\n可悲的是，就这样\n本来连续的生命里\n我活成了不连续的自、己\n\n\n我即将在这午后的阳光下睡去\n我即将在这午后的阳光下睡去\n面对门窗，背靠办公椅\n黄白的光斜着倾洒\n在栏杆处投下斑驳的灰影\n侵袭宿舍的每一个方寸\n也照亮着这容身的方寸\n微闭的眼睛里\n存着远方天空的片段\n连续地播放着\n是白芒白茫的记忆\n一只难以呼唤其名的鸟\n横着从中间飞过\n叫着春天，催我睡去\n\n\n我们这座城市\n阳光混合氧气充斥这座城市\n像世界上的其他一万座城市。\n看的见的光与看不见的虚无和着\n输送给这座城市人口养料。\n百万，千万\n寥寥字眼里埋葬人口下的生活：\n我们精心得像蚂蚁构造这座城市，\n也忙碌得像蚂蚁存在着却好像只是路过。\n我们像单纯地渴望活着一样\n渴望着黑暗甬道的下一个出口——\n可是，我们依然按照路牌的指引\n不自主地拥抱人流，涌入车厢\n毫无畏惧地驶向又一个\n又下一个黑暗的地方。\n前进啊，\n像与车轨一路摩擦前行的列车，\n我们与这座城市碰撞相融，\n直到分不清它是青春\n还是它就是我们。\n\n\n相交的平行线\n我曾经以为，\n两条平行直线不会相交，\n但宇宙就是这样，\n在视线的远方，\n人生的某个拐点，\n我们彼此开始交汇，\n融合。\n这是相对论的咒语——\n它在一直在那里，\n而我们一起在这里。\n\n在欧氏几何中，在两条平行线中做一条直线AB，以直线AB为半径以逆时针方向做圆，然后以直线AB为半径以顺时针方向再做一个圆，从两个圆的交点做垂线CD垂直于直线AB，若CD与AB的角的角度是90度，则说明两条平行线不会相交。\n但欧几里得不敢思考当两条平行线无限长时的情况…..\n于是包括罗素、黎曼在内的科学家假设当两条平行线无限长时，他们会在无穷远处相交。（例如：在地球的球面上，就会发现，相互垂直于赤道的经线会相交于北极点和南极点。）后来，非欧几何和黎曼空间就诞生了，该成果给了爱因斯坦很大的启发．\n\n\n\n2018-06-15\n由来万夫勇，挟此生雄风。笑尽一杯酒，杀人都市中。\n“剑术已成君把去，有蛟龙处斩蛟龙”。\n\n\n探春\n樱红在园绿在野，东君扶柳捧金蝶。\n人间终把寒根断，三月花开好时节。\n\n\n2019-04-05\n自然的规律\n真真伟大的造物\n它从无声息地彰显\n你不由分说地融入\n物理学家用数学丈量它\n而我，\n于生活中感触"
  },
  {
    "objectID": "posts/2023-12-20-random/index.html",
    "href": "posts/2023-12-20-random/index.html",
    "title": "随便聊两句，可能不一定有意义",
    "section": "",
    "text": "近两年文章看的少。\n在看一些我自己觉得做的好的文章（并不一定是高IF文章）时，总觉得我的工作质量有所不如（这已经谦虚了）。\n细细想来，还是不够细心、耐心、专业。\n在当前社会和周遭的环境里，有时候被潜移默化得比较急躁。很多感官上不重要的细节，成了不被关注的。 甚至有些都从来没有想到需要关注。这就是差距。\n应该反思。现实所压迫的，除了一些无可奈何的事情，其他要勇于反抗，哪怕可能失去潜在的名利。\n雷军最近说的好，如果可能有问题/错误的，就一定有问题/错误（大概是这么个意思）。要真诚地面对问题，解决问题。\n幸运的是，我还比较年轻。很多事情做的不好是难免的，学习也是一直在进行的，重点在于追求精进，不要往后看，而要往前行。\n一生很有限，要用好的工作内容回馈自己付出的时间。生活也应该这样。"
  },
  {
    "objectID": "posts/2023-12-25-lazy-loading-data-in-r/index.html",
    "href": "posts/2023-12-25-lazy-loading-data-in-r/index.html",
    "title": "R 针对数据进行惰性加载",
    "section": "",
    "text": "在做 Shiny 开发时，有些数据太大，非常影响加载数据，这里提供了一种 lazyloading 策略，可以值得一试。\n\nX = matrix(1, nrow = 1000, ncol = 1000)\nY = list(X = X)\ne=new.env(parent=emptyenv())\ne$X = X\ne$Y = Y\n\ntools:::makeLazyLoadDB(e, filebase = path.expand(\"~/Downloads/lazytest\"))\nrm(list = ls())\n\nlazyLoad(path.expand(\"~/Downloads/lazytest\"))\n\n参考：https://stackoverflow.com/questions/21583382/r-how-to-lazyload-variables-from-inst-extdata-in-r-package\n\n关于参考内容中还提到 delayedAssign 可以实现类似的结果，但没有尝试。"
  },
  {
    "objectID": "posts/2024-01-17-github-hosts/index.html",
    "href": "posts/2024-01-17-github-hosts/index.html",
    "title": "解决 GitHub 代码拉取和推送问题",
    "section": "",
    "text": "国内 Linux 服务器有时候存在 GitHub 仓库无法 push 或 pull 的问题，这里主要记录下修改 hosts 来解决该问题（注意不是 100% 有效）\n编辑 /etc/hosts 文件：\nsudo vim /etc/hosts\n追加下面的内容：\n140.82.112.3 github.com\n151.101.1.194 github.global.ssl.fastly.net  \n151.101.65.194 github.global.ssl.fastly.net  \n151.101.129.194 github.global.ssl.fastly.net  \n151.101.193.194 github.global.ssl.fastly.net \n这里的 IP 地址可能会变，如果不行可以通过 https://sites.ipaddress.com/ 查一下。\n重启下网络：\nsudo /etc/init.d/network restart\n\n如果是其他平台，操作类似。\n\n\n参考：https://juejin.cn/post/7135346231593599012"
  },
  {
    "objectID": "posts/2024-01-22-immune/index.html",
    "href": "posts/2024-01-22-immune/index.html",
    "title": "免疫",
    "section": "",
    "text": "细胞因子的分类：\n\n白细胞介素\n干扰素\n肿瘤坏死因子\n集落刺激因子\n生长因子\n趋化性细胞因子\n\n生物学活性：\n\n介导固有免疫：抗细菌和病毒作用\n介导和调节特异性免疫应答\n刺激造血\n促进血管生成\n诱导凋亡"
  },
  {
    "objectID": "posts/2024-01-22-immune/index.html#细胞因子",
    "href": "posts/2024-01-22-immune/index.html#细胞因子",
    "title": "免疫",
    "section": "",
    "text": "细胞因子的分类：\n\n白细胞介素\n干扰素\n肿瘤坏死因子\n集落刺激因子\n生长因子\n趋化性细胞因子\n\n生物学活性：\n\n介导固有免疫：抗细菌和病毒作用\n介导和调节特异性免疫应答\n刺激造血\n促进血管生成\n诱导凋亡"
  },
  {
    "objectID": "posts/2024-02-20-telescope-installation/index.html",
    "href": "posts/2024-02-20-telescope-installation/index.html",
    "title": "telescope installation",
    "section": "",
    "text": "注意这个帖子的有效性可能在未来某一天丧失。\n\n简单记录下 telescope 的安装，因为 https://github.com/mlbendall/telescope 提过的官方说明直接装后测试是有问题的。\n一步步安装和debug过程：\nmamba create -n telescope -c bioconda -c conda-forge telescope python=3.8\nconda activate telescope\nmamba install numpy=1.19\nmamba install -c conda-forge -c bioconda 'pysam&lt;0.17'\n\n现在回头看可以缩减为一句命令：mamba create -n telescope -c bioconda -c conda-forge telescope python=3.8 numpy=1.19 pysam&lt;0.17。\n\n测试：\n(telescope) zhou2 22:12:47 ~/test \n$eval $(telescope test)\n2024-02-18 22:13:03 INFO     \nVersion:                          1.0.3\nInput Options\n    samfile:                      /home/zhou2/miniconda3/envs/telescope/lib/python3.8/site-packages/telescope/data/alignment.bam\n    gtffile:                      /home/zhou2/miniconda3/envs/telescope/lib/python3.8/site-packages/telescope/data/annotation.gtf\n    attribute:                    locus\n    no_feature_key:               __no_feature\n    ncpu:                         1\n    tempdir:                      None\nReporting Options\n    quiet:                        False\n    debug:                        False\n    logfile:                      &lt;stderr&gt;\n    outdir:                       .\n    exp_tag:                      telescope\n    updated_sam:                  False\nRun Modes\n    reassign_mode:                exclude\n    conf_prob:                    0.9\n    overlap_mode:                 threshold\n    overlap_threshold:            0.2\n    annotation_class:             intervaltree\nModel Parameters\n    pi_prior:                     0\n    theta_prior:                  200000\n    em_epsilon:                   1e-07\n    max_iter:                     100\n    use_likelihood:               False\n    skip_em:                      False\n (from run in telescope_assign.py:197)\n[E::idx_find_and_load] Could not retrieve index file for '/home/zhou2/miniconda3/envs/telescope/lib/python3.8/site-packages/telescope/data/alignment.bam'\n2024-02-18 22:13:03 INFO     Loading annotation...                                        (from run in telescope_assign.py:205)\n2024-02-18 22:13:03 INFO     Loaded annotation in 0 minutes and 0 secs                    (from run in telescope_assign.py:208)\n2024-02-18 22:13:03 INFO     Loaded 99 features.                                          (from run in telescope_assign.py:209)\n2024-02-18 22:13:03 INFO     Loading alignments...                                        (from run in telescope_assign.py:214)\n[E::idx_find_and_load] Could not retrieve index file for '/home/zhou2/miniconda3/envs/telescope/lib/python3.8/site-packages/telescope/data/alignment.bam'\n2024-02-18 22:13:05 INFO     Loaded alignment in 0 minutes and 1 secs                     (from run in telescope_assign.py:217)\n2024-02-18 22:13:05 INFO     Alignment Summary:                                           (from print_summary in model.py:519)\n2024-02-18 22:13:05 INFO         1000 total fragments.                                    (from print_summary in model.py:520)\n2024-02-18 22:13:05 INFO             1000 mapped as pairs.                                (from print_summary in model.py:521)\n2024-02-18 22:13:05 INFO             0 mapped as mixed.                                   (from print_summary in model.py:522)\n2024-02-18 22:13:05 INFO             0 mapped single.                                     (from print_summary in model.py:523)\n2024-02-18 22:13:05 INFO             0 failed to map.                                     (from print_summary in model.py:524)\n2024-02-18 22:13:05 INFO     --                                                           (from print_summary in model.py:525)\n2024-02-18 22:13:05 INFO         1000 fragments mapped to reference; of these             (from print_summary in model.py:526)\n2024-02-18 22:13:05 INFO             0 had one unique alignment.                          (from print_summary in model.py:528)\n2024-02-18 22:13:05 INFO             1000 had multiple alignments.                        (from print_summary in model.py:529)\n2024-02-18 22:13:05 INFO     --                                                           (from print_summary in model.py:530)\n2024-02-18 22:13:05 INFO         1000 fragments overlapped annotation; of these           (from print_summary in model.py:531)\n2024-02-18 22:13:05 INFO             0 map to one locus.                                  (from print_summary in model.py:533)\n2024-02-18 22:13:05 INFO             1000 map to multiple loci.                           (from print_summary in model.py:535)\n2024-02-18 22:13:05 INFO     \n                                                            (from print_summary in model.py:537)\n2024-02-18 22:13:05 INFO     Running Expectation-Maximization...                          (from run in telescope_assign.py:250)\n2024-02-18 22:13:05 INFO     Iteration 1, diff=1.3795                                     (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 2, diff=0.7388                                     (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 3, diff=0.24275                                    (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 4, diff=0.065133                                   (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 5, diff=0.017653                                   (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 6, diff=0.0050039                                  (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 7, diff=0.0014807                                  (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 8, diff=0.00045365                                 (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 9, diff=0.00014275                                 (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 10, diff=4.5844e-05                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 11, diff=1.4953e-05                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 12, diff=4.9364e-06                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 13, diff=1.645e-06                                 (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 14, diff=5.5228e-07                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 15, diff=1.8652e-07                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     Iteration 16, diff=6.3301e-08                                (from em in model.py:709)\n2024-02-18 22:13:05 INFO     EM converged after 16 iterations.                            (from em in model.py:722)\n2024-02-18 22:13:05 INFO     Final log-likelihood: 95252.596293.                          (from em in model.py:723)\n2024-02-18 22:13:05 INFO     EM completed in 0 minutes and 0 secs                         (from run in telescope_assign.py:253)\n2024-02-18 22:13:05 INFO     Generating Report...                                         (from run in telescope_assign.py:256)\n2024-02-18 22:13:05 INFO     telescope assign complete (0 minutes and 1 secs)             (from run in telescope_assign.py:263)"
  },
  {
    "objectID": "posts/2024-02-20-backtogz/index.html",
    "href": "posts/2024-02-20-backtogz/index.html",
    "title": "回广州路上的感想",
    "section": "",
    "text": "年后开车回广州太堵了，打破了国庆离开广州回老家开车的时长。 考虑到不会再在广州待这样的大长假了，应该以后不会再有类似的经历？\n这里记录我感受堵车产生的两个天真想法：\n\n高速的来回车道是分割的，一边堵一边完全不堵，是否可以在特殊节假日（特殊日期），在不同收费站之间封控对向车流，把对向的车道也给堵车的这条车道使用？\n未来自动驾驶普及后，是否可能在特殊节假日高速道路汽车全部由计算中心总控，把私家车组网拼接为同一个公路网络？堵车的本质原因是车多情况下不同人的不协调导致，这种统一的调度应当可以完全解决这个问题？"
  },
  {
    "objectID": "posts/2024-02-12-shiny-plot-dpi/index.html",
    "href": "posts/2024-02-12-shiny-plot-dpi/index.html",
    "title": "shiny plot dpi",
    "section": "",
    "text": "96 is recommended.\n\nhttps://mastering-shiny.org/basic-ui.html\nhttps://github.com/rstudio/shiny/issues/1832"
  },
  {
    "objectID": "posts/2024-02-23-use-singularity/index.html",
    "href": "posts/2024-02-23-use-singularity/index.html",
    "title": "use singularity",
    "section": "",
    "text": "Singularity 日常使用\n文档\nhttps://singularityhub.github.io/\nhttps://cloud.sylabs.io/library"
  },
  {
    "objectID": "posts/2024-02-25-can-i-run-vs-code-server-on-older-linux-distributions/index.html",
    "href": "posts/2024-02-25-can-i-run-vs-code-server-on-older-linux-distributions/index.html",
    "title": "Can I run VS Code Server on older Linux distributions",
    "section": "",
    "text": "VSCode 的不断更新导致最新版本 Remotes 已经在一些 Linux server 上不再支持，这里记录下官方的解决办法，就是将版本降级到 1.85\n\nhttps://code.visualstudio.com/docs/remote/faq#_can-i-run-vs-code-server-on-older-linux-distributions\nhttps://code.visualstudio.com/updates/v1_85\n\nDownloads: Windows: x64 Arm64 | Mac: Universal Intel silicon | Linux: deb rpm tarball Arm snap\n\nDepending on your platform, make sure to disable updates to stay on that version. A good recommendation is to have release 1.85 as a separate installation, set up with Portable Mode（就是独立的软件目录）.\nPortable mode is supported on the ZIP download for Windows, and the TAR.GZ download for Linux, as well as the regular Application download for macOS. See the Download page to find the correct .zip / .tar.gz file for your platform."
  },
  {
    "objectID": "posts/2024-02-28-snakemake-basics/index.html",
    "href": "posts/2024-02-28-snakemake-basics/index.html",
    "title": "snakemake 教程精要",
    "section": "",
    "text": "这里把 Snakemake 教程 的精要记录了下来。"
  },
  {
    "objectID": "posts/2024-02-28-snakemake-basics/index.html#基础",
    "href": "posts/2024-02-28-snakemake-basics/index.html#基础",
    "title": "snakemake 教程精要",
    "section": "基础",
    "text": "基础\nSnakefile:\n\n# Remember that Snakemake works backwards from requested output, and not from available input.\n#\n# Check: snakemake -np mapped_reads/A.bam\n# Run: snakemake --cores 1 mapped_reads/A.bam\n# snakemake -np mapped_reads/{A,B}.bam\n# snakemake -np sorted_reads/B.bam\n# snakemake --dag sorted_reads/{A,B}.bam.bai | dot -Tsvg &gt; dag.svg\n# snakemake --dag calls/all.vcf | dot -Tsvg &gt; dag.svg\n\nSAMPLES = [\"A\", \"B\"]\n\n# target rule\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        \"mapped_reads/{sample}.bam\"\n    shell:\n        # Since the rule has multiple input files, Snakemake will concatenate them, separated by a whitespace\n        \"bwa mem {input} | samtools view -Sb - &gt; {output}\"\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam\"\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        # expand: a helper function for collecting input files\n        # 注意这里 SAMPLES 必须要有定义，而这里的 sample 也是提取其元素，与上面\n        # rule 中 sample wildcards 不同\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=SAMPLES),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=SAMPLES)\n    output:\n        \"calls/all.vcf\"\n    shell:\n        \"bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv - &gt; {output}\"\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    # It is best practice to use the script directive whenever an inline code block would have more than a few lines of code\n    script:\n        \"scripts/plot-quals.py\"\nscripts/plot-quals.py:\n# Similar in R: snakemake@input[[1]], snakemake@input[[\"myfile\"]]\n\n# all properties of the rule like input, output, wildcards, etc.\n# are available as attributes of a global snakemake object.\nimport matplotlib\nmatplotlib.use(\"Agg\")\nimport matplotlib.pyplot as plt\nfrom pysam import VariantFile\n\nquals = [record.qual for record in VariantFile(snakemake.input[0])]\nplt.hist(quals)\n\nplt.savefig(snakemake.output[0])"
  },
  {
    "objectID": "posts/2024-02-28-snakemake-basics/index.html#advanced",
    "href": "posts/2024-02-28-snakemake-basics/index.html#advanced",
    "title": "snakemake 教程精要",
    "section": "Advanced",
    "text": "Advanced\nSnakefile:\n\n# Remember that Snakemake works backwards from requested output, and not from available input.\n#\n# snakemake --cores 10 # The maximum cores used\n#                      # If --cores is given without a number, all available cores are used.\n#\n# With the flag --forceall you can enforce a complete re-execution of the workflow\n#\n# snakemake -n --forcerun $(snakemake --list-input-changes)\n# snakemake -n --forcerun bcftools_call\n# snakemake -np --summary\n\nconfigfile: \"config.yaml\"\n\n# target rule\nrule all:\n    input:\n        \"plots/quals.svg\"\n\n# https://snakemake.readthedocs.io/en/stable/tutorial/advanced.html#step-3-input-functions\ndef get_bwa_map_input_fastqs(wildcards):\n    return config[\"samples\"][wildcards.sample]\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        get_bwa_map_input_fastqs\n    output:\n        # mark output files as temporary. \n        # Snakemake will delete the marked files for you, \n        # once all the consuming jobs (that need it as input) have been executed\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    # It is best practice to store all log files in a subdirectory logs/, \n    # prefixed by the rule or tool name.\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    # Snakemake provides a resources directive that can be used to specify arbitrary resources,\n    # e.g., memory usage or auxiliary computing devices like GPUs. \n    threads: 8\n    shell:\n        # Since the rule has multiple input files, Snakemake will concatenate them, separated by a whitespace\n        # both bwa and samtools and pipe it into the file referred to by {log}\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        # protect the final BAM file from accidental deletion or modification\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        # expand: a helper function for collecting input files\n        # 注意这里 SAMPLES 必须要有定义，而这里的 sample 也是提取其元素，与上面\n        # rule 中 sample wildcards 不同\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=config[\"samples\"]),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=config[\"samples\"])\n    output:\n        \"calls/all.vcf\"\n    log:\n        \"logs/bcftools_call/all.log\"\n    params:\n        rate=config[\"prior_mutation_rate\"]\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv -P {params.rate} - &gt; {output}) 2&gt; {log}\"\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    # It is best practice to use the script directive whenever an inline code block would have more than a few lines of code\n    script:\n        \"scripts/plot-quals.py\"\n\nconfig.yaml:\nsamples:\n    A: data/samples/A.fastq\n    B: data/samples/B.fastq\n    C: data/samples/C.fastq\n\nprior_mutation_rate: 0.001"
  },
  {
    "objectID": "posts/2024-02-28-snakemake-basics/index.html#custom",
    "href": "posts/2024-02-28-snakemake-basics/index.html#custom",
    "title": "snakemake 教程精要",
    "section": "Custom",
    "text": "Custom\n上面手动设定了要处理的样本列表，能不能通过目录来处理呢？即不管有多少文件，目录下所有符合要求的样本列表都进行处理。\nconfig.yaml:\nsamples: data/samples\nprior_mutation_rate: 0.001\n在探索后我在之前的基础上给出了以下方案：\n利用 glob 进行文件列表的解析，然后处理相关的 pattern 把样本列表提取出来，并以此 更新涉及到样本列表指定的部分：\nbam=expand(\"sorted_reads/{sample}.bam\", sample=samples),\nbai=expand(\"sorted_reads/{sample}.bam.bai\", sample=samples)\n全部内容如下：\nimport glob\n\nconfigfile: \"config.yaml\"\n\ninput_path = config[\"samples\"]\ninput_files = glob.glob(input_path + \"/*.fastq\")\n\nsamples = set()\nfor f in input_files:\n    name = f.split('/')[-1].split('.')[0]\n    samples.add(name)\n\n#print(samples)\n\n# target rule\nrule all:\n    input:\n        \"plots/quals.svg\"\n\nrule bwa_map:\n    input:\n        \"data/genome.fa\",\n        \"data/samples/{sample}.fastq\"\n    output:\n        # mark output files as temporary. \n        # Snakemake will delete the marked files for you, \n        # once all the consuming jobs (that need it as input) have been executed\n        temp(\"mapped_reads/{sample}.bam\")\n    params:\n        rg=r\"@RG\\tID:{sample}\\tSM:{sample}\"\n    # It is best practice to store all log files in a subdirectory logs/, \n    # prefixed by the rule or tool name.\n    log:\n        \"logs/bwa_mem/{sample}.log\"\n    # Snakemake provides a resources directive that can be used to specify arbitrary resources,\n    # e.g., memory usage or auxiliary computing devices like GPUs. \n    threads: 8\n    shell:\n        # Since the rule has multiple input files, Snakemake will concatenate them, separated by a whitespace\n        # both bwa and samtools and pipe it into the file referred to by {log}\n        \"(bwa mem -R '{params.rg}' -t {threads} {input} | \"\n        \"samtools view -Sb - &gt; {output}) 2&gt; {log}\"\n\nrule samtools_sort:\n    input:\n        \"mapped_reads/{sample}.bam\"\n    output:\n        # protect the final BAM file from accidental deletion or modification\n        protected(\"sorted_reads/{sample}.bam\")\n    shell:\n        \"samtools sort -T sorted_reads/{wildcards.sample} \"\n        \"-O bam {input} &gt; {output}\"\n\nrule samtools_index:\n    input:\n        \"sorted_reads/{sample}.bam\"\n    output:\n        \"sorted_reads/{sample}.bam.bai\"\n    shell:\n        \"samtools index {input}\"\n\nrule bcftools_call:\n    input:\n        fa=\"data/genome.fa\",\n        # expand: a helper function for collecting input files\n        # 注意这里 SAMPLES 必须要有定义，而这里的 sample 也是提取其元素，与上面\n        # rule 中 sample wildcards 不同\n        bam=expand(\"sorted_reads/{sample}.bam\", sample=samples),\n        bai=expand(\"sorted_reads/{sample}.bam.bai\", sample=samples)\n    output:\n        \"calls/all.vcf\"\n    log:\n        \"logs/bcftools_call/all.log\"\n    params:\n        rate=config[\"prior_mutation_rate\"]\n    shell:\n        \"(bcftools mpileup -f {input.fa} {input.bam} | \"\n        \"bcftools call -mv -P {params.rate} - &gt; {output}) 2&gt; {log}\"\n\nrule plot_quals:\n    input:\n        \"calls/all.vcf\"\n    output:\n        \"plots/quals.svg\"\n    # It is best practice to use the script directive whenever an inline code block would have more than a few lines of code\n    script:\n        \"scripts/plot-quals.py\"\n\nQA 中的 How do I run my rule on all files of a certain directory? 提供的下面的办法似乎也能解决该问题：\nIDS, = glob_wildcards(\"thedir/{id}.fastq\")"
  },
  {
    "objectID": "posts/2024-03-20-r-makevars-compiler-setting/index.html",
    "href": "posts/2024-03-20-r-makevars-compiler-setting/index.html",
    "title": "R 设置编译选项",
    "section": "",
    "text": "在 R 中，特别是 CentOS Linux 进行包安装和编译时，需要使用比较新的 C/C++ 编译器，在 ~/.R/Makevars 中作如下设置。\nCC=/opt/rh/devtoolset-9/root/usr/bin/gcc -fPIC\nCXX=/opt/rh/devtoolset-9/root/usr/bin/g++ -std=c++11 -fPIC\nCXX11=/opt/rh/devtoolset-9/root/usr/bin/g++ -std=c++11 -fPIC\nCXX14=/opt/rh/devtoolset-9/root/usr/bin/g++ -std=c++14 -fPIC\nCXX17=/opt/rh/devtoolset-9/root/usr/bin/g++ -std=c++17 -fPIC\n\n一些情况下会加上 -fopenmp 选项。\n\n本文对之前所做记录 CentOS/Redhat R包使用新的gcc编译【更新】 的补全。"
  },
  {
    "objectID": "posts/2024-04-11-type-1-2-errors/index.html",
    "href": "posts/2024-04-11-type-1-2-errors/index.html",
    "title": "第一/二类型错误",
    "section": "",
    "text": "From Kimi\n\n在统计假设检验中，第一类错误和第二类错误是两种常见的错误类型，它们描述了在决策过程中可能出现的不同类型的错误。\n\n第一类错误（Type I Error）\n第一类错误，也称为α错误，发生在我们错误地拒绝了一个真实的零假设（H0）。换句话说，当实际上零假设为真时，我们却得出结论认为它为假。这种情况下，我们认为观察到的效应是显著的，但实际上这种效应并不存在。\n例如，在药物测试中，如果一个新药物实际上没有疗效，但研究结果却错误地表明它有效，这就是犯了第一类错误。这种错误的风险通常与研究的显著性水平（α水平）相关，α水平是研究者事先设定的，用于决定何时拒绝零假设的概率阈值。\n\n\n第二类错误（Type II Error）\n第二类错误，也称为β错误，发生在我们错误地接受了零假设，即当实际上零假设为假时，我们却未能拒绝它。这意味着我们错过了发现真实效应的机会，得出结论认为没有效应，而实际上效应是存在的。\n继续上面药物测试的例子，如果一个新药物实际上是有效的，但研究结果未能显示出其疗效，从而得出结论认为它无效，这就是犯了第二类错误。这种错误的风险与研究的统计效力（power）有关，统计效力是指正确检测到真实效应的概率。\n\n\n错误类型的关系\n第一类错误和第二类错误之间存在权衡关系。降低第一类错误的风险（降低α水平）可能会增加第二类错误的风险（降低统计效力），反之亦然。研究者需要在设计研究时平衡这两种错误的风险，以确保研究结果的准确性和可靠性。\n\n\n其他错误类型\n除了第一类和第二类错误外，还有其他类型的错误，如：\n\n第三类错误：当研究者未能正确地识别和解释数据中的模式时发生的错误。\n第四类错误：在元分析或综合分析中，当研究者错误地将不同研究的结果合并时发生的错误。\n\n在进行统计分析和解释结果时，了解这些错误类型对于避免错误的结论和提高研究质量至关重要。"
  },
  {
    "objectID": "posts/2024-04-15-discordant-and-clipped-reads/index.html",
    "href": "posts/2024-04-15-discordant-and-clipped-reads/index.html",
    "title": "Discordant and clipped reads",
    "section": "",
    "text": "From Kimi, 可靠性自己判断。\n\nDiscordant reads 和 clipped reads 是在基因组测序数据分析中用于检测基因组结构变异（Structural Variation, SV）的两种重要类型的读长（reads）。\nDiscordant reads 指的是那些配对末端读长（paired-end reads）中，一个读长能够正常比对到参考基因组的预期位置，而另一个读长比对到了参考基因组中的非预期位置。这种不一致性通常表明在基因组中存在某种结构变异，如易位（translocation）或倒位（inversion）。例如，当两条同源或非同源染色体之间发生易位时，配对的读长可能会显示一个读长位于易位片段的原始位置，而另一个读长则位于易位后的新位置。\nClipped reads 又称为soft-clipped reads，是指单个读长中有一部分序列能够比对到参考基因组的一个位置，而剩余的序列则比对到了另一个不同的位置。这种读长表明在比对的两个位置之间可能发生了某种结构变异，如插入（insertion）。Clipped reads中的缺口（gap）或不匹配部分可以揭示插入序列的边界，从而帮助确定结构变异的精确位置。\n这两种读长类型在结构变异检测中非常关键，因为它们提供了直接的证据来揭示基因组中可能发生的重排、插入或缺失等变异。通过分析这些读长，研究人员能够更准确地识别和表征基因组结构的变化，这对于理解基因功能、疾病机理以及进化过程都有重要意义。\nDiscordant reads 在双端测序比对中是通过比较配对末端读数（paired-end reads）的比对结果来判断的。在使用 BWA-MEM 算法进行比对时，discordant reads 的判断过程可以概括为以下几个步骤：\n\n比对读数到参考基因组：BWA-MEM 首先会对每个读数（reads）进行比对，尝试找到最佳的比对位置。这一过程包括使用种子和扩展（seeding and extending）策略来处理较长的读数，以及使用基于Smith-Waterman算法的局部比对来处理可能的重复区域或结构变异区域。\n评估插入大小：对于双端测序数据，BWA-MEM 会估计插入大小（insert size）的均值和方差，并基于此信息评估每个读数对的两个末端是否在预期的插入大小范围内。\n识别不一致的配对：在比对过程中，如果一个读数的两个末端比对到了参考基因组中相距甚远的位置，这可能表明存在结构变异，如插入、缺失或易位。BWA-MEM 会识别这些不一致的配对，即discordant reads。\n使用动态规划优化比对：BWA-MEM 使用动态规划算法来优化比对结果，自动选择局部比对或端到端比对。如果一个读数的末端比对到了一个真实的变异位点，并且比对质量足够好，BWA-MEM 可能会接受这样的局部比对，而不是强制进行端到端比对，这样可以减少参考偏差并避免引入过多的不匹配和间隙。\n输出结果：最终，BWA-MEM 会输出比对结果，包括所有成功的比对以及识别出的 discordant reads。这些 discordant reads 可以用于后续的结构变异分析。\n\n通过上述步骤，BWA-MEM 能够有效地处理双端测序数据，并识别出可能指示结构变异的 discordant reads。这种方法对于发现和研究基因组中的结构变异非常重要，有助于理解基因组多样性和疾病相关变异。"
  }
]