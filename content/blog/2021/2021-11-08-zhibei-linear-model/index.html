---
title: 「转载」线性模型
author: 王诗翔
date: '2021-11-08'
slug: zhibei-linear-model
categories:
  - Blog
tags:
  - stats
description: 深入了解和学习
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<!-- Links -->
<p>本文主要是转载<a href="https://bookdown.org/yufree/sciguide/data.html#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B">《指北》线性模型</a>一节的内容，有删减。</p>
<hr />
<p>在线性模型中，如果我们将两个共相关的变量放到同一个线性回归模型之中，那么这两个变量的系数估计的标准误都会扩大。下面我们展示下这个干扰过程：</p>
<pre class="r"><code>set.seed(42)
a &lt;- runif(100,min=0,max=10)+rnorm(100)
b &lt;- a*1.2+rnorm(100)
c &lt;- b*1.2+rnorm(100)
y &lt;- a+b+c+rnorm(100)</code></pre>
<p>上面我们由两个变量生成了一个新变量，然而这两个变量是相关的，此时我们进行回归分析：</p>
<pre class="r"><code>summary(lm(y~a))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.1929 -1.9935 -0.2595  1.9301  5.3011 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.14295    0.44101  -0.324    0.747    
## a            3.65619    0.07162  51.048   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.339 on 98 degrees of freedom
## Multiple R-squared:  0.9638, Adjusted R-squared:  0.9634 
## F-statistic:  2606 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p>这里对y和a关系的评估是比较准确的<code>1 + 1.2 + 1.2*1.2 = 3.64</code>。</p>
</blockquote>
<pre class="r"><code>summary(lm(y~a+b+c))</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ a + b + c)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.10681 -0.69646 -0.06907  0.66959  2.31841 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.03088    0.18341  -0.168    0.867    
## a            1.20491    0.12910   9.333 4.03e-15 ***
## b            0.69905    0.16038   4.359 3.28e-05 ***
## c            1.11364    0.10523  10.583  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9716 on 96 degrees of freedom
## Multiple R-squared:  0.9939, Adjusted R-squared:  0.9937 
## F-statistic:  5194 on 3 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>这里我们可以看到，如果只把第一个变量放入模型，那么斜率理论上应该是3.6。<strong>但此时我们把跟这个变量相关的另外两个变量放进去后回归模型对三个相关变量的系数估计都不再准确（应该都是1），标准误也扩大了</strong>。这个现象对理解线性模型非常重要，如果我们任意在线性模型里加入变量，变量间的随机相关会导致整个模型的回归系数估计性能都下降。因此<strong>线性模型不能随意加减自变量，最好事先考察自变量间的关系</strong>。</p>
<blockquote>
<p>在处理多变量模型时尽量处理好自变量本身的关系，再进行后续分析。</p>
</blockquote>
<p>当线性模型的自变量只有一项时，其实考察的就是自变量与响应变量间的相关性。当自变量为多项时，也就是多元线性回归，考察的是你自己定义的“自变量”与“协变量”还有响应变量的关系。<strong>如果自变量间不能互相独立，那么最好将独立的部分提取出来作为新的变量，这种发现潜在变量的过程归属于因子分析，可以用来降维</strong>。</p>
<p>自变量本身存在随机性，特别是个体差异，这种随机性可能影响线性模型自变量的系数或斜率，也可能影响线性模型的截距，甚至可能同时影响，此时<strong>考虑了自变量的随机性的模型就是线性混合模型</strong>。线性混合模型其实已经是层级模型了，自变量的随机性来源于共同的分布。如果自变量间存在层级，例如有些变量会直接影响其他变量，那么此时线性模型就成了决策/回归树模型的特例了。如果层级关系错综复杂，那不依赖结构方程模型是没办法搞清楚各参数影响的。然而模型越复杂，对数据的假设就越多，对样本量的要求也就越高。同时，自变量或因变量有些时候也要事先进行连续性转换，这就给出了logistics回归、生存分析等特殊的回归模型。<strong>科研模型如果是依赖控制实验的，那么会在设计阶段随机化绝大部分变量</strong>，数据处理方面到线性混合模型就已经很少见了。</p>
<p>但对于观测数据，线性混合模型只是起点，对于侧重观察数据的社会科学研究，样本量与效应大小是结论可靠性的关键，精细的模型无法消除太多的个体差异。这种复杂关系走到极端就是网络分析了，网络分析适合用来研究多样本或特性间的关系，这类关系通常用互相连接的节点来表示，在可视化中节点一般指代一个样本或特性，连线则代表了样本间或特性间的关系。说白了<strong>网络分析是另一层意义上的因子分析，起一个降维作用，只是降维方式不是简单的线性组合而是引入了图论的一些统计量</strong>。</p>
<p>高维数据是线性模型的一大挑战，当维度升高后，变量间要么可能因为变异来源相似而共相关，要么干脆就是随机共相关。在某些场景下，高维数据可能都没有目标变量，需要先通过探索性数据分析找出样本或变量间的组织结构。这种场景下应通过变量选择过程来保留独立且与目标变量有潜在关系的变量。也就是说，<strong>变量选择的出发点是对数据的理解，优先考虑相关变量而非简单套用统计分析流程。当然，统计方法上也有变量选择的套路，评判标准可能是信息熵或模型稳健度的一些统计量，可以借助这些过程来简化模型或者说降维</strong>。</p>
<p>回归或模型拟合都存在过拟合的风险，所谓过拟合，就是模型对于用来构建模型的数据表现良好，但在新数据的预测性上却不足的情况。与过拟合对应的是欠拟合，此时拟合出的模型连在构建模型的数据验证上表现都不好。这里的表现可以用模型评价的一些指标，其实跟上面进行变量选择的指标是一样的，<strong>好的模型应该能捕捉到数据背后真实的关系，也因此在训练数据与新数据上表现一致</strong>。</p>
<p>在技术层面上，调参过程有两种基本应对方法，<strong>第一种是重采样技术，第二种是正则化</strong>，两种方法可以组合使用。重采样技术指的是通过对训练集反复采样多次建模来调参的过程。常见的重采样技术有留一法，交叉检验与bootstrap。留一法在每次建模留一个数据点作为验证集，重复n次，得到一个CV值作为对错误率的估计。交叉检验将训练集分为多份，每次建模用一份检验，用其他份建模。bootstrap更可看作一种思想，在训练集里有放回的重采样等长的数据形成新的数据集并计算相关参数，重复多次得到对参数的估计，计算标准误。在这些重采样技术中，因为进行的多次建模，也有多次评价，最佳的模型就是多次评价中在验证集上表现最好的那一组。</p>
<p>正则化则是在模型构建过程中在模型上对参数的效应进行人为减弱，用来降低过拟合风险。具体到线性模型上，就是在模型训练的目标上由单纯最小化均方误改为最小化均方误加上一个对包含模型参数线性组合的惩罚项，这样拟合后的模型参数对自变量的影响就会减弱，更容易影响不显著，如果自变量过拟合的话就会被这个正则化过程削弱。<strong>当惩罚项为模型参数的二次组合时，这种回归就是岭回归；当惩罚项为模型参数的一次绝对值组合时，这种回归就是lasso；当惩罚项为一次与二次的组合时，这种回归就是弹性网络回归</strong>。实践上正则化过程对于降低过拟合经常有神奇效果，同时正则化也可作为变量选择的手段，虽然岭回归无法将系数惩罚为0，但<strong>lasso可以，这样在参数收缩过程中也就同时实现了变量选择</strong>。</p>
<p>对于科研数据的线性回归，还有两个常见问题，一个是截断问题，另一个是缺失值处理。截断问题一般是采样精度或技术手段决定的，在数值的高位或低位无法采集高质量数据，此时可以借助截断回归等统计学方法来弥补。另一种思路则是在断点前后构建不同的模型，这样分别应对不同质量的数据。对于数据缺失值的问题，统计学上也提供了很多用来删除或填充缺失值的方法，填充数据不应影响统计推断，越是接近的样本，就越是可以用来填充缺失值，当然这个思路反着用就是个性化推荐系统模型的构建了。</p>
<p>线性模型如果跟线性模型组合在一起，相当于在同一个模型里加了两个共相关自变量，很可能导致组合后的模型还不如之前的模型效果好。常见的模型组合策略基本假设就是不同模型提取了数据中不同角度的信息，线性模型捕捉的是自变量与响应间加性关系、决策树模型捕捉的是自变量与响应间的层级结构、多项式模型捕捉的是自变量与响应间的非线性高阶关系、神经网络捕捉的是不同核函数假设下自变量与响应间的复杂关系等，甚至深度学习可以看做用一个层次数不断叠加的巨大神经网络来捕捉数据中可能存在的模式。有时候神经网络跟其他模型组合时表现反而下降，原因就在于神经网络已经捕捉到了其他模型的模式，加进去其实是降低了预测模型的稳健度。因此，如果要进行模型组合，一定要对待组合的模型有原理上的理解，否则很可能适得其反。</p>
<p>模型组合策略其实已经体现在很多机器学习算法之中了，特别是随机森林算法里就体现了小模型组合这类思想。实践中进行模型组合是可以体现在不同步骤里的，例如我可以把训练集切成若干份，每一份用一种模型训练，预测的时候是针对不同模型预测结果进行加权综合。当然也可以在训练集上训练所有的模型，然后对结果进行综合，甚至还可以事先利用相关性或聚类把变量分组，然后不同组变量用不同模型预测，这就类似广义线性模型的思路了。具体选择哪种组合策略也可以去寻优，理论上组合策略是无穷的且很难说哪种策略是通用且最好的，最好的组合策略一定是最能挖掘数据信息的那一种。</p>
<p>模型组合策略也可以用来进行解释性研究。最简单的例子就是在线性模型里加上一个二次项发现拟合效果变好了，那么可能暗示数据内在结构就是一个多项式模型。另一种解释方法则是对不同模型的预测结果的残差分布进行研究，看看具体哪一种模型加入后残差分布更接近噪音的正态分布，这样可以去推测数据中起主要作用的模型是哪一种，进而通过模型假设反推数据中存在的主要模式或结构。<strong>这种探索性分析一定要做好做足而不是简单去套数据分析模版，套模版是很难发现新模式的</strong>。</p>
