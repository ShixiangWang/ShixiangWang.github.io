---
title: 「转载」可重复性危机
author: Yufree
date: '2021-09-29'
slug: reproducibility-issue
categories:
  - Blog
tags:
  - Research
description: 一篇非常值得思考和记录的科研文章
---

> 原文来自[《现代科研指北》](https://github.com/yufree/sciguide)第3.3节。

可重复性危机是当前科研领域里最大的问题，如果结论不可被重复验证，那么科学性就无从谈起。这里我们先讨论科研里通用假设检验的问题，然后讨论下规律性，最后介绍应对这个危机的可重复性研究与开放科学趋势。

### 3.3.1 零假设显著性检验（NHST）

零假设显著性检验（NHST）则是可重复性危机的核心。NHST 更常见的形式是 p 值，也就是在零假设成立的条件下某事件发生的概率。打个比方，我们从一个混合了黑白两种颜色小球的口袋里有放回的取一个小球三次，结果都是白球。这里我们设定零假设为黑球白球各一半，那么发生三次白球的概率为12.5%，这个不算极端。但是，如果有放回取了十次，结果还是都是白球，这情况发生概率大概为千分之一，这就比较极端了。在此基础上，我们有理由认为零假设不成立，而此时就需要一个阈值来帮助我们判断是否成立，目前学术界会认为5%或0.05的概率可以作为显著性与否的阈值。科研中我们会去计算零假设下出现当前实验结果的概率，也就是p值，如果低于阈值就可以认为是极端事件就拒绝零假设而高于阈值则认为零假设下可能发生。

当然，我们现在科研用的p值还会考虑零假设之外的备择假设，如果拒绝了零假设就转而接受备择假设。不过一旦引入备择假设就需要讨论错误，这里我们把决策出的结果分为阴性与阳性，而事实分为真假。零假设为真但接受了备择假设的情况，这就是假阳性或者第一类错误；零假设为假但没拒绝零假设就是假阴性或者第二类错误。这里我们可以看到第一类错误与前面设定的决策阈值密切相关，如果设定在5%或者0.05，那么我们就有5%的可能性做出了错误判断。第二类错误则与统计功效也就是真阴性的概率有关，通常会设定在80%，如果功效过低，例如10%，那么犯第二类错误的概率就很高。举例来说，我脚43码的但我不知道，这时去买鞋别人问我脚尺码我说44码的其实是错了，但不影响脚能穿进去，此时尺码的区别功效就不足。但如果我穿久了就会发现确实是大了，此时相当于我通过多次实验或采样提高了统计功效，但可能这个差别虽然明显但也不影响穿。通常NHST关心第一类错误，但设计实验会考虑第二类错误，通过提高样本量来提高统计功效。

p 值有多流行呢？根据 Jeff Leek 的[估计](https://docs.google.com/presentation/d/1hzdSDaPPSE9xUYZHhOVfQIRPPdwe0A9SdE7QDsK3bOA/edit#slide=id.g255a5ace66_3_796)，如果把 p 值当成一篇文献，那么其被引次数已经超过 300 万次了，当之无愧的史上被引次数之王，甩[第二名](http://www.nature.com/news/the-top-100-papers-1.16224)一个数量级。原因其实很简单，p 值已经渗透到几乎所有学科的研究中了，特别是实验学科。可想而知，如果产生 p 值的 NHST 出了问题其影响力有多大。下面谈下 NHST 具体的问题：

如果一个假设对另一个假设来说很稀少，NHST 会在很低的条件概率下拒绝掉，然后那些稀少的事情在 NHST 里就成了无法被检验的事情。这个例子最早是 Cohen [提出](http://ist-socrates.berkeley.edu/~maccoun/PP279_Cohen1.pdf)用来说明人们在使用 NHST 时的问题。零假设是某人是中国人，备择假设是非中国人。我们知道张三是人大代表的概率大概是百万分之二，这是个事实。不过这个事实在零假设里很难发生，备择假设里也无法发生。零假设我们拒绝了某人是中国人，那么根据 NHST，他不是中国人。但问题是人大代表一定要是中国人，此时就会出现事实跟NHST矛盾的情况。在此类问题里，NHST 永远无法认定稀有事件，也就是功效永远不足，并会给出错误答案。

这个问题本质上是多数人在使用 p 值时搞混了条件概率，拿上面人大代表的例子来说，我们的假设 H0 在面对张三这个数据 D 时给出了拒绝 p(H0|D)=0p(H0|D)=0，这个决定是构建在假设 H0 成立时出现 D 的概率太低（即 p(D|H0)p(D|H0)）之上，也就是说 NHST 下，我们默认下面的概率是成立的：

p(D|H0)=p(H0|D)p(D|H0)=p(H0|D)如果你修过任何基础的统计学课程都会知道这两个概率之间差了一个贝叶斯公式。通过使用贝叶斯定理，在新数据出现后原有概率是要被更新而不是直接拒绝掉的。p 值给的是前者，要想知道随机生成的概率，需要知道零假设为真的概率。通俗点说就是 NHST 属于革命派，不认可就打倒你；贝叶斯属于改良派，用新的证据更新原有理论。这个问题的本质就是把假设下的事实与事实下的假设搞混导致的，这是 NHST 的一个致命问题，然而致命问题可不止这一个。

过去的一百年，测量方法的精度是在不断提高的，而精度其实又会影响研究结果，很不幸，也是通过 NHST 来进行的。其实 NHST 在实验物理学里用的还是好好的，例如我去检测一个物理量，只有数据出现在其理论预测下数值四五个标准差以外才会对理论产生实质作用。此时，测量精度越高，由于测量误差导致的对原有理论的冲击就会越少，因为物理学的预测性要比化学生物等学科要好不少且此时 NHST 检测的原有理论是比较真实的。但在其他学科，特别是心理学跟医学的控制实验里，在实验开始前你几乎就可以确定零假设是不成立的，要不然你也没必要分组，此时你去搞 NHST ，几乎一定可以找到差异，此时测量精度如果不断上升，那么你会识别到一系列差异，但这些差异的效果是无法体现在p值里的，p值可能非常小，但效应却属于明显但很微弱，这样的结果也许可以发表，但对实际问题的解决几乎没有贡献。更极端的情况是如果你加大了样本量来提高统计功效，你总是能发现差异的，也就是你的零假设里原有学科理论为真也是会被方法学进步给推翻的。总结下就是 Meehl 在60年代就提出的[悖论](https://philpapers.org/rec/MEETIP)：方法学的进步与增大样本数对于相对硬（理论根基深厚）的学科证伪是正面的，但对相对软（理论比较模糊）的学科则是弱化。方法学悖论的根基其实是应用学科与基础学科的矛盾，基础学科用 NHST 检验观察事实中的理论，但应用学科用 NHST 来检验的是实验设计预测下的事实，此时实验设计的那个假设与 NHST 的零假设并不对应，而 NHST 先天弱化零假设的问题就凸显了。

事实上，p 值正在成为测量投资与努力而不是事实的标准，给定差异，我们总能找到足够的样本来发现这个差异（这也就是前面说的功效分析）。也就是说，NHST 有时候功效不足测不到差异，有时候又一定会能测出差异，但科学事实并不会因为你使用了 NHST 而发生变化，特别是有意义的变化。而作为标准的 p 值其实在被样本数决定同时又综合了测定效果强度与不确定性，这样的一个标准其实有点多余，你完全可以用描述性统计与置信区间来分别表示效果强度与不确定性。同时，p 值也并不能增加新知识，考虑一个多元线性模型，我们只能在多元模型里得到参数，也就是有限检验，不能发现未知参数，但科学就是寻找未知；变量间的关系在数值改变后如何考察，正负关系如何预测，预测性也就无法实现。那么此时还有必要使用 NHST 吗？

20 世纪的技术有了意义深远的进步，但更现实的问题是，科研里低垂果实已经没有了，学科从分立走向交叉，服务社会职能的出现要求科学家回答的不再是科学问题而是现实问题，或者说，科学地回答现实问题。但现实问题非常复杂，科学家要想排除影响，大都采用控制实验与随机化来验证观察研究中的事实。注意，这里的事实不再是理论假设，而是一个现象，如果本来就观察到了差异，用 NHST 根本就不会让我们知道更多的事实，我们可以用无数独立手段证明这个事实的存在然后整合进学科知识体系，但并不能产生更多的思考，理论的预测效能在 NHST 里实际是体现不出来的。而预测效果不显著在 NHST 里还不能说明效果不存在，到这里 NHST 基本就成了鸡肋。

p 值还被一些人认为可以代表结果的可重复性，但重复性是统计功效的函数，跟 p 值无关，p 值不能传达真实与否的信息。统计功效很重要，跟样本数关系大当样本数增大时，空检验总会被拒绝，因此当零假设为感兴趣的理论时，样本数与准确性会提高理论强度，但零假设不存在时，样本数与准确性提高只会弱化理论。此外，p 值控制只考虑假阳性而不是假阴性，所以对假阴性有要求的实验也要慎重使用 p 值，但遗憾的是对很多研究人员而言搞清楚NHST已经不容易了，再去理解这里面的问题会被认为过于麻烦。

那么我们的控制实验里对无关因素的平衡与随机如何呢？显然，平衡掉不随机的部分需要你事先知道这部分是什么，很遗憾，目前科研特别是基于观察的研究并不能事先知道，有时候就是想发现这些不知道自己不知道的东西。这种情况下基于 p 值或零假设的假设检验其实是不应该用的，打个比方，你发现观测数据中 A 基因与甲疾病相关，但究竟是不是 A 基因引发甲疾病还是需要用控制变量来验证的，很有可能 A 基因与甲疾病同样被 B 基因调控，但你根本就没测 B 基因，所以研究本身就是不完整的。

那么通过组学技术等先进分析手段把知道的不知道的一起去测不就完整了吗？也不是，当你测量数量增加时，假设检验的个数也增加了，此时你的 p 值阈值如果是 0.05，那么 10000 个测量变量中会有 500 个即使随机测定都会出现差异的基因。去年有人建议把 p 值阈值设到 0.005，但 p 值这个问题，重要的不是把 0.05 降到 0.005，通用阈值这个想法太偷懒，应该让研究人员充分理解 p 值实际意义与使用方法，毕竟在有些研究领域控制错误发现率后阈值实际比 0.005 [低得多](http://www.nature.com/news/one-size-fits-all-threshold-for-p-values-under-fire-1.22625)，改变阈值只是把需要核实的数量减少了，虽然这也有一定意义。举个例子，10000 个基因中有一个是真实的，你测定后按照 0.05 发现了 501 个，按照 0.005 发现了 51 个，也就是说需要验证的数量减少了。但真实研究中，你会遇到 0.05 发现了 501 个但 0.005 只发现了 50 个的情况，真实差异由于效应量或造成的差异量不够大而被你的决策方法给漏掉了。甚至也会出现 0.05 发现了 480 个而 0.005 只发现了 48 个的情况。也就是说，当你观察的问题效应不大时，p 值有可能不管怎么调整都无法发现。这个锅不在 p 值，在于你要研究的效应效应太低而你用了不恰当的研究方法与假设来检验这个现象。这类效应大小问题就是 type M 型错误，这里M代表数量级，只要你假设检验的效应总是介于有无之间，这个问题就很难规避。

同时，当你进行条件控制时，其实又掉到了高维诅咒的坑里。打比方我做了一组实验，最后发现某种药在 A 条件 B 参数面对 C 人群中 D 年龄分组里是有效的。那么问题来了，假设 ABCD 全是互斥的二元变量，那么我这个结果实际上是做了24=1624=16次对比得到了一次显著性结果。然而，如果我们采用 p 值，那么 16 次随机假设检验里出现一次 p 值小于 0.05 的概率是1−(1−0.05)16=0.561−(1−0.05)16=0.56，也就是说这个结果在完全随机状态下也有高于一半的概率可能会发生。那么这个结论是否可靠呢？其实跟 p 值没关系，最终是跟样本量挂钩，如果你的满足条件的目标样本很大，那这个结果很可能就是对的。相反，如果这个结果是来自于小样本，虽然根据多元模型是显著的，但具体到这个条件下其实就几个样本，此时结果就不能算靠谱。

探索性数据分析通常会面对这个无穷假设困境，当你不断引入协变量后，维度的增加导致样本实际是稀疏欠拟合的，最后看到现象可能就是假象。发现的价值在这里也不依赖 p 值，依赖效果大小与参数，进一步依赖样本量。不过条件控制中也要考虑因果关系，否则单纯增加特征值可能并不好提高模型的性能。高维诅咒实际还连着多重比较的坑，多次比较后 p 值是可以校正后继续用的，不过这里套的坑是无穷假设，也就是说当你比较的次数多时，有效应也看不出来了，这就要求对数据本身的结构有深入了解。当前在错误率控制上也有很多校正方法，当然也是对 p 值的校正，所以批评声从来也没断过。

现在只能相信强结论，也就是说无论你用哪种统计方法去进行检验，这个现象都是客观存在的，不会因为决策方法的变化而出现结论差异。不过这个提法现在看还是太理想了，因为强结论真的很强或显而易见，现代科研也发展上百年了，基本也都发现了。如果一个现象足够强，p 值一定会发现，贝叶斯方法也一定会发现，此时不存在效应大小问题。但更多的事实或规律是埋藏在当前认为的随机或噪音之中的，我们的分析水平也就刚刚好能把疑似信号与噪音进行区分，而这个区分是否靠谱则完全成了迷，统计学在这里帮不上忙，技术进步倒成了关键。我看到一些研究寄希望于数据挖掘技术解决学科内现象发现问题，这里只能说对于显而易见但被忽视的现象是有帮助的，但对于高噪音数据，降低测量噪音对结论的帮助要远大于遴选能发现差异统计方法的努力。数据迷信会让你看到伪规律，而测量技术进步才会真的发现价值规律。

NHST 的另一个问题在于其本身表示不了效应的方向。p 值经常是双边概率取中间那一部分，所以当你看到一个很小的 p 值时，你并不知道这个效应的方向是更大还是更小，此时你还是需要去看效应值。在这个情况下，如果报道 p 值不报道效应，那么就好比我告诉你明天要变天但又不告诉你变成什么一样毫无意义。在多数实验设计中，变化几乎是一定存在的，例如我敲掉了某个基因去验证功能，基因的变化与功能肯定有区别，大都来源于观察实验，更有意义的是影响大小，这个大小更多需要专业判断而不是简单的 p 值。如果理科学生学了半天最后就知道用 p 值来判断结论，那么这个学位不给也罢。这类搞不清楚效应方向的问题是 type S 型错误，S代表符号，此时验证性实验特别需要注意。

NHST 还有个问题在于 p 值的选择性报道或者说发表歧视，研究人员通常会尝试大量的实验条件组合但发表的论文里只有那些有显著性的结果，这导致了科研文献不能准确反应研究现状。这不一定意味着结果是错的，如果这个实验条件具有广泛的数据支持而不是仅仅来自于模型推断的选择性报道，那么我们只能说推理上不严谨。不过，从这里我们可以看出，依赖 p 值来判断结果是存在问题的，而不使用 p 值，很多研究结果实际是无法简单报道的。然而，大量选择性报道可能出现一个副作用，论文的讨论很多是依赖引文，如果引文结果不靠谱，那么后续研究只有同样采用了选择性报道才能继续跟进发表，最后形成一种确认偏误，这情况比想象的要常见。甚至在系统综述过程中，对确认偏误的忽略可能通过结论影响决策者，那就成了一种自我实现。此时，进行综述的科学家可以通过数据的二次分析与整合来凸显那些本来由于样本量不够而忽略的现象。

另外一种隐性 p 值选择性报道是通过模型选择来实现的，不同的统计模型会产生不同的假设检验结果，研究人员通常只会报道那些有阳性结果的统计模型。这个非常难识别，因为统计模型通常比较复杂且研究人员有可能是先上船后买票，也就是先发现这个模型结果有利然后根据模型组织文章。这里面会牵扯到探索性数据分析与论证的差异，如果研究人员把新模型当成了文章亮点，那读者是完全看不出来这里面的不恰当行为的。此时还是要依赖数据公开，如果大家都能下载到数据，标准化后然后同时跑多个模型，只有共存结果才有可能可靠。不过，这里面的问题在于多个模型的假设是不一样的，只有符合数据本身统计特征的模型才能被加入到评价体系里。

关于 NHST 虽然问题一大把，但系统去看，p 值也有着自己的生命力，我想更多人关心的是如果我不用 NHST ，拿什么证明我的结果可靠？如果没得选，这剂毒药还是得吃啊。答案其实上面都大概提到了，你如果坚持使用 p 值，那么就也请同时报告参数估计与置信区间，虽然这个方法也被人喷过。如果你打算完全开一条新路，那就去学贝叶斯统计，贝叶斯统计有自己成套的处理体系，简单说就是先假设参数分布，然后用数据更新分布，后验分布计算出来就同时有点估计跟方差估计，同时多重比较问题也不存在，但随机错误无法避免，此时参数估计方差大也能体现，后续研究可以使用这次的后验数据作为下次先验数据，这样你可以实现完全的 N + 1 模式科研，其验证与预测性也很大程度依赖采样与模拟技术，之前贝叶斯方法不能流行很重要的一个原因就在于计算比较贵，现在就便宜很多了。

这次的危机能不能解决？什么时候解决？现在谁也不知道，但了解问题是解决问题的第一步，而路还很长。

### 3.3.2 规律性

从另一个视角看，可重复性危机的一个重点在于如何判断规律性。所谓规律性，本质在于数据中存在模式或者异质性，如果数据是均质的，那么要么是均匀分布，要么是噪音的正态分布。

前面提到的NHST的原版也就是Fisher那个没有备择假设的版本其实在应对规律性问题上是有点不一样的。在只有零假设的语境下，备择假设其实只是零假设所对应的可能性空间里的一种，只是出现概率比较低。换句话说，如果可能性空间里出现概率高，那么信息含量也就低，因此拒绝也没什么问题。同时，这里面也就不存在条件概率的问题，因为零假设包含了所有假设的可能性空间，数据是一定会支持零假设的。

不过在这个原版里，可能性空间的计算就不是很方便了。经典女士品茶那个实验里，其可能性空间为70种不同的猜测，只有八次猜测每次都猜对的概率是小于0.05的。如果猜的次数更多，那么在0.05这个阈值下其实可以允许犯错；反之，猜的次数少则实验本身的功效不足以发现规律。这里这个p值是穷举完可能性空间计算出的精确值而不是根据分布来算的，那么我们是否可以在保留数据不变的条件下去随机化分组，然后通过仿真模拟来判断是否存在规律性呢？

如果有规律的事实在实验限定的空间里发生，其p值的分布应该会与随机过程产生的p值不一样。这里我不打算采用多次随机抽样，因为此时分布事实上是已知的，此时进行随机实验其实是在假设分布存在且成立的条件下判断事实。相反，我会随机生成一组数据但保留这一组数据当成既成事实，但随机化分组过程来检验p值的分布，此时应该更符合事实存在后对假设的判断这个思路。这里我们考虑三种情况：

- 真实差异固定
- 完全是随机数
- 固定的真实差异加上随机数

第一种情况是规律完全成立；第二种是完全无规律；第三种是可观察或可测量的数据。生成三组数据后我们对其分组（简单二分）进行10000次随机化操作，然后进行t检验，记录并观察p值。



```
set.seed(1)
# 真实差异
x <- c(rep(100,260),rep(200,260))
# 随机差异
xr <- rnorm(520)
# 考虑误差的真实差异
xm <- c(rep(100,260),rep(200,260))+rnorm(520)
p <- pr <- pm <- c()
for(i in 1:10000){
        # 随机化分组
        g <- factor(sample(c(1,2),520,replace = T))
        p[i] <- t.test(x~g)$p.value
        pr[i] <- t.test(xr~g)$p.value
        pm[i] <- t.test(xm~g)$p.value
}
hist(p,breaks = 100)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534310.png)



```
hist(pr,breaks = 100)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534600.png)



```
hist(pm,breaks = 100)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534214.png)

这个结果非常有意思，第一个能看到的现象是如果数据本身存在规律性，那么p值的分布是一个离散分布。这个分布介于0到1之间，越接近0的部分越密集，越接近1的的部分越稀疏，但是如果计算小于0.05，0.5，0.9的比例情况，会发现这种稀疏分布依旧符合均匀分布的概率分布特征。如果数据不存在规律性，那么p值的分布就是很均匀的。如果数据混合了规律性与噪音，依然会显示出这种离散分布特征。下面我用qq图来观察下这个分布跟均匀分布的区别：



```
set.seed(42)
ref <- runif(10000)
qqplot(ref,p)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534634.png)



```
qqplot(ref,pr)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534700.png)



```
qqplot(ref,pm)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534961.png)

可以看到，如果数据本身存在规律性，其随机化分组后的p值虽然跟均匀分布很接近，但qq图上确实会表现出前密后舒的螺旋延伸状态。这里为了区别我再做一个仿真，这次我不是对分组随机而是对采样随机：



```
set.seed(1)
# 固定分组
g <- factor(c(rep(1,260),rep(2,260)))
p <- c()
for(i in 1:10000){
        # 随机化采样
        x <-  sample(c(rep(100,260),rep(200,260))+rnorm(520),520)
        p[i] <- t.test(x~g)$p.value
}
hist(p,breaks = 100)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291535334.png)



```
qqplot(ref,p)
```

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534186.png)

这里我们同样能看到这种蛇形走位，而且似乎随机化样品看到的趋势更明显。但同样的，这里模拟逻辑还是保持数据不变，只是随机化过程。

这个规律性数据的p值分布可能对实验学科非常有意义。如果规律会造成数据异质性而我们的分组过程就是试图发现这种规律性，那么不可避免的会在p值分布上造成离散分布的状态。相反，随机相关则不会呈现出这种p值的离散分布而是均匀分布。这个均匀与离散分布的差异如果能用一个统计量来描述，那么我们事实上就能根据这个统计量区别出真实规律与随机相关。

在这个语境下，我们就不用搞这些多重检验下p值的阈值矫正了，直接对每一次假设检验进行分组随机化模拟过程，然后生成p值分布。如果其对应分布统计量表示为离散均匀分布，那么这一组假设检验的规律性就是有保证的，如果指示为均匀分布，那么这组数据本身就可以判定为无法检测规律而排除。这样我们可以对数据在进行统计推断前做一个规律性测试，只有通过了规律性测试的数据才值得进行统计推断。而且，只要单次统计推断给出小于0.05的p值，我们就可以直接相信，因为那些可能出现随机相关的数据已经被我们排除掉了。

当然，这个方法可能只适用于多重检验语境下可重复性危机，也仅对存在离散分布规律的数据有用，不过这种源于 Fisher 的基于模拟的精确检验思想也许会在计算模拟比较容易的今天推广到更多的应用场景里。

### 3.3.3 可重复性研究

实验结果的可重复性研究不是个新鲜概念，上个世纪就开始有讨论了，最早是心理学，现在已经蔓延到多个学科。但可重复性研究的流程其实比想象的要复杂，完整的可重复性研究包含多个步骤，例如人群可重复性、科学问题是否一致、假设是否一致、实验设计是否一致、实验人员是否一致、数据分析流程是否一致、代码是否可公开、推断的合理性与结论是否一致。新闻里常说的可重复性不好主要是最后一项结论的差异，但整体流程的任何一个环节都可能出问题。不同学科对可重复性的要求也并不一致，实验学科如果是验证简单体系要求会特别高，例如理论物理，而复杂体系考察综合指标要求就可能降到结果数量级上可重复就可以了，例如环境痕量污染物的分析，因为样品前处理复杂待测物含量经常逼近检出限，此时要求很高的可重复性意义不大而含量数量级通常就能反映足够的环境信息。下图是一个常见的可重复性研究需要考虑的步骤，想完整重现别人的研究需要对方提供每一步的信息。

![img](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/202109291534362.png)

不过，狭义的可重复性研究仅关心数据产生后步骤的可重复性，包括但不限于原始数据共享与数据处理代码共享。当前很多学科的期刊都开始要求论文作者共享原始数据，这是一个很好的起点，但更重要的是数据分析步骤。科研用图形化界面软件的流行降低了科研数据分析的门槛，但因为大多数软件并不同时提供用户数据分析的脚本，一个副作用就是使数据分析重现变得很困难。从科研角度来看，保证数据分析的透明度很关键，因此现代科研人员应该花时间去掌握一门高级编程语言，在公布论文的同时也公布论文中分析推断的生成源码，这样既方便自己的成果传播也提高了研究可信度。从原始数据到成果这一阶段的可重复性是应该开放给整个学术圈进行检验的，这样机制的存在会使可重复性效果低的研究成果能被快速排除而不是消耗资源去验证一个数据处理问题，毕竟也很少有基金是支持验证别人研究成果的。

NHST 是科研可重复性危机的核心，但科学家的人性也不可忽视。科学本身是构建在错误校正过程上的，但科学家评价却是人性化的，良好的评价很多时候成了科学家追求的标的。不论是对影响因子的追求还是学术明星的打造，非科学的评价与评优其实影响到了科研结果的报道。科学家正在作为一个团体来维护自己的利益与社会地位，其副作用就是对失败的低容忍度，年龄限制与成果限制使得探索必须要符合效率原则，很多年轻人在年富力强的时候做了大量排列组合而非探索性的工作来确保个人的生存无虞，这样造成的损失目前我们无法衡量。随之伴生的学术不规范、不端与造假则是层出不穷，很多人开始利用一些规则上的漏洞来实现非科研目的，例如审稿流程与评优。科学家的形象要由团体的文化来体现，阳光底下没有新鲜事，除了开放获取的研究成果，研究整体流程也应该实现透明化，这样可以很大程度防止暗箱操作。

流程透明的开放科学趋势也是一种正在流行的解决方案。所谓流程透明，指的是从基金申请到修改到进度报告到结题报告到文章的投稿接受与后续跟进研究及评优都要有公开的记录可以查询，文书都要经过版本控制方便返溯，所有研究人员都要实名负责对应的项目。学术团体接受公众舆论监督与同行监督，日常学术交流也要有公开的记录与反馈机制，所有的记录不直接对外公开但接受实名查询并留存查询记录。这样透明化的流程可以保证学术团体除了发表论文之外还有其他的结果展示途径，进而避免实验结果的选择性汇报与资源的过度集中。打破自我包装与人脉对科研的束缚，让结果更直白地展示给所有人。此外，关于可重复性，Nature 就近年来的科研可重复性危机采访了五组科学家，分别从认知、NHST、FDR、数据共享与范式转化的角度进行了[论述](https://www.nature.com/articles/d41586-017-07522-z)，值得一读。在医疗领域也有了一些有意思的讨论，例如认为基于人群的归纳式诊断会被个性化精准医疗所替代，此时可重复性里内含的平均律就会被彻底颠覆，分子水平的因果逻辑可能成为未来的主要知识探索方向。预印本、开放获取与审稿、科研社交媒体及数据共享等新[趋势](https://theoreticalecology.wordpress.com/2019/01/22/tree-species-richness-and-its-effects-on-productivity-neither-global-nor-consistent/)也孕育着新的问题解决方法。