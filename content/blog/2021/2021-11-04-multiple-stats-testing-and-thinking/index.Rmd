---
title: 读《指北》：多重假设检验记录与思考
author: 王诗翔
date: '2021-11-04'
slug: multiple-stats-testing-and-thinking
categories:
  - Blog
tags:
  - R
  - stats
description: 常用却少思的统计基础
---

<!-- Links -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE, warning = FALSE, message = FALSE,
  fig.width = 7, fig.height = 6
)
```

本文有记录和思考2个方面，记录是根据[《现代科研指北》](https://bookdown.org/yufree/sciguide/data.html#%E7%BB%9F%E8%AE%A1%E6%8E%A8%E6%96%AD)的统计推断的一部分内容进行记录和学习，思考是在记录和学习的过程中添加一些自己的理解和思索。

首先谈谈为什么是这样的形式，而不是直接转载。对于个人而言，学习的本质是为了掌握知识，而不是记录知识。本文的主题是统计分析特别是组学统计分析中常用却甚少思考的一个基本点：**多重比较与假设检验**。我不知道有多少人像我一样，在有几年的数据处理经验之后，对这种比较基础的理论还一知半解。
现在，我们可以轻而易举的使用R的`p.adj()`对p值进行校正，甚至使用
Bioconductor的一些专门的包（如qvalue）进行处理。但我们真的了解它吗？你能简单地说出`p.adj()`中提供的方法原理和区别吗？如果你的目标是数据分析**师**，完成工作任务，仅仅作为赚钱养家的技能。ok，没必要深入学习，会调包调函数完全足够了。但如果我们有更高的追求，比如数据科学**家**，
无论是工业界还是学术界，那么我们必须对概念和问题产生自己的见解。

下面是《指北》中的一些内容。

## 多重比较的场景

科研里最常用的比较是两独立样本均值比较的t检验与评价单因素多水平影响的方差分析。**t检验可以看作方差分析的特例，使用统计量t来比较而方差分析通常是用分类变量所解释的变异比上分类变量以外的变异去进行F检验**。换句话讲，如果分类变量可以解释大部分响应变量的变异，我们就说这种分类变量对响应变量的解释有意义。

但是仅仅知道是否受影响是不够的，我们知道的仅仅是存在一种分类方法可以解释响应的全部变化，其内部也是均匀的，但不同分类水平间的差异我们并不知道，这就需要多重比较了。例如，当我们对两组数据做置信度0.05的t检验，我们遇到假阳性的概率为5%。但如果面对多组数据例如3组，进行两两比较的话就有$3\choose2$也就是3组对比，那么我们遇到假阳性的概率就为$1-(1-0.05)^3$，也就是14.3%，远高于0.05的置信度。组越多，**两两对比就越多，整体上假阳性的概率就越来越大，到最后就是两组数据去对比，无论如何你都会检验出差异**。

>值得思考的一个点是：这里一般提出的比较是多组，如A、B、C这3个组比较同一个指标的差异。而在组学分析中的比较是固定的A、B这2个组不同的指标的比较。它们能看作一样的事情吗？

>本质上是一样的，关键在对比的数量。我们可以把比较拆开为独立的1对1的比较。那么比较一次假设出现错误的概率是0.05，那么比对的数量越多，整体上的分析结果中出现一次错误的概率会越大于0.05。

此外就方向而样，虽然我们都不承认零假设（要不然还做什么实验），但当我们默认设定为双尾检验时，假阳性就被默认发生在两个方向上了，这样的多重比较必然导致在其中一个方向上的错误率被夸大了。就影响大小而言，如果我们每次重复都选择效应最强的那一组，重复越多，预设的偏态就越重，换言之，我们的零假设因为重复实验的选择偏好而发生了改变。

## 多重比较

那么多重比较如何应对这个问题呢？有两种思路，一种思路是我依旧采取两两对比，进行t检验，但p值的选取方法要修改，例如**Bonferroni方法中就把p的阈值调整为进行多重比较的次数乘以计算得到的p值**。如果我们关心的因素为2，那么计算得到的p值都要乘2来跟0.05或0.01的边界置信度进行比较；另一种思路则是修改两两比较所用的统计量，给出一个更保守的分布，那么得到p值就会更大。**不论怎样，我们这样做都是为了降低假阳性，但同时功效不可避免的降低了**。（有得必有失）


多重比较的方法类型包括**单步法与逐步法**。
**单步法只考虑对零假设的影响而不考虑其他影响而逐步法则会考虑其他假设检验对单一检验的影响**，例如可以先按不同分组均值差异从大到小排序，先对比第一个，有差异对比下一个，当出现无差异时停止对比；或者从下到大排序，有差异时停止对比，之后均认为有差异。此时还要注意一种特殊情况，**因为F检验是从方差角度来考虑影响显著性与否，所以可能存在F检验显著但组间均值差异均不显著的情况，此时要考虑均值间线性组合的新均值的差异性**（？？？）。不过，大多数情况我们只用考虑不同组间两两差异比较即可。

具体而言，单步法等方差多重比较最常见的是Tukey’s HSD方法，这是一个两两比较的方法，基于 studentized range 分布计算出q统计量，然后基于这个统计量进行两两间差异的假设检验。该方法适用于分组间等方差等数目的场景，如果分组内数目不同，需要用 Tukey-Kranmer 方法。该方法适用于两两比较，在分组数目相同时统计功效等同于从大到小排序的逐步法。

此外，还有些多重比较的方法在特定学科里也很常见。从总体控制错误率的角度，如果是两两比较应该选 Tukey’s HSD方法；如果侧重组间差异线性组合的均值用 Scheffe test；如果对比数指定了，功效按 Gabriel、GT2、DST、 Bonferroni顺序来选；如果是各分组都跟控制组比，应该选Dunnett法；如果各分组方差不相等，用GH，C，T3等方法。此外，如果打算保证每个比较中的置信水平，应该选 Tukey、 Scheffe、Dunnett法。

> 远比想象中要复杂。

## 多重检验

与多重比较类似的一个统计推断问题是多重检验问题。多重检验指的是同时进行多次假设检验的场景，其实多重比较可以看作多重检验在方差分析里的一个特例。

举例而言，我对两组样品（暴露组跟对照组）中每一个样品测定了10000个指标，每组有10个样品，那么如果我想知道差异有多大就需要对比10000次，具体说就是10000次双样本t检验。那么如果我对t检验的置信水平设置在95%，也就是5%假阳性，做完这10000次检验，我会期望看到500个假阳性，而这500个有显著差异的指标其实对分组不敏感也可以随机生成。**假如真实测到了600个有显著差异的指标，那么如何区分其中哪些是对分组敏感？哪些又仅仅只是随机的呢？随机的会不会只有500个整呢**？这个场景在组学技术与传感器技术采集高通量高维数据的今天变得越来越普遍。

这个问题在做经典科研实验时往往会忽略，**深层次的原因是经典的科研实验往往是理论或经验主导需要进行检验的假说**（注：经典实验比较的数目量也上不去）。例如，我测定血液中白血球的数目就可以知道你是不是处于炎症中，其背后是医学知识的支撑。然而，**在组学或其他高通量实验中，研究实际是数据导向的，也就是不管有用没用反正我测了一堆指标，然后就去对比差异，然后就是上面的问题了，我们可能分不清楚哪些是真的相关，哪些又是随机出现的**。

对于单次比较，当我们看到显著差异的p值脑子里想的是零假设为真时发生的概率，当我们置信水平设定在0.95而p值低于对应的阈值，那么我们应该拒绝零假设。**但对比次数多了从概率上就会出现已经被拒绝的假设实际是错误的而你不知道是哪一个**。**整体错误率控制的思路就是我不管单次比较了，我只对你这所有的对比次数的总错误率进行控制**。还是上面的例子，对于10000次假设检验我只能接受1个错误，整体犯错概率为0.0001，那么对于单次比较，其假阳性也得设定在这个水平上去进行假设检验，**结果整体上错误率是控制住了，但对于单次比较就显得十分严格了**。下面用一个仿真实验来说明：

```{r}
# 随机数的10000次比较
set.seed(42)
pvalue <- NULL
for (i in 1:10000){
  a <- rnorm(10)
  b <- rnorm(10)
  c <- t.test(a,b)
  pvalue[i] <- c$p.value
}
# 看下p值分布
hist(pvalue)

# 小于0.05的个数
sum(pvalue<0.05)

# 小于0.0001的个数
sum(pvalue<0.0001)
```

这样我们会看到进行了整体的控制之后，确实是找不到有差异的了，而且此时我们也可以看到p值的分布应该是0到1之间的均匀分布。**但假如里面本来就有有差异的呢**？

```{r}
set.seed(42)
pvalue <- NULL
for (i in 1:10000){
  a <- rnorm(10,1)
  b <- a+1
  c <- t.test(a,b)
  pvalue[i] <- c$p.value
}
# 看下p值分布
hist(pvalue)

# 小于0.05的个数
sum(pvalue<0.05)

# 小于0.0001的个数
sum(pvalue<0.0001)
```

上面我们模拟了10000次有真实差异的假设检验，结果按照单次检验0.05的阈值能发现约7000有差异（注：实际上模拟的数据10000个都有差异），而使用0.0001却只能发现不到100次有显著差异。那么问题很明显，或许**控制整体错误率可以让我们远离假阳性，但假阴性就大幅提高了或者说功效降低了**，最后的结果可能是什么差异也看不到。

下面我们尝试一个更切合实际的模拟，混合有差异跟无差异的检验：

```{r}
set.seed(42)
pvalue <- NULL
for (i in 1:5000){
  a <- rnorm(10,1)
  b <- a+1
  c <- t.test(a,b)
  pvalue[i] <- c$p.value
}
for (i in 1:5000){
  a <- rnorm(10,1)
  b <- rnorm(10,1)
  c <- t.test(a,b)
  pvalue[i+5000] <- c$p.value
}
# 看下p值分布
hist(pvalue)
# 小于0.05的个数
sum(pvalue<0.05)
# 小于0.0001的个数
sum(pvalue<0.0001)
```

**此时结果就更有意思了，明明应该有5000次是有差异的，但阈值设定在0.05只能看到约3500次，而0.0001只能看到24次**。

上面的模拟告诉我们，**降低假阳性的方法会提高假阴性的比率，也就是功效不足，无法发现本来就有的差异**。而且似乎本来0.05的阈值对于真阳性也是偏小的（注：5000个模拟有差异的只发现了3500个左右）。同时，面对假设检验概率低于0.05的那些差异，我们也没有很好的方法区别哪些是真的，哪些是随机的。其实很多人都知道整体错误率控制是比较严格的，会损失统计功效。但也不是完全没人用，例如寻找生物标记物做重大疾病诊断时就不太能接受假阳性而可以接受一定的假阴性，此时如果标准放宽就会找到一大堆假信号，到时候标记不准就会对诊断产生负面影响。

## 整体错误率控制方法

下面介绍下常见的两种**整体错误率控制**方法。第一种是Bonferroni方法，控制的是整体错误率（FWER），思路很简单，就是控制显著性，例如单次检验假阳性比率α控制在0.05，那么n次检验假阳性比率控制为$\frac{\alpha}{n}$。这样实际是对整体采用了个体控制的控制思路：

$$
P(至少一个显著)=1-P(无显著差异) = 1-(1-\alpha/n)^n
$$

我们来看下α=0.05随比较数增加的效果：

```{r}
# 比较次数
n <- c(1:10 %o% 10^(1:2))
# 整体出现假阳性概率
p0 <- 1-(1-0.05)^n
# Bonferroni方法控制后的整体假阳性概率
p <- 1-(1-0.05/n)^n
# 不进行控制
plot(p0~n,ylim = c(0,1),xlab='numbers of tests',ylab = 'FWER')
# Bonferroni方法控制
points(p~n,pch=19,col='red')
legend('right',c('Bonferroni','Raw'),col = c('red','black'),pch = 1,cex=1)
```

其实，这样的控制得到的整体错误率是略低于0.05的，并且**数目越大，整体错误率越低**。这个方法十分保守，有可能什么差异你都看不到，因为**都变成假阴性**了。**在实际应用中一般不调节p值的假阳性比率而直接调节p值**，取原始p值跟整体检验数目的乘积与1的最小值作为调节p值，还可以用0.05或0.01进行判断，不过这时候控制的整体而不是单一检验了。

当然这只是最原始的Bonferroni方法，后来**Holm改进了这种一步法为逐步法**，此时我们需要首先对原始p值进行排序，然后**每个原始p值乘上其排序作为调节p值**。例如三次多重检验的p值分别是0.01、0.03与0.06，排序为3、2、1，其调节后的p值为0.03，0.06，0.06。如果我们控制整体假阳性比率低于0.05，那么调解后只有第一个检验可以拒绝零假设。值得注意的是**Holm的改进是全面优于原始方法的**，也就是说当你一定要去用Bonferroni方法控制整体错误率，优先选Holm的改进版。

上面那种方法其实有点非参的意思，其实数学上我们是可以精确的把假阳性比率控制在某个数值的，也就是Sidak方法：

$$
P(至少一个显著)=1-P(无显著差异) = 1-(1-\alpha')^n = 0.05
$$

但是，这个方法有个前提就是各个检验必须是独立的，这在生物学实验里几乎不可能，所以这个方法的应用远没有Bonferroni方法广。

刚才的模拟中我们可以看到，控制整体错误率比较严格，假阴性比率高，**那么有没有办法找到假阴性比率低或者说功效高的呢**？要知道我们其实**只关心有差异的那部分中那些是真的，哪些是假的**，无差异的可以完全不用考虑。那么我们可以尝试控制错误发现率（FDR），**也就是在有差异的那一部分指标中控制错误率低于某一水平**。（注：也就是Holm与FDR是存在思想上的大区别的）

```{r}
# 所有有差异的
R <- sum(pvalue<0.05)
R
# 假阳性
V <- sum(pvalue[5001:10000]<0.05)
V
# 错误发现率
Q <- V/R
Q
```

上面的计算显示虽然我们漏掉了很多阳性结果，但错误发现率并不高。事实上如果我们控制错误率到0.01，错误发现率会更低：

```{r}
# 所有有差异的
R <- sum(pvalue<0.01)
R
# 假阳性
V <- sum(pvalue[5001:10000]<0.01)
V
# 错误发现率
Q <- V/R
Q
```

>需要注意我们实际有差异的也下降了好几倍。

其实出现这个问题不难理解，零假设检验里p值是均匀分布的而**有差异检验的p值是有偏分布且偏向于较小的数值**，所以假阳性控制的越小，有偏分布占比例就越高，但同时会造成假阴性提高的问题。

**那么错误发现率会不会比整体错误率的控制更好呢**？这里通过两种常见的控制方法进行说明。

**Benjamini-Hochberg方法跟Holm方法很像**，也是先排序，但之后**p值并不是简单的乘排序，而是乘检验总数后除排序**：

$$
p_i \leq \frac{i}{m} \alpha
$$

举例来说就是假设三次多重检验的p值分别是0.01、0.03与0.06，其调节后的p值为0.03，0.45，0.06（`0.06 * 3 / 3`）。那么为什么说这种方法控制的是错误发现率呢？我们来看下α是如何得到的：**p值乘总数m得到的是在该p值下理论发现数，而除以其排序实际是该p值下实际发现数**，理论发现数基于在这里的分布是均匀分布，也就是零假设的分布，这两个的比值自然就是错误发现率。下面我用仿真实验来说明一下：

```{r}
# BH法控制错误发现率
pbh <- p.adjust(pvalue,method = 'BH')
# holm法控制错误发现率
ph <- p.adjust(pvalue,method = 'holm')
plot(pbh~pvalue,ylab = 'FDR',xlab = 'p value')
points(ph~pvalue,col='red')
legend('bottomright',c('Holm','BH'),col = c('red','black'),pch = 1,cex=1)
```

> FDR是假阳性数目占全部阳性数目比率。

从上面图我们可以看出，**如果控制整体错误率（红色），那么p值很容易就到1了，过于严格。而如果用BH方法控制错误发现率，那么原始p值越大，调节后的错误发现率也逐渐递增，这就符合了区分真实差异与随机差异就要假设真实差异更可能出现更小的p值这个现象**。值得注意的是这个错误发现率求的是有差异存在的情况，不然零发现就出现除数为零了。

如果说BH方法还算是调节了p值，那么Storey提出的方法则直接去估计了错误发现率本身。**刚才介绍BH算法时我提到总数m与p值的乘积是基于这里的分布是均匀分布，但实际上按照错误发现率的定义，这里应该出现的是零假设总数。直接使用所有检验数会造成一个问题，那就是对错误发现率的高估，为了保证功效，这里应该去估计零假设的总体比例**。这里我们去观察混合分布会发现在p值较大的时候基本可以认为这里分布的都是零假设的p值，那么我们可以用：

$$
\hat\pi_0 = \frac{\#\{p_i>\lambda\}}{(1-\lambda)m}
$$
估计这个比例$\hat\pi_0$。其中参数λ的跟$\hat\pi_0$的关系可以用一个三阶方程拟合，然后计算出整体假阳性比例。有了这个比例，我们再去按照BH方法计算p值，**然后两个相乘就会得到q值，而q值的理论含义就是在某一概率上低于这个概率所有数里假阳性的比重**。打个比方，我测到某个指标的q值是0.05，这意味着q值低于这个数所有检验中我有0.05的可能性得到的是假阳性。。但我们会发现当零假设比重较高时BH结果跟q值很接近，而比重很低的话q值会变得更小，功效会提高，基本也符合我们对错误发现率的预期。

```{r}
library(qvalue)
q <- qvalue(pvalue)
# Q值
plot(q$qvalues~pvalue,col='blue',ylab = 'Q',xlab = 'p value')
```

> 零假设就是本身数据就没有差异的数据。

如上图所示，**q值增大后会最终逼近到0.5，而我们的模拟中零假设的比例就设定就是50%**。我们重新模拟一个零假设比例5%的实验：

```{r}
set.seed(42)
pvalue <- NULL
for (i in 1:500){
  a <- rnorm(10,1)
  b <- a+1
  c <- t.test(a,b)
  pvalue[i] <- c$p.value
}
for (i in 1:9500){
  a <- rnorm(10,1)
  b <- rnorm(10,1)
  c <- t.test(a,b)
  pvalue[i+500] <- c$p.value
}
pbh <- p.adjust(pvalue,method = 'BH')
ph <- p.adjust(pvalue,method = 'holm')
q <- qvalue(pvalue)
plot(pbh~pvalue,ylab = 'FDR',xlab = 'p value')
# Holm 方法
points(ph~pvalue,col='red')
# Q值
points(q$qvalues~pvalue,col='blue')
legend('bottomright',c('Q','Holm','BH'),col = c('blue','red','black'),pch = 1,cex=1)
```

此时我们可以看到两者结果较为接近，**q值理论上更完备，功效也更强，但算法上对$\hat\pi_0$的估计并不稳定**，特别是比例靠近1的时候，所以BH方法可能还是更容易让人接受的保守错误发现率控制方法。

**多重检验问题是高通量数据里逃不掉的问题，要想找出真正的差异数据就要面对假阳性跟假阴性问题，这是一个不可兼得的过程，看重假阳性就用整体错误率（注：记住前面提到Holm方法优于Bonferroni方法，优先选择），看重假阴性或功效就用错误发现率控制（使用BH或者Q值）**。并不是说哪种方法会好一些，**更本质的问题在于你对实际问题的了解程度及统计方法的适用范围**。实验设计本身就会影响数据的统计行为，而这个恰恰是最容易被忽视的。

## 读后感

读完这一段之后我对多重检验校正的内容算是完全通透了，其中的核心在上面的注解部分也提到了。最最实质的总结是：

- 如果想要减少假阳性，使用Holm方法（不推荐使用严格的Bonferroni方法）。
- 如果想要在控制一定假阳性的前提下尽量提升统计功效，使用BH方法或者Q值。


